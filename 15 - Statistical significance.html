
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Statistical significance &#8212; The Python Companion of Intuitive Biostatistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '15 - Statistical significance';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Statistical power and sample size" href="20%20-%20Statistical%20power%20and%20sample%20size.html" />
    <link rel="prev" title="Confidence interval of counted data" href="06%20-%20Confidence%20interval%20of%20counted%20data.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.jpg" class="logo__image only-light" alt="The Python Companion of Intuitive Biostatistics - Home"/>
    <script>document.write(`<img src="_static/logo.jpg" class="logo__image only-dark" alt="The Python Companion of Intuitive Biostatistics - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    Python Companion Guide to “Intuitive Biostatistics”
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Continuous variables</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09%20-%20Quantifying%20scatter%20of%20continuous%20data.html">Quantifying scatter of continuous data</a></li>
<li class="toctree-l1"><a class="reference internal" href="10%20-%20Gaussian%20distribution.html">The Gaussian distribution</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Confidence intervals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12%20-%20Confidence%20interval%20of%20a%20mean.html">Confidence interval of a mean</a></li>
<li class="toctree-l1"><a class="reference internal" href="04%20-%20Confidence%20interval%20of%20a%20proportion.html">Confidence interval of a proportion</a></li>
<li class="toctree-l1"><a class="reference internal" href="05%20-%20Confidence%20interval%20of%20survival%20data.html">Confidence interval of survival data</a></li>
<li class="toctree-l1"><a class="reference internal" href="06%20-%20Confidence%20interval%20of%20counted%20data.html">Confidence interval of counted data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Statistical significance and data assumptions</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Statistical significance</a></li>
<li class="toctree-l1"><a class="reference internal" href="20%20-%20Statistical%20power%20and%20sample%20size.html">Statistical power and sample size</a></li>
<li class="toctree-l1"><a class="reference internal" href="24%20-%20Normality%20tests%20and%20outliers.html">Normality tests and outliers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Statistical tests</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="27%20-%20Comparing%20proportions.html">Comparing proportions</a></li>
<li class="toctree-l1"><a class="reference internal" href="29%20-%20Comparing%20survival%20curves.html">Comparing survival curves</a></li>
<li class="toctree-l1"><a class="reference internal" href="30%20-%20Comparing%20two%20unpaired%20means.html">Comparing two unpaired means</a></li>
<li class="toctree-l1"><a class="reference internal" href="31%20-%20Comparing%20paired%20data.html">Comparing paired data</a></li>
<li class="toctree-l1"><a class="reference internal" href="32%20-%20Correlation.html">Correlation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Fitting models to data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="33%20-%20Simple%20linear%20regression.html">Simple linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="35%20-%20Comparing%20models.html">Comparing models</a></li>
<li class="toctree-l1"><a class="reference internal" href="36%20-%20Nonlinear%20regression.html">Nonlinear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="37%20-%20Multiple%20regression.html">Multiple regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="38%20-%20Logistic%20regression.html">Logistic regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">The rest of statistics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="39%20-%20ANOVA.html">ANOVA</a></li>
<li class="toctree-l1"><a class="reference internal" href="21%20-%20Meta-analysis.html">Meta-analysis {#sec-meta-analysis}</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sbwiecko/intuitive_biostatistics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sbwiecko/intuitive_biostatistics/edit/master/15 - Statistical significance.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/15 - Statistical significance.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Statistical significance</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-reproducibility-challenge">The reproducibility challenge</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variability-of-p-values-under-a-true-effect">Variability of P values under a true effect</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-risk-of-false-positives-under-the-null-hypothesis">The risk of false positives under the null hypothesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#influence-of-sample-size">Influence of sample size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-danger-of-ad-hoc-sample-size-decisions">The danger of ad hoc sample size decisions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-and-p-values">Confidence intervals and P values</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-significance-and-hypothesis-testing">Statistical significance and hypothesis testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-i-and-type-ii-errors">Type I and type II errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-statistical-significance">Interpreting statistical significance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-significance-level">The significance level</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#false-positive-report-probability-fprp">False positive report probability (FPRP)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-influences-fprp">Prior probability influences FPRP</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-of-1">Prior probability of 1%</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-of-50">Prior probability of 50%</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-challenge-of-multiple-comparisons">The challenge of multiple comparisons</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bonferroni-correction">Bonferroni correction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#false-discovery-rate-fdr">False discovery rate (FDR)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-for-equivalence-or-noninferiority">Testing for equivalence or noninferiority</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bioequivalence">Bioequivalence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#two-one-sided-tests-tost">Two one-sided tests (TOST)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-equivalence-and-noninferiority-tests">Interpreting equivalence and noninferiority tests</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-diagnostic-accuracy">Assessing diagnostic accuracy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">Definitions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-curves">ROC curves</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-thresholds">Decision thresholds</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#area-under-the-curve">Area under the curve</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-and-python">ROC and Python</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-studies">Case studies</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#porphyria-test-in-0-01-prevalence">Porphyria test in 0.01% prevalence</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#porphyria-test-in-50-prevalence">Porphyria test in 50% prevalence</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hiv-test">HIV test</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-with-statistical-hypothesis-testing">Analogy with statistical hypothesis testing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-revisited">Bayes revisited</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#session-information">Session information</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="statistical-significance">
<h1>Statistical significance<a class="headerlink" href="#statistical-significance" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In the world of biostatistics, we often face a crucial question: does the data we’ve gathered reveal a <em>true effect</em>, or is it simply the result of <em>random chance</em>? The concept of <strong>statistical significance</strong> offers a framework to grapple with this uncertainty, providing a tool to help us discern <em>meaningful</em> patterns from the <em>noise</em> inherent in biological systems. This chapter delves into the core of statistical significance, exploring why it matters, when it’s most useful, and how it can be a valuable guide in drawing conclusions from the data. Importantly, we’ll also discuss the potential pitfalls of relying too heavily on this concept, as well as when alternative approaches might be more appropriate.</p>
</section>
<section id="the-reproducibility-challenge">
<h2>The reproducibility challenge<a class="headerlink" href="#the-reproducibility-challenge" title="Link to this heading">#</a></h2>
<section id="variability-of-p-values-under-a-true-effect">
<h3>Variability of P values under a true effect<a class="headerlink" href="#variability-of-p-values-under-a-true-effect" title="Link to this heading">#</a></h3>
<p>P values, a cornerstone of statistical significance testing, can often give the illusion of definitive answers. However, they are inherently sensitive to the specific data set at hand, leading to a reproducibility challenge. To illustrate this, let’s consider a Python simulation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">ttest_ind</span>

<span class="c1"># For reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Parameters</span>
<span class="n">sample_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">population_std_dev</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">mean_difference</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store P values</span>
<span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Run simulations</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
    <span class="c1"># Sample from two populations with different means</span>
    <span class="n">data1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">population_std_dev</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">)</span>
    <span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean_difference</span><span class="p">,</span> <span class="n">population_std_dev</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">)</span>
    
    <span class="c1"># Perform t-test</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">)</span>
    <span class="n">p_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>

<span class="c1"># Analyze P values</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Proportion of P values &lt; 0.05:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Proportion of P values &lt; 0.05: 0.592
</pre></div>
</div>
</div>
</div>
<p>This code repeatedly samples 10 data points from two Gaussian distributions with the same standard deviation (5.0) but a mean difference of 5.0, so under a <em>true effect</em>. Each time, it performs a <strong>t-test</strong> to compare the means and records the resulting P value.</p>
<p>What we often find is a surprising inconsistency. Even though the underlying populations have a true difference in means, the proportion of P values considered “statistically significant” (typically <span class="math notranslate nohighlight">\(\alpha \lt 0.05\)</span>) fluctuates significantly across simulations. This highlights the inherent variability of P values and the potential for misleading conclusions when a single analysis is treated as definitive.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># For reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Simulation parameters</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1"># Sample size per group</span>
<span class="n">SD</span> <span class="o">=</span> <span class="mf">5.0</span>    <span class="c1"># Standard deviation</span>
<span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span>    <span class="c1"># Mean of the first population</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">2500</span>

<span class="c1"># Store P values</span>
<span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Run simulations</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
    <span class="n">pop_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">SD</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
    <span class="n">pop_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span> <span class="o">+</span> <span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">SD</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>  
    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">pop_1</span><span class="p">,</span> <span class="n">pop_2</span><span class="p">)</span>
    <span class="n">p_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>

<span class="c1"># Calculate percentiles</span>
<span class="n">percentiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">])</span>  <span class="c1"># Get 2.5% and 97.5% percentiles</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The 2.5th and 97.5th percentiles are </span><span class="si">{</span><span class="n">percentiles</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span> <span class="c1"># Optional: set a clean seaborn style</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="c1"># Histogram with log scale</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;skyblue&quot;</span><span class="p">)</span>
<span class="c1"># Vertical line at p=0.05</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;α = 0.05&quot;</span><span class="p">)</span>

<span class="c1"># Add percentile lines and annotations</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">percentiles</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;P value (log scale)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency (log scale)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="s2">&quot;Distribution of P values from 2500 simulations</span><span class="se">\n</span><span class="s2">(true mean difference = 5.0)&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The 2.5th and 97.5th percentiles are [1.69152124e-04 7.02604441e-01]
</pre></div>
</div>
<img alt="_images/d3f1f2113043dd04d395addd4a5b7861ae82f2c0d9dfea53061d3a9bfaeeb58c.png" src="_images/d3f1f2113043dd04d395addd4a5b7861ae82f2c0d9dfea53061d3a9bfaeeb58c.png" />
</div>
</div>
<p>As our simulation demonstrates, the middle 95% of P values can span a vast range, from as low as 0.000169 to as high as 0.7026 - a difference of over three orders of magnitude! This immense variability underscores a crucial point: P values are far less reproducible than most researchers anticipate.</p>
<p>This lack of reproducibility has significant implications for the conclusions we draw about statistical significance. Consider a study that finds a P value of exactly 0.05, barely crossing the threshold for significance. If this study were repeated, there’s only a 50% chance that the new P value would also be below 0.05, as revealed by simulations:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Initial P value</p></th>
<th class="head text-left"><p>Probability of P value &lt; 0.05 in replication</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>0.10</p></td>
<td class="text-left"><p>38 %</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>0.05</p></td>
<td class="text-left"><p>50 %</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>0.01</p></td>
<td class="text-left"><p>73 %</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>0.001</p></td>
<td class="text-left"><p>91 %</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>0.00031</p></td>
<td class="text-left"><p>95 %</p></td>
</tr>
</tbody>
</table>
</div>
<p>In other words, even a seemingly “significant” finding is surprisingly susceptible to being overturned by chance in subsequent experiments.</p>
<p>This phenomenon isn’t limited to borderline cases. Even with a highly “significant” initial P value of 0.001, there’s no guarantee that a repeat study will yield a P value below the traditional 0.05 threshold. This highlights a fundamental truth: statistical significance, as determined by P values, is not a static property of a phenomenon but rather a probabilistic outcome dependent on the specific data at hand.</p>
</section>
<section id="the-risk-of-false-positives-under-the-null-hypothesis">
<h3>The risk of false positives under the null hypothesis<a class="headerlink" href="#the-risk-of-false-positives-under-the-null-hypothesis" title="Link to this heading">#</a></h3>
<p>While the variability of P values under a true effect is a concern, an even more insidious issue arises when there is no real effect to be found. In this scenario, the <em>null hypothesis is true</em>, yet random sampling can still lead to statistically significant results. This simulation explores the frequency of such false positives and their implications for drawing reliable conclusions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulation parameters</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1"># Sample size per group</span>
<span class="n">SD</span> <span class="o">=</span> <span class="mf">5.0</span>    <span class="c1"># Common standard deviation</span>
<span class="n">mean</span> <span class="o">=</span> <span class="mi">10</span>   <span class="c1"># Common mean for both populations</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">2500</span>

<span class="c1"># For reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Store P values</span>
<span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Run simulations</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
    <span class="n">pop_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">SD</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
    <span class="c1"># Same mean and SD for both populations, same sample size</span>
    <span class="n">pop_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">SD</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">pop_1</span><span class="p">,</span> <span class="n">pop_2</span><span class="p">)</span>
    <span class="n">p_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>

<span class="c1"># Calculate significance proportion</span>
<span class="n">signif_proportion</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">0.05</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;skyblue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;α = 0.05&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;P value (log scale)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency (Log Scale)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="s2">&quot;Distribution of P values from 2500 simulations</span><span class="se">\n\</span>
<span class="s2">(null hypothesis true: no mean difference)&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="mf">0.0004</span><span class="p">,</span> <span class="mi">290</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">signif_proportion</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">num_simulations</span><span class="p">)</span><span class="si">}</span><span class="s2"> experiments out of </span><span class="se">\n\</span>
<span class="si">{</span><span class="n">num_simulations</span><span class="si">}</span><span class="s2"> yield p ≤ 0.05 (</span><span class="si">{</span><span class="n">signif_proportion</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span> 
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fe047310a346ba19c0130399e9f1f1695f9a3bc30eb590e2fa336d08b1f50251.png" src="_images/fe047310a346ba19c0130399e9f1f1695f9a3bc30eb590e2fa336d08b1f50251.png" />
</div>
</div>
<p>In our simulation, where both populations had the same mean, approximately 5% of the experiments yielded P values below the 0.05 significance threshold. This phenomenon is known as a <strong>type I error</strong>, or a <strong>false positive</strong>. This aligns with the chosen significance level, highlighting that statistical significance does not guarantee a true effect.</p>
<p>This underscores the importance of interpreting P values cautiously and considering them within the broader context of the research question, effect size, and potential for replication.</p>
</section>
<section id="influence-of-sample-size">
<h3>Influence of sample size<a class="headerlink" href="#influence-of-sample-size" title="Link to this heading">#</a></h3>
<p>The size of the sample (the number of observations in each group) plays a crucial role in determining the outcome of a statistical test. Even with the same underlying effect size and variability, a larger sample size can drastically alter the resulting P value. Let’s illustrate this with a simulation using the t-distribution’s cumulative distribution function (CDF). Here the number of degrees of freedom equals <span class="math notranslate nohighlight">\(n_1 + n_2 - 2\)</span> which, with <span class="math notranslate nohighlight">\(n_1 = n_2 = n\)</span>, equals <span class="math notranslate nohighlight">\(2 \times n - 2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">t</span>

<span class="c1"># Simulation parameters</span>
<span class="n">mean1</span><span class="p">,</span> <span class="n">mean2</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span>    <span class="c1"># Group means (fixed difference)</span>
<span class="n">mean_diff</span> <span class="o">=</span> <span class="n">mean2</span> <span class="o">-</span> <span class="n">mean1</span>
<span class="n">SD</span> <span class="o">=</span> <span class="mi">5</span>                   <span class="c1"># Standard deviation (fixed)</span>

<span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">31</span><span class="p">):</span>   <span class="c1"># Varying sample sizes</span>
    <span class="c1"># Calculate t-statistic (see previous chapter)</span>
    <span class="n">t_stat</span> <span class="o">=</span> <span class="n">mean_diff</span> <span class="o">/</span> <span class="p">(</span><span class="n">SD</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">p_value</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_stat</span><span class="p">),</span> <span class="n">df</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># Two-tailed P value</span>
    <span class="n">p_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">31</span><span class="p">)),</span> <span class="n">y</span><span class="o">=</span><span class="n">p_values</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sample size (per group)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;P value&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="s2">&quot;Impact of sample size on P values</span><span class="se">\n\</span>
<span class="s2">(fixed mean difference and standard deviation)&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
    <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;α = 0.05&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/234c2a9e8ce37b8252ab9da5a2b8b18fee852b2e9caa109f66bdac4e4c04851e.png" src="_images/234c2a9e8ce37b8252ab9da5a2b8b18fee852b2e9caa109f66bdac4e4c04851e.png" />
</div>
</div>
<p>Of course, we can achieve the same results using both <code class="docutils literal notranslate"><span class="pre">scipy.stats.ttest_ind</span></code> (for the t-test) and directly working with the t-distribution, because the t-test relies on the t-distribution to model the behavior of the test statistic under the null hypothesis (will be discussed in later chapter).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sample data (two groups)</span>
<span class="n">data1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">14</span><span class="p">]</span>
<span class="n">data2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>

<span class="c1"># Method 1: Using scipy.stats.ttest_ind</span>
<span class="n">t_stat</span><span class="p">,</span> <span class="n">p_value_ttest</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">)</span>

<span class="c1"># Method 2: Calculating directly from t-distribution</span>
<span class="n">n1</span><span class="p">,</span> <span class="n">n2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)</span>                <span class="c1"># Sample sizes</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">n1</span> <span class="o">+</span> <span class="n">n2</span> <span class="o">-</span> <span class="mi">2</span>                               <span class="c1"># Degrees of freedom</span>
<span class="n">mean1</span><span class="p">,</span> <span class="n">mean2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data2</span><span class="p">)</span>  <span class="c1"># Sample means</span>
<span class="n">pooled_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
    <span class="p">(</span>   <span class="c1"># Pooled standard deviation</span>
        <span class="p">(</span><span class="n">n1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">n2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span> <span class="o">/</span> <span class="n">df</span>
<span class="p">)</span>
<span class="n">t_stat_manual</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean1</span> <span class="o">-</span> <span class="n">mean2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">pooled_std</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">n2</span><span class="p">))</span>

<span class="c1"># Calculate P value using t-distribution CDF</span>
<span class="n">p_value_manual</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_stat_manual</span><span class="p">),</span> <span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">))</span> 

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Results using ttest_ind:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t-statistic: </span><span class="si">{</span><span class="n">t_stat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, P value: </span><span class="si">{</span><span class="n">p_value_ttest</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Results using manual calculation:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t-statistic: </span><span class="si">{</span><span class="n">t_stat_manual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, P value: </span><span class="si">{</span><span class="n">p_value_manual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Results using ttest_ind:
t-statistic: -4.6677, P value: 0.0016

Results using manual calculation:
t-statistic: -4.6677, P value: 0.0016
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-danger-of-ad-hoc-sample-size-decisions">
<h3>The danger of ad hoc sample size decisions<a class="headerlink" href="#the-danger-of-ad-hoc-sample-size-decisions" title="Link to this heading">#</a></h3>
<p>In research, it’s tempting to collect data <em>incrementally</em> and assess statistical significance after each addition. However, this <strong>ad hoc</strong> approach to sample size can lead to misleading conclusions. Let’s simulate how <strong>cumulative P values</strong> behave under the null hypothesis, where there’s no true difference between groups.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulation parameters</span>
<span class="n">sample_size_max</span> <span class="o">=</span> <span class="mi">75</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">common_mean</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">common_std_dev</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="c1"># Seaborn color palette</span>
<span class="n">palette</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">n_colors</span><span class="o">=</span><span class="n">num_simulations</span><span class="p">)</span>

<span class="c1"># For reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Plotting setup</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>

<span class="c1"># Run simulations and plot</span>
<span class="k">for</span> <span class="n">sim_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
    <span class="c1"># Generate data for two populations with the same mean and SD</span>
    <span class="n">pop_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">common_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">common_std_dev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">sample_size_max</span><span class="p">)</span>
    <span class="n">pop_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">common_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">common_std_dev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">sample_size_max</span><span class="p">)</span>

    <span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">sample_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Iterate over increasing sample sizes</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">pop_1</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">pop_2</span><span class="p">[:</span><span class="n">n</span><span class="p">])</span>
        <span class="n">p_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>

    <span class="c1"># Plot for this simulation using the Seaborn palette</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">p_values</span><span class="p">,</span>
        <span class="c1"># marker=&#39;o&#39;,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">palette</span><span class="p">[</span><span class="n">sim_num</span><span class="p">],</span>
        <span class="c1"># alpha=0.7, </span>
        <span class="c1"># label=f&quot;Simulation {sim_num + 1}&quot;,</span>
    <span class="p">)</span>

<span class="c1"># Add labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
    <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;α= 0.05&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sample size (per group)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;P value (log scale)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Impact of increasing incrementally sample size on P values</span><span class="se">\n</span><span class="s2"> </span><span class="se">\</span>
<span class="s2">(</span><span class="si">{</span><span class="n">num_simulations</span><span class="si">}</span><span class="s2"> simulations) - null hypothesis true: no mean difference&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">sample_size_max</span> <span class="o">+</span> <span class="mi">5</span><span class="p">))</span>
<span class="c1"># plt.grid(axis=&#39;y&#39;, linestyle=&#39;--&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/39a723ae05175c634f476b1acad7643cd7380e94fcce269c59f46f0c41057800.png" src="_images/39a723ae05175c634f476b1acad7643cd7380e94fcce269c59f46f0c41057800.png" />
</div>
</div>
<p>Consider the potentially misleading practice of collecting data until a statistically significant result is achieved, then abruptly stopping the experiment. This approach capitalizes on the inherent randomness of sampling. If data collection were to continue beyond the initial “significant” finding, the P value might fluctuate back into the non-significant range as new data points are added.</p>
<p>This is evident with the first line in our previous simulation (where the curve rises between 13 and 14). However, for other samples, the rise of the P value back into non-significance could occur later or even never happen, leaving a false sense of confidence in the results. In real-world research, this fluctuation would often remain hidden due to the premature halt of data collection.</p>
<p>This dynamic adjustment of sample size, known as <strong>sequential data analysis</strong>, introduces <strong>bias</strong> and inflates the rate of <strong>false positives</strong> (<strong>type I errors</strong>). It’s a statistical illusion, creating the misleading impression of a significant effect where none truly exists.</p>
</section>
</section>
<section id="confidence-intervals-and-p-values">
<h2>Confidence intervals and P values<a class="headerlink" href="#confidence-intervals-and-p-values" title="Link to this heading">#</a></h2>
<p>Confidence intervals offer a valuable alternative to P values for assessing statistical significance. They provide a <em>range of plausible values for the true population parameter</em> (e.g., mean difference, effect size) with a specified level of confidence (typically 95%). Unlike P values, which focus solely on rejecting or failing to reject a null hypothesis, confidence intervals offer a more informative picture of the magnitude and uncertainty of an effect.</p>
<p>There’s a direct relationship between confidence intervals and P values, particularly for two-sided tests:</p>
<ul class="simple">
<li><p><strong>95% CI excludes the null value</strong>: if the 95% confidence interval for a parameter (e.g., mean difference) does not include the null hypothesis value (often 0 for differences or 1 for ratios), then the corresponding P value will be less than 0.05, indicating a <strong>statistically significant</strong> result.</p></li>
<li><p><strong>95% CI includes the null value</strong>: conversely, if the 95% confidence interval does contain the null hypothesis value, the P value will be greater than or equal to 0.05, suggesting that the results are <strong>not statistically significant</strong>. For example:</p>
<ul>
<li><p>Comparing means: if the 95% CI for the difference between two means does not include zero, we can conclude a statistically significant difference exists between the groups (P &lt; 0.05).</p></li>
<li><p>Comparing proportions: if the 95% CI for the ratio of two proportions does not include 1.0, we can conclude a statistically significant difference exists (P &lt; 0.05).</p></li>
<li><p>Comparing percentages: if we’re testing whether a set of percentages differs from a hypothesized value of 100, and the 95% CI of the mean of the percentages excludes 100, then the discrepancy is statistically significant.</p></li>
</ul>
</li>
</ul>
<p>Confidence intervals offer several advantages over P values alone:</p>
<ul class="simple">
<li><p>Effect size estimation: CIs provide an estimate of the magnitude of the effect, not just whether it’s statistically significant.</p></li>
<li><p>Precision assessment: the width of the CI reflects the precision of the estimate. Narrower CIs indicate more precise estimates.</p></li>
<li><p>Decision relevance: CIs help assess the practical significance of the effect by showing the range of plausible values.</p></li>
</ul>
<p>In biostatistics and many other fields, it’s increasingly recommended to report confidence intervals alongside P values. This provides a more comprehensive and informative view of our findings, allowing for better decision-making based on both statistical significance and the estimated magnitude of the effect.</p>
</section>
<section id="statistical-significance-and-hypothesis-testing">
<h2>Statistical significance and hypothesis testing<a class="headerlink" href="#statistical-significance-and-hypothesis-testing" title="Link to this heading">#</a></h2>
<p>Statistical hypothesis testing offers a standardized framework for drawing conclusions from data. At its core, this approach streamlines the complex process of interpreting results by distilling findings into a <strong>binary outcome</strong>: <em>statistically significant</em> or <em>not statistically significant</em>. This automation of decision-making can be both a strength and a weakness. In this section, we’ll delve deeper into the principles of hypothesis testing, exploring how it simplifies decision-making, the criteria it relies upon, and the potential pitfalls that arise from oversimplification. We’ll also discuss when and how hypothesis testing can be a valuable tool in the biostatistical arsenal.</p>
<section id="type-i-and-type-ii-errors">
<h3>Type I and type II errors<a class="headerlink" href="#type-i-and-type-ii-errors" title="Link to this heading">#</a></h3>
<p>The binary nature of statistical hypothesis testing (reject H0 or do not reject H0) creates a landscape of potential errors, as our decisions are made under uncertainty. These errors are classified into two distinct types, each with its own consequences:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Reject H0</p></th>
<th class="head"><p>Do not reject H0</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>H0 is true</p></td>
<td><p>type I error</p></td>
<td><p>(no error)</p></td>
</tr>
<tr class="row-odd"><td><p>H0 is false</p></td>
<td><p>(no error)</p></td>
<td><p>type II error</p></td>
</tr>
</tbody>
</table>
</div>
<ul class="simple">
<li><p><strong>Type I error</strong> (<strong>false positive</strong>): this occurs when we reject the null hypothesis (H0) even though it’s actually true. In other words, we conclude there’s a significant effect when, in reality, it’s due to chance. Think of this as a “false alarm”. In everyday life, we might mistakenly flag a legitimate email as spam or, more seriously, wrongly convict someone of a crime they didn’t commit.</p></li>
<li><p><strong>Type II error</strong> (<strong>false negative</strong>): this happens when we fail to reject the null hypothesis (H0) when it’s actually false. This means we miss a real effect and incorrectly conclude there’s no significant difference. Consider this a “missed opportunity.” In everyday life, we might mistakenly let a spam email slip into our inbox or, more seriously, wrongly acquit someone who actually committed a crime.</p></li>
</ul>
<p>A third type of error, known as a <strong>type S</strong> or <strong>type III error</strong>, occurs when the direction of the effect is incorrectly concluded. In other words, we might find a statistically significant difference, but the direction of that difference is the <em>opposite</em> of what is actually true. Imagine a study that aims to determine if a new drug lowers blood pressure. A type S error would occur if the study concludes that the drug raises blood pressure when it actually lowers it, even though the statistical test shows a significant difference.</p>
</section>
<section id="interpreting-statistical-significance">
<h3>Interpreting statistical significance<a class="headerlink" href="#interpreting-statistical-significance" title="Link to this heading">#</a></h3>
<section id="the-significance-level">
<h4>The significance level<a class="headerlink" href="#the-significance-level" title="Link to this heading">#</a></h4>
<p>When a statistical test yields a “significant” result, it means the observed effect is unlikely to have occurred by chance alone, given a pre-specified threshold of improbability. This threshold is known as the significance level, denoted by the Greek letter alpha (α).</p>
<p>Typically, researchers set alpha at 0.05, meaning there’s a 5% chance of incorrectly rejecting the null hypothesis (H0) when it’s actually true (a type I error). A P value less than alpha leads to rejecting H0, while a P value greater than or equal to alpha leads to failing to reject H0.</p>
<p>While a significant result (P &lt; α) suggests an effect is unlikely due to chance, it doesn’t guarantee the effect is:</p>
<ul class="simple">
<li><p>Large: a small effect can be statistically significant with a large enough sample size.</p></li>
<li><p>Important: statistical significance doesn’t equate to practical or clinical significance.</p></li>
<li><p>Accurate: a significant result could still be a type I error.</p></li>
</ul>
<p>Now, we can ask ourselves:</p>
<ul class="simple">
<li><p>Assuming the null hypothesis is true, what is the probability of obtaining a P value that would lead us to incorrectly reject the null hypothesis?</p></li>
<li><p>If we repeatedly conduct experiments where there is no true effect, what proportion of these experiments will mistakenly lead us to conclude that there is a significant effect?</p></li>
</ul>
<p>In practical terms, α can also be understood through repeated experimentation. Imagine conducting numerous experiments where the null hypothesis (H0) is true. Let <span class="math notranslate nohighlight">\(A\)</span> represent the number of times we incorrectly reject H0 (type I errors), and let <span class="math notranslate nohighlight">\(B\)</span> represent the number of times we correctly fail to reject H0:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head text-center"><p>Reject H0</p></th>
<th class="head text-center"><p>Do not reject H0</p></th>
<th class="head text-center"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>H0 is true</p></td>
<td class="text-center"><p>A (type I)</p></td>
<td class="text-center"><p>B</p></td>
<td class="text-center"><p>A + B</p></td>
</tr>
<tr class="row-odd"><td><p>H0 is false</p></td>
<td class="text-center"><p>C</p></td>
<td class="text-center"><p>D (type II)</p></td>
<td class="text-center"><p>C + D</p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td class="text-center"><p>A + C</p></td>
<td class="text-center"><p>B + D</p></td>
<td class="text-center"><p>A+B+C+D</p></td>
</tr>
</tbody>
</table>
</div>
<p>The significance level only considers analyses where <em>H0 is true</em> (first row of the table). Of all experiments (A+B), the number of times H0 is rejected equals A, so that:</p>
<div class="math notranslate nohighlight">
\[\alpha = \frac{A}{A+B}\]</div>
<p>In other words, α is the proportion of experiments where we mistakenly reject H0 when it’s actually true. This highlights a key point: even when there is no real effect, a certain percentage of studies will yield statistically significant results simply due to random chance.</p>
</section>
<section id="false-positive-report-probability-fprp">
<h4>False positive report probability (FPRP)<a class="headerlink" href="#false-positive-report-probability-fprp" title="Link to this heading">#</a></h4>
<p>While α focuses on the risk of a type I error within a single experiment, the <strong>false positive report probability (FPRP)</strong> takes a broader perspective. It considers the probability that a statistically significant finding in a research report actually represents a false positive result.</p>
<p>The FPRP acknowledges that research doesn’t exist in isolation. In practice, multiple studies are often conducted to investigate a particular phenomenon. Some of these studies might yield significant results by chance (type I errors). The FPRP asks: given a significant finding in a published report, what’s the probability that this result is actually a false positive?</p>
<p>The FPRP only considers analyses that <strong>reject H0</strong> (first column of the table). Of all these studies that report a statistically significant result (reject H0) A+C, and with C the number of studies that report a significant result and where H0 is truly false (true positive), the number in which H0 is true equals A, therefore:</p>
<div class="math notranslate nohighlight">
\[\text{FPRP} = \frac{A}{A+C}\]</div>
<p>In other words, the FPRP is the proportion of significant results that are actually false positives.</p>
</section>
</section>
<section id="prior-probability-influences-fprp">
<h3>Prior probability influences FPRP<a class="headerlink" href="#prior-probability-influences-fprp" title="Link to this heading">#</a></h3>
<p>The FPRP isn’t a fixed value; it varies depending on several factors, including the significance level (α) and statistical power of the study. However, an often overlooked but critical factor is the <strong>prior probability</strong> of a true effect. This reflects our <em>existing knowledge</em> or <em>belief</em> about the likelihood of the phenomenon under investigation before we even conduct the experiment.</p>
<p>Intuitively, if we’re testing a hypothesis that seems highly implausible based on prior research or scientific understanding, even a statistically significant result should be met with greater skepticism. In contrast, a significant finding for a well-established phenomenon is more likely to be a true positive.</p>
<p>The table below illustrates how the FPRP changes depending on the prior probability of a true effect, assuming a fixed significance level and power.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Prior probability</p></th>
<th class="head"><p>FPRP as P&lt;0.05</p></th>
<th class="head"><p>FPRP as 0.045&lt;P&lt;0.05</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0%</p></td>
<td><p>100%</p></td>
<td><p>100%</p></td>
</tr>
<tr class="row-odd"><td><p>1%</p></td>
<td><p>86%</p></td>
<td><p>97%</p></td>
</tr>
<tr class="row-even"><td><p>10%</p></td>
<td><p>36%</p></td>
<td><p>78%</p></td>
</tr>
<tr class="row-odd"><td><p>50%</p></td>
<td><p>5.9%</p></td>
<td><p>27%</p></td>
</tr>
<tr class="row-even"><td><p>100%</p></td>
<td><p>0%</p></td>
<td><p>0%</p></td>
</tr>
</tbody>
</table>
</div>
<p>Let’s explore these scenarios to understand the nuanced relationship between FPRP and the broader scientific context.</p>
<section id="prior-probability-of-1">
<h4>Prior probability of 1%<a class="headerlink" href="#prior-probability-of-1" title="Link to this heading">#</a></h4>
<p>Consider a scenario where we’re testing 1,000 new drug compounds, but prior research indicates that the probability of any single drug being successful is a mere 1%. What can we expect in terms of both true and false positive findings?</p>
<p><em>We need to consider not just the risk of false positives, but also the chance of missing a truly effective drug (a false negative). This is where the concept of <strong>power</strong> comes in. We will discuss it in more details in another section, but briefly, power is the probability that our experiment will correctly identify a truly effective drug as significant. In this case, with a 1% prior probability, a high-powered study is crucial. If our study has low power, we might miss out on most of the effective drugs, even if they exist.</em></p>
<ul class="simple">
<li><p>Of 1000 drugs screened we expect 10 (1%) that really work</p></li>
<li><p>Of the 10 drugs that really work we expect to obtain a statistically significant result in 8 (80 % power)</p></li>
<li><p>Of the 990 drugs that are really ineffective we expect to obtain a statistically significant result in 5% (α set to 0.05), i.e. 0.05 * 990 = 49 false positive</p></li>
<li><p>Of 1000 tests of different drugs we therefore expect to obtain statistically significant difference in 8+49=57</p></li>
<li><p>The FPRP equals 49/57=86%</p></li>
</ul>
<div class="highlight-markdown notranslate"><div class="highlight"><pre><span></span>            8 have P&lt;0.05 (80% power)
           /
       10 work (prior probability=50%)
      /    \
     /      2 have P&gt;0.05
1000 drugs
     \          49 have P&lt;0.05 (5% significance level)
      \        /
       990 don&#39;t work
               \
                941 have P&gt;0.05
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head text-center"><p>Statistically significant</p></th>
<th class="head text-center"><p>Not significant</p></th>
<th class="head text-center"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Drug is ineffective</p></td>
<td class="text-center"><p>49</p></td>
<td class="text-center"><p>941</p></td>
<td class="text-center"><p>990</p></td>
</tr>
<tr class="row-odd"><td><p>Drug is effective</p></td>
<td class="text-center"><p>8</p></td>
<td class="text-center"><p>2</p></td>
<td class="text-center"><p>10</p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td class="text-center"><p>57</p></td>
<td class="text-center"><p>943</p></td>
<td class="text-center"><p>1000</p></td>
</tr>
</tbody>
</table>
</div>
<p>With such a low prior probability of success (1%), conducting this experiment with a conventional significance level of 5% (α= 0.05) is likely to yield a high rate of false positives. This means that many of the drugs identified as ‘significant’ would actually be ineffective. To ensure the validity and usefulness of this drug screening, a much stricter significance level, such as 0.1% (α = 0.001), may be necessary. This stricter threshold would reduce the risk of falsely identifying ineffective drugs as promising candidates, but it would also increase the risk of missing truly effective drugs (type II errors). Balancing these risks requires careful consideration of the specific context, costs, and potential benefits of the drug screening program.</p>
</section>
<section id="prior-probability-of-50">
<h4>Prior probability of 50%<a class="headerlink" href="#prior-probability-of-50" title="Link to this heading">#</a></h4>
<p>Even perfectly performed experiments are less reproducible than most expect, and many statistically significant results are false positives in situations where false positive results are likely, e.g. with low prior probability in observational studies or when multiple comparisons are made. In contrast, when the prior probability of a true effect is higher, such as 50%, the likelihood of a significant result being a true positive increases substantially, with here the FPRP equals 25/425 = 5.9%, even if the study is underpowered:</p>
<div class="highlight-markdown notranslate"><div class="highlight"><pre><span></span>             400 have P&lt;0.05 (80% power)
            /
       500 work (prior probability=50%)
      /     \
     /       100 have P&gt;0.05
1000 drugs
     \          25 have P&lt;0.05 (5% significance level)
      \        /
       500 don&#39;t work
               \
                475 have P&gt;0.05
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head text-center"><p>Statistically significant</p></th>
<th class="head text-center"><p>Not significant</p></th>
<th class="head text-center"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Drug is ineffective</p></td>
<td class="text-center"><p>25</p></td>
<td class="text-center"><p>475</p></td>
<td class="text-center"><p>500</p></td>
</tr>
<tr class="row-odd"><td><p>Drug is effective</p></td>
<td class="text-center"><p>400</p></td>
<td class="text-center"><p>100</p></td>
<td class="text-center"><p>500</p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td class="text-center"><p>425</p></td>
<td class="text-center"><p>943</p></td>
<td class="text-center"><p>1000</p></td>
</tr>
</tbody>
</table>
</div>
<p>Furthermore, if an experiment has low power to find the difference/effect we are looking for, we are likely to end up with a result that is not statistically significant even if the effect is real. And if the low power study does reach a conclusion that the effect is statistically significant, the results are hard to interpret because the FPRP will be high, and the effect size observed in that study is likely to be larger than the actual effect size because only large observed effects (even if due to chance) will yield a P value less than 0.05. Experiments designed with low power cannot be very informative, and it should not be surprising when they cannot be reproduced.</p>
</section>
</section>
</section>
<section id="the-challenge-of-multiple-comparisons">
<h2>The challenge of multiple comparisons<a class="headerlink" href="#the-challenge-of-multiple-comparisons" title="Link to this heading">#</a></h2>
<p>When conducting multiple statistical tests within a study, the probability of encountering at least one false positive result increases significantly. This is known as the multiple comparisons problem.</p>
<p>Each statistical test carries an inherent risk of a type I error (false positive), and conducting multiple tests compounds this risk. Think of it like rolling the dice multiple times - the more you roll, the higher the chance of getting a “lucky” (but misleading) significant result.</p>
<p>The multiple comparisons problem can lead to false discoveries and unreliable conclusions. In this section, we’ll explore the causes and consequences of this issue, and we’ll introduce statistical methods to mitigate the inflated risk of false positives.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>  <span class="c1"># Number of comparisons (1 to 59)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.95</span><span class="o">**</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Probability of at least one significant result (%)</span>

<span class="c1"># Plot</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
    <span class="mi">5</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;α = 0.05&quot;</span><span class="p">)</span>  <span class="c1"># Add alpha level line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="s2">&quot;Probability of obtaining ≥ 1 statistically significant result by chance&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of comparisons&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Probability (%)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9b24c9cb9dc20b86b59c69161cefb8e033ee73210e07f003e92e8dc450448645.png" src="_images/9b24c9cb9dc20b86b59c69161cefb8e033ee73210e07f003e92e8dc450448645.png" />
</div>
</div>
<p>This plot illustrate the concept of the multiple comparisons problem. The x-axis represents the number of comparisons made in a study, while the y-axis shows the probability of obtaining at least one statistically significant result purely by chance.</p>
<p>As we can see, even with a single comparison (x=1) and an alpha level of 0.05, there’s a 5% chance of getting a false positive. As the number of comparisons increases, this probability rapidly rises. For instance, with just 14 comparisons, the probability of at least one false positive exceeds 50%!</p>
<section id="bonferroni-correction">
<h3>Bonferroni correction<a class="headerlink" href="#bonferroni-correction" title="Link to this heading">#</a></h3>
<p>The simplest approach to controlling the familywise error rate (FWER) is the <strong>Bonferroni correction</strong>, which involves dividing the desired overall significance level (<span class="math notranslate nohighlight">\(\alpha\)</span>) by the number of comparisons <span class="math notranslate nohighlight">\(K\)</span>: <span class="math notranslate nohighlight">\(\alpha_\text{adjusted} = \alpha / K\)</span>. This adjustment ensures that if all the null hypotheses are true, the probability of observing at least one false positive (type I error) among all K comparisons is at most α.</p>
<p>The Bonferroni correction does not guarantee exactly a 95% chance of seeing no significant results if all null hypotheses are true. It provides a more conservative upper bound on this probability, making it less likely to make any false discoveries. However, this comes at the cost of reduced power to detect true effects (increased risk of type II errors).</p>
</section>
<section id="false-discovery-rate-fdr">
<h3>False discovery rate (FDR)<a class="headerlink" href="#false-discovery-rate-fdr" title="Link to this heading">#</a></h3>
<p>The <strong>false discovery rate (FDR)</strong> offers a more flexible alternative to the Bonferroni correction for addressing multiple comparisons. Instead of strictly controlling the probability of any false positives, the FDR aims to control the proportion of false positives among the discoveries we make. The FDR addresses these key questions:</p>
<ul class="simple">
<li><p>If a comparison is deemed significant (P value below a threshold), what’s the chance it’s actually a false positive (H0 is true)?</p></li>
<li><p>Of all significant findings, what fraction are expected to be false positives?</p></li>
</ul>
<p>The desired FDR, often denoted as Q, is the acceptable proportion of false discoveries. For example, with Q = 10%, we aim for at least 90% of our significant results to be true positives.</p>
<p>A widely used method to control the FDR is the <strong>Benjamini-Hochberg</strong> procedure. It sets dynamic thresholds for each comparison, depending on its rank among the P values. For example, if we want an FDR of 5% across 100 comparisons, we would:</p>
<ol class="arabic simple">
<li><p>Rank P values: order the P values from smallest to largest, i.e., <span class="math notranslate nohighlight">\(P(1), P(2), ..., P(100)\)</span></p></li>
<li><p>Calculate Thresholds: for each P value <span class="math notranslate nohighlight">\(P(i)\)</span>, calculate a threshold <span class="math notranslate nohighlight">\(T(i)\)</span> using this formula <span class="math notranslate nohighlight">\(T(i) = (i / m) \times Q\)</span>, with <span class="math notranslate nohighlight">\(i\)</span> the rank of the P value, i.e., <span class="math notranslate nohighlight">\(1, 2, ..., 100\)</span>, and <span class="math notranslate nohighlight">\(m\)</span> the total number of comparisons, i.e., <span class="math notranslate nohighlight">\(100\)</span></p></li>
<li><p>Compare and Decide: if <span class="math notranslate nohighlight">\(P(i) \leq T(i)\)</span>, declare the comparison as significant, if <span class="math notranslate nohighlight">\(P(i) \gt T(i)\)</span>, declare the comparison as not significant</p></li>
</ol>
<p>For example, for the smallest P value <span class="math notranslate nohighlight">\(i = 1\)</span>, <span class="math notranslate nohighlight">\(T(1) = (1 / 100) \times 0.05 = 0.0005\)</span>, so, the smallest P value would be considered significant if it’s less than or equal to 0.0005. The second smallest P value’s threshold would be <span class="math notranslate nohighlight">\((2/100) \times 0.05 = 0.001\)</span>. For the largest P value (<span class="math notranslate nohighlight">\(i = 100\)</span>), <span class="math notranslate nohighlight">\(T(100) = (100 / 100) \times 0.05 = 0.05\)</span>, so the largest P value is compared directly to the overall alpha level (0.05).</p>
<p>This approach ensures that, on average, no more than 5% of the significant results we identify will be false positives.</p>
</section>
</section>
<section id="testing-for-equivalence-or-noninferiority">
<h2>Testing for equivalence or noninferiority<a class="headerlink" href="#testing-for-equivalence-or-noninferiority" title="Link to this heading">#</a></h2>
<p>Traditional hypothesis testing focuses on demonstrating a significant difference between treatments. However, in certain contexts, like generic drug development, the goal shifts towards demonstrating equivalence or noninferiority compared to a standard drug.</p>
<section id="bioequivalence">
<h3>Bioequivalence<a class="headerlink" href="#bioequivalence" title="Link to this heading">#</a></h3>
<p>The U.S. Food and Drug Administration (FDA) defines two drug formulations as <strong>bioequivalent</strong> if the 90% confidence interval (CI) of the ratio of peak concentrations (Cmax) and total drug exposure (AUC) in blood plasma falls entirely within the range of <strong>0.80 to 1.25</strong> (note that the reciprocal of 80% is 1/0.8 = 125%). This means the generic drug’s performance is expected to be within 20% of the reference drug’s performance in most cases, a difference usually considered clinically insignificant.</p>
<p>Using a 90% confidence interval (CI) for each of the two one-sided tests results in an overall 95% confidence level for the equivalence conclusion. This might seem counterintuitive, but it’s a result of how the two tests and their associated CIs combine.</p>
</section>
<section id="two-one-sided-tests-tost">
<h3>Two one-sided tests (TOST)<a class="headerlink" href="#two-one-sided-tests-tost" title="Link to this heading">#</a></h3>
<p>Standard hypothesis testing (null hypothesis significance testing or NHST) is designed to detect differences between groups or treatments. However, in equivalence testing, the goal is to demonstrate that two treatments are not meaningfully different, i.e., that they fall within a pre-defined range of equivalence.</p>
<p>The problem with applying NHST directly to equivalence testing is that failing to reject the null hypothesis (H0: no difference) doesn’t prove equivalence. It could simply mean our study lacked sufficient power to detect a small difference. The TOST procedure cleverly adapts NHST to the equivalence framework by posing <strong>two separate null hypotheses</strong>:</p>
<ol class="arabic simple">
<li><p>H01: the true mean ratio is less than or equal to the lower equivalence bound (0.80 in the FDA example).</p></li>
<li><p>H02: the true mean ratio is greater than or equal to the upper equivalence bound (1.25 in the FDA example).</p></li>
</ol>
<p>By conducting two <strong>one-sided tests</strong>, each with a significance level of 0.05, we essentially create two “null hypotheses of non-equivalence.”</p>
<p>Now, how do we interpret TOST results?</p>
<ul class="simple">
<li><p>Reject both null hypotheses: if we reject both H01 and H02 (i.e., both one-sided P values are less than 0.05), we conclude that the true mean ratio lies within the equivalence interval with 90% confidence. This is the desired outcome for demonstrating equivalence.</p></li>
<li><p>Fail to reject either null hypothesis: if we fail to reject either H01 or H02 (P value &gt;= 0.05), we cannot conclude equivalence. This doesn’t necessarily mean the treatments are not equivalent; it could be due to insufficient statistical power.</p></li>
</ul>
</section>
<section id="interpreting-equivalence-and-noninferiority-tests">
<h3>Interpreting equivalence and noninferiority tests<a class="headerlink" href="#interpreting-equivalence-and-noninferiority-tests" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Equivalence</strong>: when the entire 90% CI lies within the 0.80-1.25 equivalence zone, the generic drug is deemed equivalent to the standard drug.</p></li>
<li><p><strong>Non-equivalence</strong>: if the 90% CI falls completely outside the equivalence zone, the drugs are considered not equivalent.</p></li>
<li><p><strong>Inconclusive results</strong>: a 90% CI that straddles the equivalence zone, i.e., partially inside, partially outside, means the data are inconclusive regarding equivalence.</p></li>
<li><p><strong>Noninferiority</strong>: noninferiority trials aim to demonstrate that a new treatment is not unacceptably worse than an existing treatment. This is done by specifying a pre-defined margin of inferiority Δ, which represents the largest clinically acceptable difference between the new and existing treatment. This margin is not always 0.80 and it’s carefully chosen based on factors like the clinical importance of the effect of the existing treatment, the severity of the disease, and potential risks and benefits of the new treatment. To conclude non-inferiority, the entire 90% CI for the comparison between the new and existing treatments must lie above the pre-defined non-inferiority margin.</p></li>
</ul>
<p><img alt="noninferiority testing" src="_images/bioequivalence.svg" /></p>
<p>There is also a nice interactive visualization of <a class="reference external" href="https://rpsychologist.com/d3/equivalence/">equivalence, non-inferiority and superiority testing</a>, where we can play with the effect size, the sample, size and the margin and see how these parameters affect equivalence.</p>
<p>Equivalence and non-inferiority testing present unique challenges and require specialized methods beyond the scope of this jupyter-book, so that consulting with a statistician familiar with the relevant regulatory guidelines is strongly recommended.</p>
</section>
</section>
<section id="assessing-diagnostic-accuracy">
<h2>Assessing diagnostic accuracy<a class="headerlink" href="#assessing-diagnostic-accuracy" title="Link to this heading">#</a></h2>
<p>Statistical significance is crucial for determining if an effect exists, but it doesn’t tell the whole story when evaluating the <strong>performance</strong> of diagnostic tests or classification models. In the realm of medical diagnosis, epidemiology, and machine learning, we need metrics that assess how well a test or model can distinguish between different conditions or classes.</p>
<p>Sensitivity, specificity, and receiver operating characteristic (ROC) curves provide a powerful framework for quantifying and visualizing diagnostic <strong>accuracy</strong>. They offer insights into the trade-offs between true positive and false positive rates, allowing us to select optimal decision thresholds and compare the performance of different tests or models.</p>
<section id="definitions">
<h3>Definitions<a class="headerlink" href="#definitions" title="Link to this heading">#</a></h3>
<p>Deciding whether a clinical laboratory result is normal or abnormal involves a similar logic to statistical hypothesis testing, but with a focus on diagnostic accuracy:</p>
<ul class="simple">
<li><p><strong>False negative (FN)</strong>: a test result is classified as negative (normal) when the patient actually has the disease.</p></li>
<li><p><strong>False positive (FP)</strong>: a test result is classified as positive (abnormal) when the patient actually does not have the disease.</p></li>
</ul>
<p>The accuracy of a diagnostic test is assessed by two fundamental metrics:</p>
<ul class="simple">
<li><p><strong>Sensitivity</strong>, also commonly referred to as <strong>recall</strong>, <strong>hit rate</strong> or <strong>True Positive Rate (TPR)</strong>, quantifies how well the test <em>identifies individuals with the disease</em>. It’s the proportion of true positives (those who have the disease and test positive; TP) out of all individuals with the disease. Mathematically, it’s represented as <span class="math notranslate nohighlight">\(\text{sensitivity}=\frac{\text{TP}}{\text{TP}+\text{FN}}\)</span>, where FN represents false negatives (those with the disease who test negative).</p></li>
<li><p><strong>Specificity</strong>, also commonly referred to as <strong>selectivity</strong> or <strong>True Negative Rate (TNR)</strong>, quantifies how well the test <em>identifies individuals without the disease</em>. It’s the proportion of true negatives (those who don’t have the disease and test negative; TN) out of all individuals without the disease. Mathematically, it’s represented as: <span class="math notranslate nohighlight">\(\text{specificity}=\frac{\text{TN}}{\text{TN}+\text{FP}}\)</span>, where FP represents false positives (those without the disease who test positive).</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head text-center"><p>Disease present (true)</p></th>
<th class="head text-center"><p>Disease absent (false)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Test positive</p></td>
<td class="text-center"><p>true positive (TP)</p></td>
<td class="text-center"><p>false positive (FP)</p></td>
</tr>
<tr class="row-odd"><td><p>Test negative</p></td>
<td class="text-center"><p>false negative (FN)</p></td>
<td class="text-center"><p>true negative (TN)</p></td>
</tr>
</tbody>
</table>
</div>
<p>Test <strong>accuracy</strong>, sometimes abbreviated as <strong>ACC</strong>, is a measure of how often a diagnostic test correctly classifies individuals as having or not having a particular disease or condition. It’s the proportion of true results (both true positives and true negatives) out of all test results: <span class="math notranslate nohighlight">\(\text{accuracy}=\frac{\text{TP}+\text{TN}}{\text{TP}+\text{FP}+\text{TN}+\text{FN}}\)</span>.</p>
<p>While accuracy is a useful overall metric, it can be misleading when dealing with <em>imbalanced datasets</em>, where the number of samples in one class is significantly higher than the other. In such cases, a classifier could achieve high accuracy by simply predicting the majority class most of the time, even if it performs poorly on the minority class. <strong>Balanced accuracy (BA)</strong> addresses this issue by taking the average of sensitivity (true positive rate) and specificity (true negative rate): <span class="math notranslate nohighlight">\(\text{BA}=\frac{\text{TPR}+\text{TNR}}{2}\)</span>.</p>
<p>While sensitivity and specificity are fundamental metrics, they don’t directly answer the questions that matter most to patients and clinicians:</p>
<ul class="simple">
<li><p>If a test result is positive, what is the probability that the patient actually has the disease?</p></li>
<li><p>If a test result is negative, what is the probability that the patient truly does not have the disease?</p></li>
</ul>
<p>These questions are addressed by two crucial measures of diagnostic accuracy:</p>
<ul class="simple">
<li><p><strong>Positive Predictive Value (PPV)</strong>, also commonly referred to as <strong>precision</strong>, is the probability that a positive test result correctly indicates the presence of the disease, i.e., <span class="math notranslate nohighlight">\(\text{PPV}=\frac{\text{TP}}{\text{TP}+\text{FP}}\)</span>.</p></li>
<li><p><strong>Negative Predictive Value (NPV)</strong>: the probability that a negative test result correctly indicates the absence of the disease., i.e., <span class="math notranslate nohighlight">\(\text{NPV}=\frac{\text{TN}}{\text{TN}+\text{FN}}\)</span>.</p></li>
</ul>
<p>While the Benjamini-Hochberg procedure offers a valuable method for controlling the false discovery rate in multiple hypothesis testing, the general concept of FDR extends beyond this specific method. In the context of diagnostic accuracy, we can also define the FDR as the proportion of positive test results that are actually incorrect (false positives) Mathematically, the <strong>False Discovery Rate (FDR)</strong> in diagnostic testing can be expressed as <span class="math notranslate nohighlight">\(\text{FDR}=\frac{\text{FP}}{\text{FP}+\text{TP}}\)</span>. The FDR is the complement of precision: <span class="math notranslate nohighlight">\(\text{PPV} = 1 - \text{FDR}\)</span>.</p>
<p>Similarly, the <strong>false positive Rate (FPR)</strong>, sometimes called the “fall-out” or “false alarm rate”, is the probability of a positive test result (or a positive prediction by a model) when the condition or event being tested for is not actually present, i.e., <span class="math notranslate nohighlight">\(\text{FDR}=\frac{\text{FP}}{\text{FP}+\text{TN}}\)</span>. The FPR is the complement of specificity: <span class="math notranslate nohighlight">\(\text{TNR}=1-\text{FDR}\)</span>.</p>
<p>Finally, while sensitivity and specificity are crucial, they often paint an incomplete picture of a test’s <strong>overall performance</strong>, especially in situations with imbalanced class distributions (e.g., when the disease is rare). The <strong>F1-score</strong> addresses this by <em>combining both precision (positive predictive value) and recall (sensitivity)</em> into a single metric. The F1-score is the <em>harmonic mean of precision and recall</em>. It’s a balanced measure that gives equal weight to both the ability to correctly identify positive cases (recall) and the ability to avoid false positives (precision), i.e., <span class="math notranslate nohighlight">\(\text{F1-score}=2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}\)</span>.</p>
</section>
<section id="roc-curves">
<h3>ROC curves<a class="headerlink" href="#roc-curves" title="Link to this heading">#</a></h3>
<section id="decision-thresholds">
<h4>Decision thresholds<a class="headerlink" href="#decision-thresholds" title="Link to this heading">#</a></h4>
<p>Sensitivity and specificity provide valuable insights into a test’s performance at a fixed threshold. However, in practice, the optimal <strong>threshold</strong> for a diagnostic test can vary depending on the clinical setting and the consequences of different types of errors (false positives and false negatives).</p>
<p>Receiver Operating Characteristic (ROC) curves offer a solution by visualizing the trade-offs between sensitivity and specificity across a range of possible thresholds. An ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.</p>
<img src="https://docs.eyesopen.com/toolkits/cookbook/python/_images/roc2img.svg" alt="Example of a simple ROC curve" style="width: 400px;"/>
<p>An ROC curve is created by plotting the true positive rate (<em>sensitivity</em>) on the y-axis against the false positive rate (<em>one-minus-specificity</em>) on the x-axis for <em>various decision thresholds</em> of a test. The resulting curve illustrates the relationship between <em>sensitivity and specificity</em> as the threshold changes. A perfect test would have a curve that hugs the top-left corner of the plot (high sensitivity and high specificity at all thresholds). A random guess would result in a diagonal line (no better than chance). At one extreme (bottom-left), the test is overly <em>conservative</em>, never producing a positive result, even for true cases. This translates to zero sensitivity but perfect specificity. At the other extreme (upper-right), the test is overly <em>liberal</em>, always yielding a positive result, regardless of the true condition. This results in 100% sensitivity but zero specificity.</p>
<p>The choice of threshold for a diagnostic test significantly affects its sensitivity and specificity, creating an inherent trade-off:</p>
<ul class="simple">
<li><p>High threshold: setting a high threshold (requiring stronger evidence for a positive result) leads to:</p>
<ul>
<li><p>Low sensitivity: the test is less likely to detect true cases of the disease, resulting in more false negatives.</p></li>
<li><p>High specificity: the test is more likely to correctly identify those without the disease, resulting in fewer false positives.
-Low threshold: setting a low threshold (requiring less evidence for a positive result) leads to:</p></li>
<li><p>High sensitivity: the test is more likely to detect true cases of the disease, minimizing false negatives.</p></li>
<li><p>Low specificity: the test is less likely to correctly identify those without the disease, leading to more false positives.</p></li>
</ul>
</li>
</ul>
<p>The optimal threshold depends on the specific clinical context and the relative consequences of false negatives and false positives. For example, screening tests for serious diseases often use a lower threshold to maximize sensitivity (catching as many cases as possible), even if it leads to more false positives that require further testing.</p>
</section>
<section id="area-under-the-curve">
<h4>Area under the curve<a class="headerlink" href="#area-under-the-curve" title="Link to this heading">#</a></h4>
<p>The <strong>Area under the ROC curve (AUC)</strong> is a single metric summarizing a diagnostic/classifier’s overall ability to discriminate between positive and negative classes across all possible thresholds.:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{AUC} = 1.0\)</span>: this represents a <strong>perfect</strong> classifier, one that can flawlessly distinguish between positive and negative cases across all thresholds.</p></li>
<li><p><span class="math notranslate nohighlight">\(0.5 &lt; \text{AUC} &lt; 1.0\)</span>: a classifier with an AUC greater than 0.5 is considered a <strong>good</strong> classifier, as it performs better than random chance. The closer the AUC is to 1.0, the better the classifier’s ability to discriminate between classes.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{AUC} = 0.5\)</span>: an AUC of 0.5 indicates that the classifier is no better than <strong>random</strong> guessing. It’s essentially a coin flip whether the classifier will make a correct prediction.</p></li>
<li><p><span class="math notranslate nohighlight">\(0.0 &lt; \text{AUC} &lt; 0.5\)</span>: a classifier with an AUC less than 0.5 is performing worse than random chance. It’s making <strong>systematic errors</strong> and could potentially be improved by simply inverting its predictions.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{AUC} = 0.0\)</span>: this represents a completely <strong>incorrect</strong> classifier. It consistently makes the wrong prediction for every instance.</p></li>
</ul>
</section>
<section id="roc-and-python">
<h4>ROC and Python<a class="headerlink" href="#roc-and-python" title="Link to this heading">#</a></h4>
<p>In practice, these values (true positives, false positives, etc.) are derived from the results of a diagnostic test or a <strong>machine learning</strong> model applied to a dataset where the true labels are known. For instance, using Python’s <a class="reference external" href="https://scikit-learn.org/stable/index.html"><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> library</a>, we can train and evaluate various machine learning classifiers on labeled data, obtaining the confusion matrix that allows us to compute metrics like the F1-score and <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_det.html#sphx-glr-auto-examples-model-selection-plot-det-py">draw the ROC curve</a>.</p>
<p>While a thorough exploration of machine learning is beyond the scope of this book, it’s important to recognize that these tools offer powerful ways to assess and optimize the performance of diagnostic tests and classification models.</p>
</section>
</section>
<section id="case-studies">
<h3>Case studies<a class="headerlink" href="#case-studies" title="Link to this heading">#</a></h3>
<section id="porphyria-test-in-0-01-prevalence">
<h4>Porphyria test in 0.01% prevalence<a class="headerlink" href="#porphyria-test-in-0-01-prevalence" title="Link to this heading">#</a></h4>
<p>Porphyria is a rare group of disorders that affect the body’s ability to produce heme, a component of hemoglobin. The prevalence is estimated to be 1 in 10,000 individuals. We have a diagnostic test for porphyria with the following characteristics:</p>
<ul class="simple">
<li><p>Sensitivity: 82% (82 out of 100 patients with porphyria will test positive).</p></li>
<li><p>Specificity: 96.3% (963 out of 1000 individuals <em>without</em> porphyria will test negative, 3.7% of individuals <em>without</em> porphyria will test positive).</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-center"><p>Has Porphyria</p></th>
<th class="head text-center"><p>Does not have Porphyria</p></th>
<th class="head text-right"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Positive test</p></td>
<td class="text-center"><p>82</p></td>
<td class="text-center"><p>36,996</p></td>
<td class="text-right"><p>37,078</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Negative test</p></td>
<td class="text-center"><p>18</p></td>
<td class="text-center"><p>962,904</p></td>
<td class="text-right"><p>962,922</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Total</p></td>
<td class="text-center"><p>100</p></td>
<td class="text-center"><p>999,900</p></td>
<td class="text-right"><p>1,000,000</p></td>
</tr>
</tbody>
</table>
</div>
<p>Despite the test’s high sensitivity (82%) and specificity (96.3%), the probability that a patient with a positive (abnormal) test result actually has porphyria is surprisingly low: only 82 out of 37078 individuals, i.e., 0.22%, or approximately 1 in 500. This means that the vast majority (99.8%) of positive results are false positives.</p>
<p>On the other hand, the test is highly reliable for ruling out porphyria. Of those who test negative, a staggering 99.998% truly do not have the disease. This high negative predictive value offers strong reassurance to individuals who receive a negative test result. A negative result on this test provides strong evidence that an individual does not have porphyria.</p>
</section>
<section id="porphyria-test-in-50-prevalence">
<h4>Porphyria test in 50% prevalence<a class="headerlink" href="#porphyria-test-in-50-prevalence" title="Link to this heading">#</a></h4>
<p>The low prevalence of porphyria (1 in 10,000) significantly impacts the test’s predictive value, even with high sensitivity and specificity.</p>
<p>Let’s analyze a scenario where we’re testing whether individuals have siblings, with a prevalence of 50% (meaning half the population has siblings). We’ll consider a sample of 1000 individuals and the same test characteristics (sensitivity = 82%, specificity = 96.3%) to see how the predictive values change compared to the rare disease example.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-center"><p>Has Porphyria</p></th>
<th class="head text-center"><p>Does not have Porphyria</p></th>
<th class="head text-right"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Positive test</p></td>
<td class="text-center"><p>410</p></td>
<td class="text-center"><p>19</p></td>
<td class="text-right"><p>429</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Negative test</p></td>
<td class="text-center"><p>90</p></td>
<td class="text-center"><p>481</p></td>
<td class="text-right"><p>571</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Total</p></td>
<td class="text-center"><p>500</p></td>
<td class="text-center"><p>500</p></td>
<td class="text-right"><p>1000</p></td>
</tr>
</tbody>
</table>
</div>
<p>In this scenario, approximately only 4.4% of positive tests are false positives (19 out of 429).</p>
</section>
<section id="hiv-test">
<h4>HIV test<a class="headerlink" href="#hiv-test" title="Link to this heading">#</a></h4>
<p>This case study examines the performance of an HIV test with a sensitivity of 99.9% and a specificity of 99.6%. We will simulate screening a population of 1 million individuals where the prevalence of HIV is 10%.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-center"><p>Has HIV</p></th>
<th class="head text-center"><p>Does not have HIV</p></th>
<th class="head text-right"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Positive test</p></td>
<td class="text-center"><p>99,900</p></td>
<td class="text-center"><p>3,600</p></td>
<td class="text-right"><p>103,500</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Negative test</p></td>
<td class="text-center"><p>100</p></td>
<td class="text-center"><p>896,400</p></td>
<td class="text-right"><p>896,500</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Total</p></td>
<td class="text-center"><p>100,000</p></td>
<td class="text-center"><p>900,000</p></td>
<td class="text-right"><p>1,000,000</p></td>
</tr>
</tbody>
</table>
</div>
<p>Of the approximately 103,500 positive tests, we estimate that 3.6% will be false positives due to the inherent limitations of the test.</p>
<p>Let’s now simulate the screening of 1 million individuals where the prevalence of HIV is 0.1%.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-center"><p>Has HIV</p></th>
<th class="head text-center"><p>Does not have HIV</p></th>
<th class="head text-right"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Positive test</p></td>
<td class="text-center"><p>999</p></td>
<td class="text-center"><p>3,996</p></td>
<td class="text-right"><p>4,995</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Negative test</p></td>
<td class="text-center"><p>1</p></td>
<td class="text-center"><p>995,004</p></td>
<td class="text-right"><p>995,005</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Total</p></td>
<td class="text-center"><p>1000</p></td>
<td class="text-center"><p>999,000</p></td>
<td class="text-right"><p>1,000,000</p></td>
</tr>
</tbody>
</table>
</div>
<p>In a scenario with a low disease prevalence of 0.1%, a significant proportion of positive test results, i.e., 80%, are likely to be false positives. Even with a highly specific test, the low prevalence means that the number of false positives can far outweigh the number of true positives. This leads to a higher chance of healthy individuals being incorrectly identified as positive.</p>
<p>The false positive rate isn’t solely determined by the test’s specificity. The prevalence of the disease within the tested population significantly influences the proportion of positive tests that are false positives.</p>
</section>
</section>
<section id="analogy-with-statistical-hypothesis-testing">
<h3>Analogy with statistical hypothesis testing<a class="headerlink" href="#analogy-with-statistical-hypothesis-testing" title="Link to this heading">#</a></h3>
<p>Interpreting the results of a diagnostic test, much like interpreting statistical significance, requires a nuanced understanding of the context. Just as the prevalence of a disease influences the meaning of a positive test, the prior probability of a true effect shapes our interpretation of statistical significance. Here’s a closer look at the parallels between the two frameworks:</p>
<ul class="simple">
<li><p><em>False negatives</em> in diagnostic tests are akin to <em>type II errors</em> in hypothesis testing. Both represent missed opportunities to detect a true condition or effect.</p></li>
<li><p><em>False positives</em> in diagnostic tests are akin to <em>type I errors</em> in hypothesis testing. Both represent erroneous claims of a condition or effect when none truly exists.</p></li>
<li><p><em>Sensitivity</em> in diagnostic tests mirrors <em>power</em> (1 - β) in hypothesis testing. Both quantify the ability to correctly identify a true positive.</p></li>
<li><p><em>Specificity</em> in diagnostic tests mirrors <em>one minus the significance level</em> (1 - α) in hypothesis testing. Both measure the ability to correctly identify a true negative.</p></li>
</ul>
</section>
<section id="bayes-revisited">
<h3>Bayes revisited<a class="headerlink" href="#bayes-revisited" title="Link to this heading">#</a></h3>
<p>In the realm of diagnostic testing, Bayes’ Theorem provides a powerful framework for <em>updating</em> our beliefs about a patient’s disease status based on the results of a test. By incorporating <strong>prior knowledge</strong> (prevalence) with the test’s characteristics (sensitivity and specificity), we can calculate the probability of the patient <em>actually</em> having the disease given a positive test result (positive predictive value, PPV) or the probability of not having the disease given a negative result (negative predictive value, NPV).</p>
<p>A key component of Bayes’ Theorem is the <strong>likelihood ratio (LR)</strong>, which quantifies how much a test result should change our belief in the presence of the disease. The <strong>positive likelihood ratio (LR+)</strong> is the probability of a positive test result in a patient with the disease divided by the probability of a positive test result in a patient without the disease:</p>
<div class="math notranslate nohighlight">
\[\text{LR+} = \frac{\text{sensitivity}}{1 - \text{specificity}}\]</div>
<p>For example, if we take the previous case studies:</p>
<ul class="simple">
<li><p>Porphyria: <span class="math notranslate nohighlight">\(\text{LR+} = 0.82 / (1 - 0.963) = 22.2\)</span></p></li>
<li><p>HIV: <span class="math notranslate nohighlight">\(\text{LR+} = 0.999 / (1 - 0.996) = 249.75\)</span></p></li>
</ul>
<p>This means that a positive result is 22.2 times <em>more likely</em> in someone with porphyria than someone without it, and 249.75 times more likely in someone with HIV.</p>
<p>Now, Bayes’ theorem allows us to <strong>update</strong> the <strong>pretest odds</strong> (odds of disease before the test) to the <strong>posttest odds</strong> (odds of disease after the test) using the likelihood ratio as <span class="math notranslate nohighlight">\(\text{posttest odds} = \text{pretest odds} \times \frac{\text{sensitivity}}{1 - \text{specificity}} = \text{pretest odds} \times \text{likelihood ratio}\)</span>.</p>
<p>The following table illustrates this process for the porphyria and HIV examples:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Who was tested?</p></th>
<th class="head text-center"><p>Pretest probability</p></th>
<th class="head text-center"><p>Pretest odds</p></th>
<th class="head text-center"><p>LR+</p></th>
<th class="head text-center"><p>Posttest probability</p></th>
<th class="head text-center"><p>Posttest odds</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Porphyria (random screen)</p></td>
<td class="text-center"><p>0.0001</p></td>
<td class="text-center"><p>0.0001</p></td>
<td class="text-center"><p>22.2</p></td>
<td class="text-center"><p>0.0022</p></td>
<td class="text-center"><p>0.22%</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Porphyria (sibling)</p></td>
<td class="text-center"><p>0.50</p></td>
<td class="text-center"><p>1.0</p></td>
<td class="text-center"><p>22.2</p></td>
<td class="text-center"><p>22.2</p></td>
<td class="text-center"><p>95.7%</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>HIV (high prevalence)</p></td>
<td class="text-center"><p>0.1</p></td>
<td class="text-center"><p>0.111</p></td>
<td class="text-center"><p>249.75</p></td>
<td class="text-center"><p>27.5</p></td>
<td class="text-center"><p>96.5%</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>HIV (low prevalence)</p></td>
<td class="text-center"><p>0.001</p></td>
<td class="text-center"><p>0.001</p></td>
<td class="text-center"><p>249.75</p></td>
<td class="text-center"><p>0.25</p></td>
<td class="text-center"><p>20.0%</p></td>
</tr>
</tbody>
</table>
</div>
<p>with <span class="math notranslate nohighlight">\(\text{odds} = \frac{\text{probability}}{1-\text{probability}}\)</span> and <span class="math notranslate nohighlight">\(\text{probability} = \frac{\text{odds}}{1 + \text{odds}}\)</span>.</p>
<p>Even with a high LR+, a positive result in a random screen only slightly increases the probability of having porphyria (0.22%). This is because the disease is very rare. A positive result with a high LR+ is much more informative when the pretest probability is high, as in the case of a sibling test (95.7%). Finally, for the same test (HIV), the posttest probability varies dramatically depending on the pretest probability. A positive result in a high-prevalence population is much more likely to be a true positive than in a low-prevalence population.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Throughout this chapter, we have embarked on a nuanced exploration of statistical significance, venturing beyond the simplistic notion of P &lt; 0.05 as the sole arbiter of truth. We have examined the probabilistic nature of P values, the importance of effect sizes, the complexities of multiple comparisons, and the influence of prior knowledge on interpreting results.</p>
<p>We have seen that statistical significance, while a valuable tool, is not a substitute for scientific reasoning or a guarantee of practical importance.  It is but one piece of evidence in the broader landscape of scientific inquiry.</p>
<p>Armed with this knowledge, we can critically evaluate statistical claims, design studies with appropriate power, and interpret results with a discerning eye. Remember, statistics is not merely a set of formulas, but a framework for reasoning under uncertainty. By embracing its subtleties and complexities, we can unlock its full potential to inform our research and contribute to the advancement of biostatistical knowledge!</p>
</section>
<section id="session-information">
<h2>Session information<a class="headerlink" href="#session-information" title="Link to this heading">#</a></h2>
<p>The output below details all packages and version necessary to reproduce the results in this report.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>python<span class="w"> </span>--version
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">importlib.metadata</span> <span class="kn">import</span> <span class="n">version</span>

<span class="c1"># List of packages we want to check the version</span>
<span class="n">packages</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;numpy&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy&#39;</span><span class="p">,</span> <span class="s1">&#39;pingouin&#39;</span><span class="p">,</span> <span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;seaborn&#39;</span><span class="p">]</span>

<span class="c1"># Initialize an empty list to store the versions</span>
<span class="n">versions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop over the packages</span>
<span class="k">for</span> <span class="n">package</span> <span class="ow">in</span> <span class="n">packages</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Get the version of the package</span>
        <span class="n">package_version</span> <span class="o">=</span> <span class="n">version</span><span class="p">(</span><span class="n">package</span><span class="p">)</span>
        <span class="c1"># Append the version to the list</span>
        <span class="n">versions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">package_version</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>  <span class="c1"># Use a more general exception for broader compatibility</span>
        <span class="n">versions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;Not installed&#39;</span><span class="p">)</span>

<span class="c1"># Print the versions</span>
<span class="k">for</span> <span class="n">package</span><span class="p">,</span> <span class="n">version</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">packages</span><span class="p">,</span> <span class="n">versions</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">package</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Python 3.12.7
-------------
numpy: 1.26.4
scipy: 1.14.1
pingouin: 0.5.5
matplotlib: 3.9.2
seaborn: 0.13.2
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="06%20-%20Confidence%20interval%20of%20counted%20data.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Confidence interval of counted data</p>
      </div>
    </a>
    <a class="right-next"
       href="20%20-%20Statistical%20power%20and%20sample%20size.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Statistical power and sample size</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-reproducibility-challenge">The reproducibility challenge</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variability-of-p-values-under-a-true-effect">Variability of P values under a true effect</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-risk-of-false-positives-under-the-null-hypothesis">The risk of false positives under the null hypothesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#influence-of-sample-size">Influence of sample size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-danger-of-ad-hoc-sample-size-decisions">The danger of ad hoc sample size decisions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-and-p-values">Confidence intervals and P values</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-significance-and-hypothesis-testing">Statistical significance and hypothesis testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-i-and-type-ii-errors">Type I and type II errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-statistical-significance">Interpreting statistical significance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-significance-level">The significance level</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#false-positive-report-probability-fprp">False positive report probability (FPRP)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-influences-fprp">Prior probability influences FPRP</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-of-1">Prior probability of 1%</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-of-50">Prior probability of 50%</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-challenge-of-multiple-comparisons">The challenge of multiple comparisons</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bonferroni-correction">Bonferroni correction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#false-discovery-rate-fdr">False discovery rate (FDR)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-for-equivalence-or-noninferiority">Testing for equivalence or noninferiority</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bioequivalence">Bioequivalence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#two-one-sided-tests-tost">Two one-sided tests (TOST)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-equivalence-and-noninferiority-tests">Interpreting equivalence and noninferiority tests</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-diagnostic-accuracy">Assessing diagnostic accuracy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">Definitions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-curves">ROC curves</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-thresholds">Decision thresholds</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#area-under-the-curve">Area under the curve</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-and-python">ROC and Python</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-studies">Case studies</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#porphyria-test-in-0-01-prevalence">Porphyria test in 0.01% prevalence</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#porphyria-test-in-50-prevalence">Porphyria test in 50% prevalence</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hiv-test">HIV test</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-with-statistical-hypothesis-testing">Analogy with statistical hypothesis testing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-revisited">Bayes revisited</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#session-information">Session information</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sébastien Wieckowski
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>