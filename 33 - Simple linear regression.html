
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Simple linear regression &#8212; The Python Companion of Intuitive Biostatistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-QQY9FLLPJ8"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-QQY9FLLPJ8');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-QQY9FLLPJ8');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '33 - Simple linear regression';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Correlation" href="32%20-%20Correlation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.jpg" class="logo__image only-light" alt="The Python Companion of Intuitive Biostatistics - Home"/>
    <script>document.write(`<img src="_static/logo.jpg" class="logo__image only-dark" alt="The Python Companion of Intuitive Biostatistics - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    Python Companion Guide to “Intuitive Biostatistics”
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Continuous variables</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09%20-%20Quantifying%20scatter%20of%20continuous%20data.html">Quantifying scatter of continuous data</a></li>
<li class="toctree-l1"><a class="reference internal" href="10%20-%20Gaussian%20distribution.html">The Gaussian distribution</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Confidence intervals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12%20-%20Confidence%20interval%20of%20a%20mean.html">Confidence interval of a mean</a></li>
<li class="toctree-l1"><a class="reference internal" href="04%20-%20Confidence%20interval%20of%20a%20proportion.html">Confidence interval of a proportion</a></li>
<li class="toctree-l1"><a class="reference internal" href="05%20-%20Confidence%20interval%20of%20survival%20data.html">Confidence interval of survival data</a></li>
<li class="toctree-l1"><a class="reference internal" href="06%20-%20Confidence%20interval%20of%20counted%20data.html">Confidence interval of counted data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Statistical significance and data assumptions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="15%20-%20Statistical%20significance.html">Statistical significance</a></li>
<li class="toctree-l1"><a class="reference internal" href="20%20-%20Statistical%20power%20and%20sample%20size.html">Statistical power and sample size</a></li>
<li class="toctree-l1"><a class="reference internal" href="24%20-%20Normality%20tests%20and%20outliers.html">Normality tests and outliers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Statistical tests</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="27%20-%20Comparing%20proportions.html">Comparing proportions</a></li>
<li class="toctree-l1"><a class="reference internal" href="29%20-%20Comparing%20survival%20curves.html">Comparing survival curves</a></li>
<li class="toctree-l1"><a class="reference internal" href="30%20-%20Comparing%20two%20unpaired%20means.html">Comparing two unpaired means</a></li>
<li class="toctree-l1"><a class="reference internal" href="31%20-%20Comparing%20paired%20data.html">Comparing paired data</a></li>
<li class="toctree-l1"><a class="reference internal" href="32%20-%20Correlation.html">Correlation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Fitting models to data</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Simple linear regression</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sbwiecko/intuitive_biostatistics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sbwiecko/intuitive_biostatistics/edit/master/33 - Simple linear regression.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/33 - Simple linear regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Simple linear regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-model-to-data">Fitting model to data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#theory-and-definitions">Theory and definitions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares">Least squares</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-coefficients-in-python">Calculating coefficients in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scipy">SciPy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pingouin">Pingouin</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statsmodels">Statsmodels</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#numpy">NumPy</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-model-fit">Assessing model fit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared">R-squared</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusted-r-squared">Adjusted R-squared</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error">Mean squared error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error">Root mean squared error</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-and-diagnostics">Assumptions and diagnostics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-for-linear-regression">Assumptions for linear regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-analysis">Residual analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-plots">Residual plots</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#histograms">Histograms</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#q-q-plots">Q-Q plots</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scale-location-plot">Scale-location plot</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostic-tests">Diagnostic tests</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statsmodels-diagnostic-table">Statsmodels diagnostic table</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#breusch-pagan">Breusch-Pagan</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exogeneity">Exogeneity</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#other-diagnostic-tools">Other diagnostic tools</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#consequences-and-remedies">Consequences and remedies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-inference-and-uncertainty">Statistical inference and uncertainty</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-error-of-the-regression">Standard error of the regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values-and-confidence-intervals">P values and confidence intervals</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#t-tests-for-the-coefficients">T-tests for the coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-t-critical-and-p-values">Visualizing t, critical and P values</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-interval">Confidence interval</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-confidence-band">Visualizing the confidence band</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping-in-linear-regression">Bootstrapping in linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-bootstrap-samples">Generating bootstrap samples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#constructing-confidence-intervals-from-bootstrap-replicates">Constructing confidence intervals from bootstrap replicates</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping-and-the-confidence-band">Bootstrapping and the confidence band</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#permutation-test-for-p-values">Permutation test for P values</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction">Prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#making-predictions">Making predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-with-python">Prediction with Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-performance">Model performance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-intervals">Prediction intervals</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-techniques">Advanced techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum likelihood estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-mle-works">How MLE works</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-and-linear-regression">MLE and linear regression</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-of-mle">Visualization of MLE</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-linear-models">Generalized linear models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cheat-sheet">Cheat sheet</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Fitting model to data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Assessing model fit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Assumptions and diagnostics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Residual analysis</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Diagnostic tests</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Statistical inference and uncertainty</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#p-value-and-confidence-intervals">P value and confidence intervals</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping">Bootstrapping</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling">Resampling</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#permutation">Permutation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Advanced techniques</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#session-information">Session information</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="simple-linear-regression">
<h1>Simple linear regression<a class="headerlink" href="#simple-linear-regression" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In the previous chapter, we explored the concept of <strong>correlation</strong>, which measures the strength and direction of the linear relationship between two variables. Now, we’ll delve into <strong>simple linear regression</strong>, a powerful technique that allows us to go beyond merely describing the relationship to actually modeling it.</p>
<p>One way to think about linear regression is that it fits the “best line” through a scatter plot of data points. This line, often called the <strong>regression line</strong>, captures the essence of the linear relationship between the variables. Imagine drawing a line through a cloud of points - linear regression finds the line that minimizes the overall distance between the points and the line itself.</p>
<p>But there’s more to it than just drawing a line. Linear regression also provides a <strong>model</strong> that represents the relationship mathematically. So, another way to look at linear regression is that it fits this simple model to the data, aiming to determine the most likely values of the parameters that define the model. These estimated parameters provide valuable insights into the relationship between the variables.</p>
<p>In this chapter, we’ll explore the key concepts of simple linear regression, including:</p>
<ul class="simple">
<li><p>Estimating the regression coefficients: we’ll learn how to calculate the best-fitting line using the method of <strong>ordinary least squares (OLS)</strong> in Python.</p></li>
<li><p>Assessing model fit: we’ll revisit <strong>R-squared</strong> and introduce other metrics to evaluate how well the model captures the data.</p></li>
<li><p>Assumptions and diagnostics: we’ll briefly recap the key assumptions of linear regression and learn how to diagnose potential violations.</p></li>
<li><p>Inference and uncertainty: we’ll explore <strong>confidence intervals</strong> and <strong>hypothesis testing</strong> to quantify the uncertainty associated with the estimated coefficients.</p></li>
<li><p>Making predictions: we’ll see how to use the fitted model to predict values of the dependent variable for new values of the independent variable.</p></li>
<li><p>Advanced techniques: we’ll touch upon topics like <strong>regularization</strong> and <strong>maximum likelihood estimation (MLE)</strong> to enhance model fitting.</p></li>
</ul>
</section>
<section id="fitting-model-to-data">
<h2>Fitting model to data<a class="headerlink" href="#fitting-model-to-data" title="Link to this heading">#</a></h2>
<section id="theory-and-definitions">
<h3>Theory and definitions<a class="headerlink" href="#theory-and-definitions" title="Link to this heading">#</a></h3>
<p>In simple linear regression, we aim to <strong>model</strong> the relationship between a <strong>response variable</strong> (<span class="math notranslate nohighlight">\(y\)</span>) and a <strong>covariate</strong> (<span class="math notranslate nohighlight">\(x\)</span>). Our measurements of these variables are not perfect and contain some inherent <strong>noise</strong> (<span class="math notranslate nohighlight">\(\epsilon\)</span>). We can express this relationship mathematically as:</p>
<div class="math notranslate nohighlight">
\[y = f(x) + \epsilon\]</div>
<p>where <span class="math notranslate nohighlight">\(f(x)\)</span> is the <strong>regression function</strong> that describes the relationship between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. This equation essentially states that the observed response is a combination of a systematic component and a random component.</p>
<p>In this chapter we focus on <strong>linear combination</strong>, a specific type of regression function. For example, we can model insulin sensitivity (the <strong>dependent variable</strong>) <em>as a function of</em> the percentage of C20-22 fatty acids (the <strong>independent variable</strong>). This simple linear regression model can be written as:</p>
<div class="math notranslate nohighlight">
\[y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> represents the insulin sensitivity <strong>response variable</strong> for the <span class="math notranslate nohighlight">\(i\)</span>-th individual</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i\)</span> represents the percentage of C20-22 fatty acids <strong>covariate</strong> for the <span class="math notranslate nohighlight">\(i\)</span>-th individual</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the <strong>intercept</strong>, representing the value of <span class="math notranslate nohighlight">\(y\)</span> when <span class="math notranslate nohighlight">\(x\)</span> is zero</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span> is the <strong>slope</strong> of the line, representing the change in <span class="math notranslate nohighlight">\(y\)</span> <em>for a one-unit change</em> in <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon_i\)</span> is the random error for the <span class="math notranslate nohighlight">\(i\)</span>-th individual</p></li>
</ul>
<p>The intercept (<span class="math notranslate nohighlight">\(\beta_0\)</span>) and slope (<span class="math notranslate nohighlight">\(\beta_1\)</span>) are the <strong>parameters</strong> of the model, and our goal is to estimate their true values from the data. We assume that the errors (<span class="math notranslate nohighlight">\(\epsilon_i\)</span>) are independent and follow a Gaussian distribution with a <em>mean of zero</em>: <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span>.</p>
<p>We can generalize the simple linear regression model to include multiple covariates, to model more complex relationships. This allows us to consider the relationship between the response variable and multiple predictors simultaneously. The model with <span class="math notranslate nohighlight">\(p\)</span> covariates can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \epsilon_i \\
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the response variable for the <span class="math notranslate nohighlight">\(i\)</span>-th observation</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{i1}, x_{i2}, ..., x_{ip}\)</span> are the values of the <span class="math notranslate nohighlight">\(p\)</span> covariates for the <span class="math notranslate nohighlight">\(i\)</span>-th observation</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0, \beta_1, \beta_2, ..., \beta_p\)</span> are the regression coefficients, including the intercept (<span class="math notranslate nohighlight">\(\beta_0\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon_i\)</span> is the error term for the <span class="math notranslate nohighlight">\(i\)</span>-th observation</p></li>
</ul>
<p>This can be expressed more compactly using summation notation:</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0 + \sum \beta_p x_{ip} + \epsilon_i
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\beta_0 = \beta_0 \times 1\)</span>, so this coefficient can be integrated into the sum, therefore the regression function can be expressed in matrix notation as:</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}_i, \boldsymbol{\beta}) = \beta_0 + \sum \beta_p x_{i,p} = \mathbf{x}_i^T \boldsymbol{\beta}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}_i = \begin{bmatrix} 1 &amp; x_{i1} &amp; x_{i2} &amp; \dots &amp; x_{ip} \end{bmatrix}^T\)</span> is the <strong>covariate vector</strong> for the <span class="math notranslate nohighlight">\(i\)</span>-th observation, including 1 for the intercept</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta} = \begin{bmatrix} \beta_0 &amp; \beta_1 &amp; \beta_2 &amp; \dots &amp; \beta_p \end{bmatrix}^T\)</span> is the <strong>vector of regression coefficients</strong>, including the intercept</p></li>
</ul>
<p>In linear models, the function <span class="math notranslate nohighlight">\(f(\mathbf{x}_i, \boldsymbol{\beta})\)</span> is designed to model how the <em>average</em> value of  <span class="math notranslate nohighlight">\(y\)</span> changes with <span class="math notranslate nohighlight">\(x\)</span>. The error term  <span class="math notranslate nohighlight">\(\epsilon\)</span> accounts for the fact that individual data points will deviate from this average due to random variability. Essentially, we’re using a linear function to <em>model the mean response</em>, while acknowledging that there will always be some unpredictable variation around that mean.</p>
<p>Finally, the relationship between the response variable and multiple predictors for all observations in the dataset can be expressed even more concisely in matrix notation as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}\)</span> is the vector of response variables</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X} = \begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \dots &amp; x_{1p} \\ 1 &amp; x_{21} &amp; x_{22} &amp; \dots &amp; x_{2p} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{np} \end{bmatrix}\)</span> is the design matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p \end{bmatrix}\)</span> is the vector of regression coefficients</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}\)</span> is the vector of error terms</p></li>
</ul>
</section>
<section id="least-squares">
<h3>Least squares<a class="headerlink" href="#least-squares" title="Link to this heading">#</a></h3>
<p>How do we actually find the “best” values for the parameters that accurately capture this relationship? <strong>Ordinary least squares (OLS)</strong> provides a method for estimating these coefficients by <em>minimizing the sum of the squared differences (the <strong>sum of squared errors (SSE)</strong>, or <strong>sum of squared residuals</strong>) between the observed data points and the values predicted by the regression line</em>:</p>
<div class="math notranslate nohighlight">
\[S(\boldsymbol{\beta}) = \sum_{i=1}^{n} [y_i - f(\mathbf{x}_i, \boldsymbol{\beta})]^2 = \sum_{i=1}^{n} \epsilon_i^2\]</div>
<p><span class="math notranslate nohighlight">\(S(\boldsymbol{\beta})\)</span> is a measure of the overall discrepancy between the observed data points (<span class="math notranslate nohighlight">\(y_i\)</span>) and the values predicted by the regression model. Squaring the residuals ensures that positive and negative differences contribute equally to the overall measure of discrepancy. In simpler terms, imagine we have a scatter plot of data points and we draw a line through them. <span class="math notranslate nohighlight">\(S(\boldsymbol{\beta})\)</span> represents the total sum of the squared vertical distances between each point and the line. OLS aims to find the line, i.e., the values of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, that makes this total distance as small as possible. Mathematically, we’re looking for:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}=\text{argmin}_{\beta}S(\boldsymbol{\beta})\]</div>
<p>The ‘hat’ notation is used to denote an <strong>estimate</strong> of a true value. So, <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> represents the estimated value of the regression coefficients, while <span class="math notranslate nohighlight">\(\hat{y}\)</span> represents the estimated or predicted value of the response variable based on our model.</p>
<p>Using the vector and matrix notation, we can write:</p>
<div class="math notranslate nohighlight">
\[S(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) = \boldsymbol{\epsilon}^T\boldsymbol{\epsilon}\]</div>
<p>When the system is <em>overdetermined</em>, i.e., more data points than coefficients, <a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Matrix/vector_formulation">an analytical solution exists</a> for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. Indeed, we can find this minimum by taking the derivative of <span class="math notranslate nohighlight">\(S(\boldsymbol{\beta})\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, setting it to zero, and solving for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. This process is similar to finding the minimum point of a curve, where the slope, i.e., the derivative, is zero.</p>
<p>More formally, we calculate the <strong>gradient</strong> of the function <span class="math notranslate nohighlight">\(S(\boldsymbol{\beta})\)</span>: <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{\beta}} S(\boldsymbol{\beta})\)</span>.
This gradient is a vector containing the <em>partial derivatives</em> of <span class="math notranslate nohighlight">\(S(\boldsymbol{\beta})\)</span> with respect to each element of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. Setting this gradient to zero, i.e., <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{\beta}} S(\boldsymbol{\beta})\)</span>, gives us a system of equations that we can solve to find the values of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> that minimize <span class="math notranslate nohighlight">\(S(\boldsymbol{\beta})\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}\]</div>
<p>In simple linear regression with one predictor variable, the design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and the parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> have the following forms:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}
\mathbf{X} = \begin{bmatrix}
1 &amp; x_1 \\
1 &amp; x_2 \\
\vdots &amp; \vdots \\
1 &amp; x_n
\end{bmatrix}\end{split}\\\qquad\\\begin{split}\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}
\end{split}\end{aligned}\end{align} \]</div>
<p>To apply the matrix formula <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}\)</span>, we need to calculate different parts:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}^T \mathbf{X} = \begin{bmatrix}
1 &amp; 1 &amp; \dots &amp; 1 \\
x_1 &amp; x_2 &amp; \dots &amp; x_n
\end{bmatrix}
\begin{bmatrix}
1 &amp; x_1 \\
1 &amp; x_2 \\
\vdots &amp; \vdots \\
1 &amp; x_n
\end{bmatrix} =
\begin{bmatrix}
n &amp; \sum x_i \\
\sum x_i &amp; \sum x_i^2
\end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
(\mathbf{X}^T \mathbf{X})^{-1} = \frac{1}{n\sum x_i^2 - (\sum x_i)^2} \begin{bmatrix} \sum x_i^2 &amp; -\sum x_i \\ -\sum x_i &amp; n \end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}^T \mathbf{y} = \begin{bmatrix}
1 &amp; 1 &amp; \dots &amp; 1 \\
x_1 &amp; x_2 &amp; \dots &amp; x_n
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix} = \begin{bmatrix}
\sum y_i \\
\sum x_i y_i
\end{bmatrix}
\end{split}\]</div>
<p>Substituing these components back into the matrix equation and perform the matrix multiplication and inverse to obtain the following equations for <span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1 = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2} = \frac{s_{xy}}{s^2_x}
\qquad
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bar{x}\)</span> is the <strong>mean</strong> of the predictor variable <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\bar{y}\)</span> is the <strong>mean</strong> of the response variable <span class="math notranslate nohighlight">\(y\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(s_{xy}\)</span> is the <strong>covariance</strong> between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(s^2_x\)</span> is the <strong>variance</strong> of <span class="math notranslate nohighlight">\(x\)</span></p></li>
</ul>
<p>These equations provide a direct way to calculate the intercept and slope in simple linear regression. Recall that covariance measures the direction and strength of the linear relationship between two variables, and variance measures the spread or variability of a single variable. The slope estimate (<span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>) is the ratio of the covariance between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> to the variance of <span class="math notranslate nohighlight">\(x\)</span>. The intercept estimate (<span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span>) is then calculated based on the slope and the means of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
</section>
<section id="calculating-coefficients-in-python">
<h3>Calculating coefficients in Python<a class="headerlink" href="#calculating-coefficients-in-python" title="Link to this heading">#</a></h3>
<p>Now that we’ve explored the theoretical foundations of simple linear regression and how to estimate the coefficients using OLS, let’s dive into some practical applications using Python. We’ll use the same dataset from the previous chapter on correlation analysis, where we examined the relationship between insulin sensitivity and the percentage of C20-22 fatty acids in muscle phospholipids.</p>
<section id="dataset">
<h4>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">#</a></h4>
<p>In the previous chapter, we explored the relationship between insulin sensitivity and the percentage of C20-22 fatty acids in muscle phospholipids using data from <a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/8418404/">Borkman and colleagues</a>. We visualized this relationship with scatterplots and quantified its strength and direction using correlation coefficients. Remember that correlation analysis treats both variables symmetrically, and it doesn’t assume a cause-and-effect relationship or a direction of influence.</p>
<p>Now, we’ll take this exploration a step further with simple linear regression. Instead of just describing the association, we’ll build a model that <em>predicts insulin sensitivity based on the percentage of C20-22 fatty acids</em>. This implies a <strong>directional relationship</strong>, where we’re specifically interested in how changes in fatty acid composition might affect insulin sensitivity.</p>
<p>While correlation provides a valuable starting point for understanding the relationship between two variables, linear regression offers a more powerful framework for modeling and predicting that relationship:</p>
<ul class="simple">
<li><p>Estimate the strength and direction of the effect: we can quantify how much insulin sensitivity is expected to change for a given change in fatty acid composition.</p></li>
<li><p>Make predictions: we can use the model to predict insulin sensitivity for new individuals based on their fatty acid levels.</p></li>
<li><p>Test hypotheses:we can formally test whether there is a statistically significant relationship between the variables.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Example data from the book, page 319 (directly into a DataFrame)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="c1"># percentage of C20-22 fatty acids</span>
    <span class="s1">&#39;per_C2022_fatacids&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">17.9</span><span class="p">,</span> <span class="mf">18.3</span><span class="p">,</span> <span class="mf">18.3</span><span class="p">,</span> <span class="mf">18.4</span><span class="p">,</span> <span class="mf">18.4</span><span class="p">,</span> <span class="mf">20.2</span><span class="p">,</span> <span class="mf">20.3</span><span class="p">,</span> <span class="mf">21.8</span><span class="p">,</span> <span class="mf">21.9</span><span class="p">,</span> <span class="mf">22.1</span><span class="p">,</span> <span class="mf">23.1</span><span class="p">,</span> <span class="mf">24.2</span><span class="p">,</span> <span class="mf">24.4</span><span class="p">],</span>
    <span class="c1"># insulin sensitivity index</span>
    <span class="s1">&#39;insulin_sensitivity&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">250</span><span class="p">,</span> <span class="mi">220</span><span class="p">,</span> <span class="mi">145</span><span class="p">,</span> <span class="mi">115</span><span class="p">,</span> <span class="mi">230</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">330</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">370</span><span class="p">,</span> <span class="mi">260</span><span class="p">,</span> <span class="mi">270</span><span class="p">,</span> <span class="mi">530</span><span class="p">,</span> <span class="mi">375</span><span class="p">],</span>
<span class="p">})</span>

<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>per_C2022_fatacids</th>
      <th>insulin_sensitivity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.9</td>
      <td>250</td>
    </tr>
    <tr>
      <th>1</th>
      <td>18.3</td>
      <td>220</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18.3</td>
      <td>145</td>
    </tr>
    <tr>
      <th>3</th>
      <td>18.4</td>
      <td>115</td>
    </tr>
    <tr>
      <th>4</th>
      <td>18.4</td>
      <td>230</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="scipy">
<h4>SciPy<a class="headerlink" href="#scipy" title="Link to this heading">#</a></h4>
<p>The SciPy package provides basic statistical functions, including <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html">a function for simple linear regression</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">linregress</span>

<span class="c1"># Load the data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;per_C2022_fatacids&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;insulin_sensitivity&#39;</span><span class="p">]</span>

<span class="c1"># Calculate the coefficients (we will discuss the metrics in a subsequent section)</span>
<span class="n">results_scipy</span> <span class="o">=</span> <span class="n">linregress</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">slope_scipy</span><span class="p">,</span> <span class="n">intercept_scipy</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">results_scipy</span> <span class="c1"># Extract the coefficients</span>

<span class="c1"># print(res)  # Print the entire result set</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept:&quot;</span><span class="p">,</span> <span class="n">intercept_scipy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Slope:&quot;</span><span class="p">,</span> <span class="n">slope_scipy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intercept: -486.54199459921034
Slope: 37.20774574745539
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">linregress</span></code> function provides the estimated coefficients for our regression model. In this case, we find a slope of 37.2. This means that for every 1 unit increase in the percentage of C20-22 fatty acids in muscle phospholipids, we expect, on average, a 37.2 mg/m²/min increase in insulin sensitivity.</p>
<p>The intercept value we obtain is -486.5. However, in this specific example, the intercept doesn’t have a clear biological interpretation. It represents the estimated insulin sensitivity when the percentage of C20-22 fatty acids is zero, which is not a realistic scenario in this context. It’s important to remember that <strong>extrapolating</strong> the linear model beyond the range of the observed data can lead to unreliable conclusions.</p>
<p>These results suggest a positive association between the percentage of C20-22 fatty acids and insulin sensitivity within the observed data range. However, to fully understand the reliability and significance of this relationship, we’ll need to further explore the model’s fit and perform statistical inference.</p>
</section>
<section id="pingouin">
<h4>Pingouin<a class="headerlink" href="#pingouin" title="Link to this heading">#</a></h4>
<p>The <a class="reference external" href="https://pingouin-stats.org/build/html/generated/pingouin.linear_regression.html"><code class="docutils literal notranslate"><span class="pre">pingouin.linear_regression</span></code> function provides a more user-friendly output than SciPy</a>, presenting the regression results in a convenient table format. While SciPy returns only the core statistics, i.e., slope, intercept, r-value, p-value, and standard errors, Pingouin provides a more comprehensive output, including additional information such as t-values, adjusted r-squared, and confidence intervals for the coefficients, all organized in a clear and easy-to-read table.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pingouin</span> <span class="k">as</span> <span class="nn">pg</span>

<span class="n">model_pingouin</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span>
    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
    <span class="n">remove_na</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">coef_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Otherwise returns only the intercept and slopes values</span>
    <span class="n">as_dataframe</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Otherwise returns a dictionnay of results</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">.05</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">pg</span><span class="o">.</span><span class="n">print_table</span><span class="p">(</span><span class="n">model_pingouin</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>names                   coef       se       T    pval     r2    adj_r2    CI[2.5%]    CI[97.5%]
------------------  --------  -------  ------  ------  -----  --------  ----------  -----------
Intercept           -486.542  193.716  -2.512   0.029  0.593     0.556    -912.908      -60.176
per_C2022_fatacids    37.208    9.296   4.003   0.002  0.593     0.556      16.748       57.668
</pre></div>
</div>
</div>
</div>
<p>The Pingouin output provides us with more detailed information about the regression model, including the 95% confidence interval for the slope and the intercept. This interval, ranging from 16.748 to 57.668, gives us a range of plausible values for the true slope of the relationship between %C20-22 fatty acids and insulin sensitivity <em>in the population</em>. The fact that this interval <em>does not include zero</em> provides further support for a relationship between these variables. If the interval included zero, it would suggest that the observed slope could plausibly be due to chance alone.</p>
<p>We also see that the R-squared value is 0.593. This means that 59% of the variability in insulin sensitivity can be explained by the linear relationship with %C20-22 fatty acids. The remaining 41% of variability could be due to other factors not included in the model, measurement error, or simply natural biological variation. R² and other metrics will be discussed in more details in the next section.</p>
<p>Finally, the P value associated with the slope is 0.002. This P value tests the null hypothesis that there is no relationship between %C20-22 fatty acids and insulin sensitivity. In this case, the very small P value provides strong evidence against the null hypothesis, suggesting that the observed relationship is unlikely to be due to random chance.</p>
<p>We’ll discuss the calculation and interpretation of standard errors, confidence intervals and P values in more detail later in the chapter.</p>
</section>
<section id="statsmodels">
<h4>Statsmodels<a class="headerlink" href="#statsmodels" title="Link to this heading">#</a></h4>
<p>While SciPy and Pingouin provide tools for linear regression, they don’t offer a comprehensive summary output like the one we see in Statsmodels. Statsmodels, with its focus on statistical modeling, provides a dedicated <code class="docutils literal notranslate"><span class="pre">OLS</span></code> class (within the <a class="reference external" href="https://www.statsmodels.org/stable/regression.html#"><code class="docutils literal notranslate"><span class="pre">statsmodels.regression.linear_model</span></code> module</a>) that not only calculates the regression coefficients but also generates a detailed summary table containing a wide range of statistical information about the model.</p>
<p>In order to utilize the convenience of R-style formulas for defining our regression model, we import the <code class="docutils literal notranslate"><span class="pre">formula.api</span></code> module from <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> and assign it the alias <code class="docutils literal notranslate"><span class="pre">smf</span></code>. This allows us to specify the model in a more concise and readable way, similar to how it’s done in the R programming language.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="c1"># Suppress all UserWarnings, incl. messages related to small sample size</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>

<span class="n">model_statsmodels</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;insulin_sensitivity ~ per_C2022_fatacids&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">results_statsmodels</span> <span class="o">=</span> <span class="n">model_statsmodels</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1">#print(results.summary())  # Classical output of the result table</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_statsmodels</span><span class="o">.</span><span class="n">summary2</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                    Results: Ordinary least squares
=======================================================================
Model:               OLS                  Adj. R-squared:      0.556   
Dependent Variable:  insulin_sensitivity  AIC:                 151.2840
Date:                2024-11-29 10:34     BIC:                 152.4139
No. Observations:    13                   Log-Likelihood:      -73.642 
Df Model:            1                    F-statistic:         16.02   
Df Residuals:        11                   Prob (F-statistic):  0.00208 
R-squared:           0.593                Scale:               5760.1  
-----------------------------------------------------------------------
                     Coef.   Std.Err.    t    P&gt;|t|    [0.025   0.975] 
-----------------------------------------------------------------------
Intercept          -486.5420 193.7160 -2.5116 0.0289 -912.9081 -60.1759
per_C2022_fatacids   37.2077   9.2959  4.0026 0.0021   16.7475  57.6680
-----------------------------------------------------------------------
Omnibus:                3.503          Durbin-Watson:             2.172
Prob(Omnibus):          0.173          Jarque-Bera (JB):          1.139
Skew:                   0.023          Prob(JB):                  0.566
Kurtosis:               1.550          Condition No.:             192  
=======================================================================
Notes:
[1] Standard Errors assume that the covariance matrix of the errors is
correctly specified.
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> OLS summary provides a wealth of information about the fitted regression model in three key sections:</p>
<ol class="arabic simple">
<li><p>Model summary - This section provides an overall assessment of the model’s fit</p></li>
<li><p>Coefficients table - This table provides detailed information about each coefficient</p></li>
<li><p>Model diagnostics - This section includes several diagnostic tests to assess the validity of the model assumptions</p></li>
</ol>
<p>By carefully examining these different sections of the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> OLS summary, we can gain a comprehensive understanding of the fitted regression model, its performance, and the validity of its underlying assumptions.</p>
<p>We can access each of these sections individually using the <code class="docutils literal notranslate"><span class="pre">tables</span></code> attribute, which is a list containing each table as a separate element. In addition, the <code class="docutils literal notranslate"><span class="pre">params</span></code> attribute of the fitted statsmodels object stores the estimated regression coefficients. For example, to access the table with the coefficients, their standard errors, t-values, P values, and confidence intervals.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the coefficients table</span>
<span class="n">coef_table_statsmodels</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Print the coefficients table</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coef_table_statsmodels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>======================================================================================
                         coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------------
Intercept           -486.5420    193.716     -2.512      0.029    -912.908     -60.176
per_C2022_fatacids    37.2077      9.296      4.003      0.002      16.748      57.668
======================================================================================
</pre></div>
</div>
</div>
</div>
<p>By carefully examining the coefficient table, we can gain valuable insights into the magnitude, significance, and uncertainty of the relationships between the predictor variables and the response variable:</p>
<ul class="simple">
<li><p>‘coef’: this column displays the estimated values of the regression coefficients. In simple linear regression, we have two coefficients: the intercept (<code class="docutils literal notranslate"><span class="pre">Intercept</span></code>) and the slope associated with the predictor variable (in our case, <code class="docutils literal notranslate"><span class="pre">per_C2022_fatacids</span></code>).</p></li>
<li><p>‘std err’: this column shows the standard errors of the coefficient estimates. The standard error is a measure of the uncertainty or variability associated with each estimate. Smaller standard errors indicate more precise estimates.</p></li>
<li><p>‘t’: this column presents the t-values for each coefficient. The t-value is calculated by dividing the coefficient estimate by its standard error. It’s used to test the null hypothesis that the true coefficient is zero.</p></li>
<li><p>‘P&gt;|t|’: this column displays the P values associated with the t-tests. The P value represents the probability of observing the estimated coefficient (or a more extreme value) if the true coefficient were actually zero. A low P value (typically below 0.05) suggests that the predictor variable has a statistically significant effect on the response variable.</p></li>
<li><p>‘[0.025   0.975]’: this column shows the 95% confidence intervals for the true values of the coefficients. The confidence interval provides a range of plausible values within which we can be 95% confident that the true coefficient lies.</p></li>
</ul>
<p>Note that the calculation of the standard errors, t-values, P values, and confidence intervals in this table relies on the assumption that the errors in the regression model are normally distributed. We will discuss these values later in this chapter.</p>
<p>We can also directly display the estimated intercept and slope.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The parameters of the model&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_statsmodels</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The parameters of the model
Intercept            -486.541995
per_C2022_fatacids     37.207746
dtype: float64
</pre></div>
</div>
</div>
</div>
</section>
<section id="numpy">
<h4>NumPy<a class="headerlink" href="#numpy" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html">NumPy offers the <code class="docutils literal notranslate"><span class="pre">numpy.polyfit</span></code> function</a> that fits a polynomial of a specified degree to the data. In our case, we use <code class="docutils literal notranslate"><span class="pre">deg=1</span></code> to fit a straight line (a polynomial of degree 1). It returns an array of polynomial coefficients with the highest power first, i.e., slope followed by intercept. The function integrates well with plotting libraries like Matplotlib. We can easily use the returned coefficients to generate the equation of the line and plot it alongside the data.</p>
<p>So for simple linear regression, <code class="docutils literal notranslate"><span class="pre">numpy.polyfit</span></code> can be more concise than SciPy, Pingouin or <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>, especially if the primary goal is to obtain the coefficients for plotting or basic calculations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Fit the model using numpy.polyfit</span>
<span class="n">coefficients_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">deg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># deg=1 specifies a linear fit (degree 1)</span>
<span class="n">slope_numpy</span><span class="p">,</span> <span class="n">intercept_numpy</span> <span class="o">=</span> <span class="n">coefficients_numpy</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Slope (NumPy):&quot;</span><span class="p">,</span> <span class="n">slope_numpy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept (NumPy):&quot;</span><span class="p">,</span> <span class="n">intercept_numpy</span><span class="p">)</span>

<span class="c1"># Plot the data and the fitted line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">slope_numpy</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">intercept_numpy</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;%C20-22 fatty acids&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Insulin sensitivity (mg/m²/min)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear regression of insulin sensitivity on %C20-22 fatty acids&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Slope (NumPy): 37.20774574745538
Intercept (NumPy): -486.54199459921
</pre></div>
</div>
<img alt="_images/66cfda090590826af52b682d378913dcfc187b1cfe499c5536ac25b1660fb034.png" src="_images/66cfda090590826af52b682d378913dcfc187b1cfe499c5536ac25b1660fb034.png" />
</div>
</div>
</section>
</section>
</section>
<section id="assessing-model-fit">
<h2>Assessing model fit<a class="headerlink" href="#assessing-model-fit" title="Link to this heading">#</a></h2>
<p>Now that we’ve visualized the relationship between insulin sensitivity and %C20-22 fatty acids and explored different ways to estimate the regression coefficients, it’s crucial to assess how well our model actually fits the data. This step helps us determine <em>how effectively the model captures the underlying relationship</em> and <em>how reliable it is for making predictions</em>.</p>
<p>The Statsmodels library provides a comprehensive summary output that offers valuable insights into these aspects. Let’s take a look at the first table from this output, which provides an overall summary of the model’s fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the coefficients table</span>
<span class="n">assessment_table_statsmodels</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Print the coefficients table</span>
<span class="nb">print</span><span class="p">(</span><span class="n">assessment_table_statsmodels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                             OLS Regression Results                            
===============================================================================
Dep. Variable:     insulin_sensitivity   R-squared:                       0.593
Model:                             OLS   Adj. R-squared:                  0.556
Method:                  Least Squares   F-statistic:                     16.02
Date:                 Fri, 29 Nov 2024   Prob (F-statistic):            0.00208
Time:                         10:34:13   Log-Likelihood:                -73.642
No. Observations:                   13   AIC:                             151.3
Df Residuals:                       11   BIC:                             152.4
Df Model:                            1                                         
Covariance Type:             nonrobust                                         
===============================================================================
</pre></div>
</div>
</div>
</div>
<p>The model summary provides an overall assessment of the fitted regression model:</p>
<ul class="simple">
<li><p>‘R-squared’: this familiar metric denoted as <strong>R²</strong>, also known as the <strong>coefficient of determination</strong>, quantifies the proportion of variance in the response variable that is explained by the predictor variable(s). It ranges from 0 to 1, where a higher value indicates a better fit.</p></li>
<li><p>‘Adj. R-squared’: the adjusted R² is a modified version of R² that takes into account the number of predictors in the model. It penalizes the addition of unnecessary predictors that don’t significantly improve the model’s explanatory power, helping to prevent overfitting.</p></li>
<li><p>‘F-statistic’: this statistic tests the overall significance of the regression model. It assesses whether at least one of the predictor variables has a non-zero coefficient, meaning it has a statistically significant effect on the response variable. This is conceptually similar to the F-test we encountered in the chapter on comparing two unpaired means, where we tested for the equality of variances.</p></li>
<li><p>‘Prob (F-statistic)’: this is the p-value associated with the F-statistic. A low P value (typically below 0.05) provides evidence that the model as a whole is statistically significant, meaning that at least one of the predictors is likely to have a real effect on the response variable.</p></li>
<li><p>‘Log-Likelihood’: this value reflects how well the model fits the observed data, assuming that the errors are normally distributed. A higher log-likelihood indicates a better fit.</p></li>
<li><p>‘AIC’: the Akaike Information Criterion (AIC) is a measure of model fit that penalizes models with more parameters. It’s useful for comparing different models, even if they are not nested. Lower AIC values generally indicate a better balance between model fit and complexity. We encountered AIC before in the chapter on comparing survival curves, where it provided valuable guidance in model selection.</p></li>
<li><p>‘BIC’: the Bayesian Information Criterion (BIC) is similar to AIC but imposes a stronger penalty for models with more parameters. Like AIC, lower BIC values are generally preferred.</p></li>
</ul>
<p>By examining these summary statistics, we can gain an overall understanding of the model’s performance, its explanatory power, and its statistical significance.</p>
<section id="r-squared">
<h3>R-squared<a class="headerlink" href="#r-squared" title="Link to this heading">#</a></h3>
<p>In the model summary table, we see several metrics that provide insights into the model’s fit. One of the most commonly used metrics is <strong>R²</strong>, which quantifies the proportion of variance in the response variable explained by the predictor variable.</p>
<p>Note that, while sometimes denoted as r² in the context of simple linear regression, we’ll use R² throughout this chapter for consistency.</p>
<p>We can extract the correlation of coefficient (Pearson’s r) calculated by <code class="docutils literal notranslate"><span class="pre">scipy.stats.linregress</span></code> and square it, or get the R² attribute of the fitted Statsmodel object and the Pingouin’s result table.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate R-squared from scipy.stats results</span>
<span class="n">r_value_scipy</span> <span class="o">=</span> <span class="n">results_scipy</span><span class="o">.</span><span class="n">rvalue</span> <span class="c1"># type: ignore</span>
<span class="n">r_squared_scipy</span> <span class="o">=</span> <span class="n">r_value_scipy</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Print R-squared values from different packages</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R² (SciPy):</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">r_squared_scipy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R² (Pingouin):</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">model_pingouin</span><span class="o">.</span><span class="n">r2</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R² (SciPy):	 0.5929
R² (Pingouin):	 0.5929
</pre></div>
</div>
</div>
</div>
<p>While Python libraries provide convenient ways to obtain R², it’s also helpful to understand how it’s calculated manually. This provides deeper insight into the meaning of R² and its connection to the concept of variance.</p>
<p>Recall from the previous chapter that <strong>Pearson’s correlation coefficient</strong> is calculated by dividing the <strong>covariance</strong> of two variables by the product of their <strong>standard deviations</strong>. R² is simply the square of this correlation coefficient:</p>
<div class="math notranslate nohighlight">
\[
r = \frac{s_{xy}}{s_x s_y}
\qquad
R^2 = \frac{s^2_{xy}}{s^2_x s^2_y}
\]</div>
<p>Let’s define the <strong>total sum of squares (TSS)</strong> as a measure of the total variability of the response variable (y) around its mean, the the <strong>residual sum of squares (RSS)</strong>, also called <strong>sum of squared residuals (SSR)</strong>, as a measure of the variability in y that is not explained by the regression model:</p>
<div class="math notranslate nohighlight">
\[
\text{TSS} = \sum{(y_i - \bar{y})^2}
\qquad
\text{RSS} = \sum{(y_i - \hat{y}_i)^2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th observed value of <span class="math notranslate nohighlight">\(y\)</span>, <span class="math notranslate nohighlight">\(\bar{y}\)</span> is the mean of <span class="math notranslate nohighlight">\(y\)</span>, and <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th predicted/estimated value of <span class="math notranslate nohighlight">\(y\)</span> from the regression model.</p>
<p>The key connection is that the variance of y (<span class="math notranslate nohighlight">\(s^2_y\)</span>) is directly proportional to TSS, as we have seen in the chapter about the quantification of scatter of continuous data with the definition of the sample variance:</p>
<div class="math notranslate nohighlight">
\[s^2_y = \frac{1}{n-1}\sum(y_i - \bar y)^2 = \frac{\text{TSS}}{n-1}\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the number of observations. Moreover, we can substitute the expression of the predicted value fror the <span class="math notranslate nohighlight">\(i\)</span>-th observation into the RSS formula using the OLS estimates for the intercept and slope:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{y}_i &amp;= \hat \beta_0 + \hat \beta_1 x_i = (\bar y - \hat \beta_1 \bar x) + \beta_1 x_i \\
\text{RSS} &amp;= \sum{(y_i - \hat{y}_i)^2} \\
&amp;=\sum{[y_i - (\bar y - \hat \beta_1 \bar x + \beta_1 x_i)]^2} \\
&amp;= \sum{[(y_i - \bar y) - \hat \beta_1 (x_i - \bar x)]^2} \\
&amp;= \sum{(y_i - \bar y)^2 + (\hat \beta_1 (x_i - \bar x))^2 - 2 (y_i - \bar y) \hat \beta_1 (x_i - \bar x)} \\
&amp;= \sum{(y_i - \bar y)^2} + \hat \beta_1^2 \sum{(x_i - \bar x)^2} - 2 \hat \beta_1 \sum{(y_i - \bar y) (x_i - \bar x)} \\
&amp;= \text{TSS} + \hat \beta_1^2 (n - 1) s^2_x - 2 \hat \beta_1 (n - 1) s_{xy} \\
&amp;= \text{TSS} + s^2_{xy}/(s^2_x)\cancel{^2} (n - 1) \cancel{s^2_x} - 2 s_{xy}/s^2_x (n - 1) s_{xy} \\
&amp;= \text{TSS} - (n - 1) s_{xy}^2 / s^2_x \\
&amp;= \text{TSS} - R^2 (n - 1) s^2_y \\
\text{RSS} &amp;= \text{TSS} - R^2 \text{TSS} \\
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[R^2 = \frac{\text{TSS} -\text{RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}\]</div>
<p>The code snippet below demonstrates how to calculate R² manually using the <code class="docutils literal notranslate"><span class="pre">compute_rss</span></code> and <code class="docutils literal notranslate"><span class="pre">estimate_y</span></code> functions. It uses the intercept and slope estimated coefficients from the NumPy results to calculate the predicted values, and subsequently the RSS. Finally, it calculates TSS and uses both RSS and TSS to compute R-squared.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define functions to compute RSS and estimate y</span>
<span class="k">def</span> <span class="nf">compute_rss</span><span class="p">(</span><span class="n">y_estimate</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_estimate</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">estimate_y</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">b_0</span><span class="p">,</span> <span class="n">b_1</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">b_0</span> <span class="o">+</span> <span class="n">b_1</span> <span class="o">*</span> <span class="n">x</span>

<span class="c1"># Calculate RSS and TSS</span>
<span class="n">rss</span> <span class="o">=</span> <span class="n">compute_rss</span><span class="p">(</span><span class="n">estimate_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">intercept_numpy</span><span class="p">,</span> <span class="n">slope_numpy</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="n">tss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Calculate and print R-squared</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TSS (manual calculation): </span><span class="si">{</span><span class="n">tss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RSS (manual calculation): </span><span class="si">{</span><span class="n">rss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R² using TSS and RSS: </span><span class="si">{</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">rss</span><span class="o">/</span><span class="n">tss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TSS (manual calculation): 155642.3077
RSS (manual calculation): 63361.3740
R² using TSS and RSS: 0.5929
</pre></div>
</div>
</div>
</div>
<p>We can also access the R², TSS, and RSS values <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html">directly from the Statsmodels results object</a> using its <code class="docutils literal notranslate"><span class="pre">rsquared</span></code>, <code class="docutils literal notranslate"><span class="pre">centered_tss</span></code>, and <code class="docutils literal notranslate"><span class="pre">ssr</span></code> attributes, respectively. This provides a convenient way to retrieve these key metrics for further analysis or comparison.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print R², TSS, and RSS from statsmodels attributes</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TSS (Statsmodels): </span><span class="si">{</span><span class="n">results_statsmodels</span><span class="o">.</span><span class="n">centered_tss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RSS (Statsmodels): </span><span class="si">{</span><span class="n">results_statsmodels</span><span class="o">.</span><span class="n">ssr</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R² (Statsmodels): </span><span class="si">{</span><span class="n">results_statsmodels</span><span class="o">.</span><span class="n">rsquared</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TSS (Statsmodels): 155642.3077
RSS (Statsmodels): 63361.3740
R² (Statsmodels): 0.5929
</pre></div>
</div>
</div>
</div>
<p>To assess how well our regression model explains the outcome (insulin sensitivity), we can compare it to a simple baseline model: <strong>predicting the mean</strong>. Imagine we have no information about the predictor variable (%C20-22 fatty acids). In this case, the best prediction we could make for any individual’s insulin sensitivity would be the overall mean insulin sensitivity.</p>
<p>This “mean model” is represented by a <strong>horizontal line</strong> on the scatter plot, and the deviations of the data points from this line represent the <em>residuals around the mean</em>. The variance of the response variable, denoted as <span class="math notranslate nohighlight">\(s^2_y\)</span>, quantifies the average squared error of this mean model.</p>
<p>By comparing the variability explained by our regression model (captured by R²) to the variability around the mean (captured by <span class="math notranslate nohighlight">\(s^2_y\)</span>), we can gauge how much better our model performs than simply predicting the mean.</p>
<p>We’ll explore this concept in more detail and discuss other methods for comparing models in the next chapter. For now let’s have a look at other metrics can help us assess model fit.</p>
</section>
<section id="adjusted-r-squared">
<h3>Adjusted R-squared<a class="headerlink" href="#adjusted-r-squared" title="Link to this heading">#</a></h3>
<p>While R² provides a useful measure of how well a model fits the data, it has a drawback: it always increases when we add more predictors to the model, even if those predictors don’t truly improve the model’s explanatory power. This can lead to <strong>overfitting</strong>, where the model performs well on the training data but poorly on new, unseen data.</p>
<p><strong>Adjusted R² (<span class="math notranslate nohighlight">\(R^2_a\)</span>)</strong> addresses this issue by taking into account the <em>number of predictors</em> in the model. It <strong>penalizes</strong> the addition of unnecessary predictors that don’t contribute significantly to explaining the variation in the response variable. This helps to prevent overfitting and provides a more realistic assessment of the model’s goodness-of-fit.</p>
<p>The formula for adjusted R² is:</p>
<div class="math notranslate nohighlight">
\[
R^2_a = 1 - (1 - R^2) \frac{n - 1}{n - p - 1}
\]</div>
<p>where <span class="math notranslate nohighlight">\(R^2\)</span> is the R-squared value, <span class="math notranslate nohighlight">\(n\)</span> is the number of observations in the dataset, and <span class="math notranslate nohighlight">\(p\)</span> is the number of predictor variables in the model.</p>
<p>Note that <span class="math notranslate nohighlight">\(n - p - 1\)</span> is the <strong>degrees of freedom of the residuals</strong>. It accounts for the fact that we’ve estimated <span class="math notranslate nohighlight">\(p + 1\)</span> parameters (including the intercept) in the regression model. In simple linear regression (where <span class="math notranslate nohighlight">\(p = 1\)</span>), we’ve estimated two parameters, the intercept and the slope. Using degrees of freedom provides a more accurate estimate of the population variance of the errors. When we estimate parameters from the data, we lose some degrees of freedom, as the estimated parameters impose constraints on the residuals. We can extract the degrees of freedom of the residuals from the fitting model results.</p>
<p>Adjusted R² can be interpreted as the proportion of the variance in the response variable explained by the predictors, <em>adjusted for the number of predictors</em>. It ranges from <span class="math notranslate nohighlight">\(-\inf\)</span> to 1, where a higher value indicates a better fit. Unlike R², adjusted R² can decrease if adding a new predictor doesn’t improve the model’s explanatory power sufficiently to offset the penalty for increased complexity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get number of observations</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">nobs</span>  <span class="c1"># Number of observations; also n=len(data)</span>
<span class="c1">#p = 1  # Number of predictors in simple linear regression</span>
<span class="n">df_residuals</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">df_resid</span>  <span class="c1">#  degrees of freedom of the residuals</span>

<span class="c1"># Calculate adjusted R²</span>
<span class="c1">#adjusted_r_squared = 1 - (1 - r_squared_scipy) * (n - 1) / (n - p - 1)</span>
<span class="n">adjusted_r_squared</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r_squared_scipy</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">df_residuals</span>

<span class="c1"># Print adjusted R² from manual calculation, and from Pingouin and Statsmodels</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted R² (manual):</span><span class="se">\t</span><span class="s2">  </span><span class="si">{</span><span class="n">adjusted_r_squared</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted R² (pingouin):</span><span class="se">\t</span><span class="s2">  </span><span class="si">{</span><span class="n">model_pingouin</span><span class="o">.</span><span class="n">adj_r2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted R² (Statsmodels):</span><span class="si">{</span><span class="n">results_statsmodels</span><span class="o">.</span><span class="n">rsquared_adj</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adjusted R² (manual):	  0.5559
Adjusted R² (pingouin):	  0.5559
Adjusted R² (Statsmodels):0.5559
</pre></div>
</div>
</div>
</div>
<p>In our simple linear regression example, the adjusted R² is slightly lower than the R² value. This is because adjusted R² applies a small penalty even for having one predictor. However, the primary interpretation of R², i.e., the proportion of variance explained, remains the same. The true benefit of adjusted R² will become clearer when we compare models with multiple predictors in later chapters.</p>
</section>
<section id="mean-squared-error">
<h3>Mean squared error<a class="headerlink" href="#mean-squared-error" title="Link to this heading">#</a></h3>
<p><strong>Mean Squared Error (MSE)</strong> is a common metric used to quantify the accuracy of a regression model. It measures the <em>average of the squared differences</em> between the actual (observed) values and the values predicted by the model:</p>
<div class="math notranslate nohighlight">
\[\text{MSE} = \frac{\sum{(y_i - \hat y_i)^2}}{n - p - 1}\]</div>
<p>MSE is closely related to the RSS that we discussed earlier. The only difference is the scaling factor (degrees of freedom of the residuals):</p>
<div class="math notranslate nohighlight">
\[\text{MSE} = \frac{\text{RSS}}{n - p - 1}\]</div>
<p>where <span class="math notranslate nohighlight">\(n - p - 1\)</span> is the <strong>degrees of freedom of the residuals</strong>, as defined for <span class="math notranslate nohighlight">\(R^2_a\)</span>. MSE essentially calculates the average squared “error” of the model’s predictions. Squaring the errors ensures that positive and negative errors contribute equally to the overall measure. A lower MSE indicates that the model’s predictions are, on average, closer to the actual values, suggesting a better fit.</p>
<p>Because MSE squares the errors, it gives more weight to larger errors. This can be useful if you want to penalize larger deviations more heavily. MSE has desirable mathematical properties that make it convenient for optimization and analysis. It’s a widely used and well-understood metric in regression analysis.</p>
<p>However, MSE is expressed in squared units of the response variable, which can be less intuitive for interpretation. And like other metrics based on squared errors, MSE can be sensitive to outliers, as large errors have a disproportionate impact on the overall value.</p>
<p>Despite these limitations, MSE remains a valuable tool for assessing the accuracy of regression models and comparing their performance, as we will see in the next chapter.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">mse_resid</span></code> attribute of the fitted statsmodels result object directly gives the MSE of the residuals. We can also connect this to the RSS by dividing RSS by the number of observations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Access MSE directly</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">mse_resid</span>

<span class="c1"># Calculate MSE from RSS</span>
<span class="n">mse_from_rss</span> <span class="o">=</span> <span class="n">rss</span> <span class="o">/</span> <span class="n">df_residuals</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE (Statsmodels):</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE (from RSS and DF):</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">mse_from_rss</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE (Statsmodels):	 5760.12
MSE (from RSS and DF):	 5760.12
</pre></div>
</div>
</div>
</div>
</section>
<section id="root-mean-squared-error">
<h3>Root mean squared error<a class="headerlink" href="#root-mean-squared-error" title="Link to this heading">#</a></h3>
<p><strong>Root Mean Squared Error (RMSE)</strong> is another commonly used metric to evaluate the accuracy of a regression model. It’s closely related to MSE but offers a more interpretable measure of error. RMSE is simply the square root of MSE:</p>
<div class="math notranslate nohighlight">
\[\text{RMSE} = \sqrt{\text{MSE}}\]</div>
<p>RMSE represents the average magnitude of the errors in the model’s predictions, expressed in the same units as the response variable. It can be interpreted as the “typical” or “standard” deviation of the residuals. A lower RMSE indicates that the model’s predictions are, on average, closer to the actual values, suggesting a better fit.</p>
<p>The main advantage of RMSE over MSE is that it’s in the <em>same units</em> as the response variable. This makes it easier to understand the magnitude of the errors in a meaningful way. RMSE allows for easier comparison of models with different response variables, as the errors are expressed in the original units. RMSE is directly derived from MSE, so they both measure the same underlying concept: the average magnitude of the errors. However, RMSE provides a more interpretable representation of this error.</p>
<p>RMSE is a valuable metric for evaluating regression models due to its interpretability in the original units of the response variable. It provides a clear and understandable measure of the typical error made by the model, making it easier to assess and compare model performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate RMSE from MSE</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE:&quot;</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSE: 75.89548673252867
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="assumptions-and-diagnostics">
<h2>Assumptions and diagnostics<a class="headerlink" href="#assumptions-and-diagnostics" title="Link to this heading">#</a></h2>
<section id="assumptions-for-linear-regression">
<h3>Assumptions for linear regression<a class="headerlink" href="#assumptions-for-linear-regression" title="Link to this heading">#</a></h3>
<p>We’ve explored how to fit a simple linear regression model and assess its overall fit using metrics like R² and MSE. Now, let’s delve deeper into the <strong>assumptions</strong> underlying this model and how we can check for potential problems. These assumptions are essential for ensuring that our results are valid and reliable.</p>
<p>In the previous chapter on correlation analysis, we discussed several important concepts that are also relevant for linear regression. These include:</p>
<ul class="simple">
<li><p><strong>Linearity:</strong> the relationship between the predictor variable and the response variable is linear.</p></li>
<li><p><strong>Normality:</strong> the errors are normally distributed.</p></li>
<li><p><strong>Sphericity</strong>: this assumption refers to the structure of the variance-covariance matrix of the errors. In simpler terms, it means that the variability of the errors is constant across all values of the predictors (<strong>homoscedasticity</strong>) and that the errors are not correlated with each other (<strong>no autocorrelation</strong>). In the context of simple linear regression with only <em>one predictor</em>, sphericity essentially boils down to homoscedasticity. However, in multiple regression, sphericity also encompasses the absence of correlations between the errors associated with different predictors.</p></li>
<li><p><strong>Absence of outliers:</strong> outliers can have a disproportionate influence on the results, so it’s important to identify and handle them appropriately.</p></li>
</ul>
<p>While these were crucial for interpreting correlation coefficients, they are even more critical in the context of linear regression. Violations of these assumptions can affect the accuracy of the estimated coefficients, standard errors, confidence intervals, and P values, potentially leading to misleading conclusions.</p>
<p>Beyond the points mentioned above, there are a few more subtle yet important assumptions that underpin the validity of our analysis:</p>
<ul class="simple">
<li><p><strong>Correct specification:</strong> this means that the chosen model (simple linear regression in our case) accurately reflects the true relationship between the variables. If the true relationship is non-linear or involves other predictors not included in our model, our results might be misleading.</p></li>
<li><p><strong>Strict exogeneity:</strong> this assumption states that the predictor variables are not correlated with the error term. Violating this assumption can lead to biased estimates of the coefficients.</p></li>
<li><p><strong>No perfect multicollinearity:</strong> this assumption is more relevant in multiple regression, where we have multiple predictors. It states that none of the predictors should be a perfect linear combination of the others. If there is perfect multicollinearity, it becomes impossible to estimate the unique effects of individual predictors.</p></li>
</ul>
<p>While normality of the errors is not strictly required for estimating the coefficients, it <em>is</em> essential for valid statistical inference, such as calculating confidence intervals and performing hypothesis tests.</p>
<p>To assess these assumptions, we’ll primarily utilize graphical methods, such as residual plots, histograms, and Q-Q plots. These visual tools provide an intuitive way to identify potential issues. Additionally, we’ll take advantage of the convenient diagnostic table provided by Statsmodels, which offers a concise summary of several key statistical tests.</p>
</section>
<section id="residual-analysis">
<h3>Residual analysis<a class="headerlink" href="#residual-analysis" title="Link to this heading">#</a></h3>
<p>Residual analysis is a cornerstone of evaluating regression models. By examining the <strong>residuals</strong>, i.e., the differences between the <em>observed</em> and <em>predicted</em> values, we can gain valuable insights into the model’s adequacy and potential areas for improvement.</p>
<p>Recall that the residuals represent the “unexplained” portion of the data, the variation that our model doesn’t capture. If the model is appropriate and its assumptions hold, we expect the residuals to behave randomly.</p>
<section id="residual-plots">
<h4>Residual plots<a class="headerlink" href="#residual-plots" title="Link to this heading">#</a></h4>
<p>One of the most informative ways to assess the residuals is by plotting them against the predicted values (or the predictor variable itself). This is a standard diagnostic plot in regression analysis. Ideally, we should see a random scatter of points around zero, with no discernible patterns. Here are some common patterns to watch out for in residual plots:</p>
<ul class="simple">
<li><p><strong>Non-linearity</strong>: if the points form a curve or a U-shape, it suggests that the relationship between the variables might not be linear. This indicates that a simple linear model might not be appropriate, and we might need to consider a more complex model or transformations of the variables.</p></li>
<li><p><strong>Heteroscedasticity</strong>: if the spread of the residuals changes systematically across the range of fitted values (e.g., fanning out or forming a cone shape), it indicates heteroscedasticity (non-constant variance). This violates one of the key assumptions of linear regression and can affect the reliability of our inferences.</p></li>
<li><p><strong>Outliers</strong>: points that lie far away from the others might be outliers. These outliers can have a disproportionate influence on the regression line and can distort the results.  Identifying potential outliers through residual analysis can guide us towards further investigation or specific methods for handling them.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the residuals and fitted values</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">resid</span>
<span class="n">fitted_values</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">fittedvalues</span>

<span class="c1"># Residuals vs. fitted values plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fitted_values</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>  <span class="c1"># Add a horizontal line at zero</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Fitted values&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Residuals&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Residuals vs. fitted values&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/399bee4076edfee62ef60db17c4aef3177f21a2b46aed107cf6693bfcd2ccac2.png" src="_images/399bee4076edfee62ef60db17c4aef3177f21a2b46aed107cf6693bfcd2ccac2.png" />
</div>
</div>
<p>This plot is similar to the <code class="docutils literal notranslate"><span class="pre">sns.residplot</span></code> we used in the previous chapter, as both visualize the residuals against the fitted values. However, we’re now creating the plot manually using Matplotlib to demonstrate how to access and work with the residuals directly from the Statsmodels results.</p>
</section>
<section id="histograms">
<h4>Histograms<a class="headerlink" href="#histograms" title="Link to this heading">#</a></h4>
<p>In addition to plotting the residuals against the predicted values, we can also examine the <em>distribution of the residuals</em> using a histogram. This can provide insights into whether the residuals are approximately normally distributed, which is one of the key assumptions of linear regression.</p>
<p>If the model’s assumptions hold, we expect the histogram of residuals to resemble a bell-shaped curve, indicating a normal distribution. However, if the histogram shows significant skewness (asymmetry) or kurtosis (peakedness or flatness), it might suggest deviations from normality. For example, if the histogram of residuals is heavily skewed to the right, it indicates that the model tends to underpredict the response variable more often than it overpredicts it. This could suggest a need for transformations or a different modeling approach.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Histogram of residuals</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Add a kernel density estimate for smoother visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Residuals&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Histogram of residuals&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bed1ab67bcb29e4356866976a0b85dfd5a636a5225c2b24a45b4add7bc31668e.png" src="_images/bed1ab67bcb29e4356866976a0b85dfd5a636a5225c2b24a45b4add7bc31668e.png" />
</div>
</div>
</section>
<section id="q-q-plots">
<h4>Q-Q plots<a class="headerlink" href="#q-q-plots" title="Link to this heading">#</a></h4>
<p>While histograms provide a general visual impression of the distribution of residuals, <strong>quantile-quantile (Q-Q) plots</strong> offer a more precise way to assess whether the residuals follow a normal distribution.</p>
<p>As we have already seen in a previous chapter on normality and outlier detection, a Q-Q plot compares the quantiles of the residuals to the quantiles of a standard normal distribution. The Q-Q plot plots the quantiles of the residuals on the y-axis against the corresponding quantiles of the <em>standard normal distribution</em> on the x-axis.</p>
<p>If the residuals are normally distributed, the points on the Q-Q plot will fall approximately along a straight diagonal line. But deviations from the straight line indicate departures from normality, in particular:</p>
<ul class="simple">
<li><p>S-shaped curve suggests that the residuals have heavier tails than a normal distribution (more extreme values)</p></li>
<li><p>U-shaped curve indicates lighter tails than a normal distribution (fewer extreme values)</p></li>
<li><p>Points above the line indicate that the residuals are skewed to the right (positive skew)</p></li>
<li><p>Points below the line indicate that the residuals are skewed to the left (negative skew)</p></li>
</ul>
<p>For example, if the points on the Q-Q plot deviate substantially from the straight line at both ends, it suggests that the residuals have heavier tails than a normal distribution. This might indicate the presence of outliers or a distribution with more extreme values than expected in a normal distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.api</span> <span class="kn">import</span> <span class="n">qqplot</span>

<span class="c1"># Q-Q plot of residuals</span>
<span class="n">qqplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">)</span>  <span class="c1"># Assess whether the residuals follow a normal distribution</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Q-Q plot of residuals&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9e4766de1dbab7e3b587d1af96e573905c5520c31c3ae10bef3c332f1b05b78b.png" src="_images/9e4766de1dbab7e3b587d1af96e573905c5520c31c3ae10bef3c332f1b05b78b.png" />
</div>
</div>
<p>While we’ve used Statsmodels to create the Q-Q plot here, remember that we can also utilize other libraries or even implement it manually, as we explored in a previous chapter. Both SciPy and Pingouin offer functions for generating Q-Q plots, each with its own set of customization options.</p>
</section>
<section id="scale-location-plot">
<h4>Scale-location plot<a class="headerlink" href="#scale-location-plot" title="Link to this heading">#</a></h4>
<p>The scale-location plot, also known as the <em>spread-location plot</em>, is another useful visualization for assessing the assumption of homoscedasticity. This plot helps us determine if the residuals are spread equally along the ranges of predictors.</p>
<p>Ideally, we want to see a horizontal line with equally (randomly) spread points in the scale-location plot. This indicates that the variability of the residuals is roughly constant across the range of predicted values, supporting the assumption of homoscedasticity.</p>
<p>However, if we observe a pattern where the spread of the residuals increases or decreases systematically with the predicted values, e.g., a funnel shape, it suggests heteroscedasticity. This means that the variability of the errors is not constant, which can violate the assumptions of linear regression and affect the reliability of our inferences.</p>
<p>For example, if the scale-location plot shows an upward trend, it suggests that the variability of the residuals increases as the predicted values increase. This might indicate that our model is less accurate for predicting higher values of the response variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Retrieve a specific type of residual that is more robust for identifying outliers, more info oon the link below</span>
<span class="c1"># https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.OLSInfluence.get_resid_studentized_external.html</span>
<span class="n">model_norm_residuals</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">get_influence</span><span class="p">()</span><span class="o">.</span><span class="n">resid_studentized_internal</span>
<span class="n">model_norm_residuals_abs_sqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">model_norm_residuals</span><span class="p">))</span>

<span class="c1"># Create the scale-location plot</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">fitted_values</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="n">model_norm_residuals_abs_sqrt</span><span class="p">,</span>
    <span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lowess</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Scale-location plot&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Fitted values&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sqrt{|z_i|}$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1affeac8d31043dc84919be18aa5a6afe46950c15fbc5ee729e26d52d70ae614.png" src="_images/1affeac8d31043dc84919be18aa5a6afe46950c15fbc5ee729e26d52d70ae614.png" />
</div>
</div>
</section>
</section>
<section id="diagnostic-tests">
<h3>Diagnostic tests<a class="headerlink" href="#diagnostic-tests" title="Link to this heading">#</a></h3>
<section id="statsmodels-diagnostic-table">
<h4>Statsmodels diagnostic table<a class="headerlink" href="#statsmodels-diagnostic-table" title="Link to this heading">#</a></h4>
<p>In addition to visual diagnostics, Statsmodels provides a convenient table in the summary output that includes several statistical tests. These tests can offer further insights into the validity of our model’s assumptions and help us identify potential issues.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the diagnostic table</span>
<span class="n">diag_table_statsmodels</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># Print the coefficients table</span>
<span class="nb">print</span><span class="p">(</span><span class="n">diag_table_statsmodels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==============================================================================
Omnibus:                        3.503   Durbin-Watson:                   2.172
Prob(Omnibus):                  0.173   Jarque-Bera (JB):                1.139
Skew:                           0.023   Prob(JB):                        0.566
Kurtosis:                       1.550   Cond. No.                         192.
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>What does this table show:</p>
<ul class="simple">
<li><p>‘Omnibus’: this test assesses the normality of the residuals. It combines measures of skewness and kurtosis to provide an overall test of whether the residuals deviate significantly from a normal distribution.</p></li>
<li><p>‘Prob(Omnibus)’: this is the P value associated with the Omnibus test. A low P value suggests that the residuals are not normally distributed, which could indicate a problem with the model’s assumptions.</p></li>
<li><p>‘Durbin-Watson’: this statistic tests for autocorrelation in the residuals. Autocorrelation occurs when the residuals are not independent of each other, which can violate the assumption of independence. A Durbin-Watson statistic around 2 suggests no autocorrelation, while values significantly less than 2 suggest positive autocorrelation.</p></li>
<li><p>‘Jarque-Bera (JB)’: this test also assesses the normality of the residuals by examining their skewness and kurtosis.  A high p-value suggests that the residuals are normally distributed.</p></li>
<li><p>‘Prob(JB)’: this is the p-value associated with the Jarque-Bera test.</p></li>
<li><p>‘Skew’: this statistic measures the asymmetry of the distribution of the residuals. A skewness of 0 indicates a perfectly symmetrical distribution, while positive values indicate right skewness and negative values indicate left skewness.</p></li>
<li><p>‘Kurtosis’: this statistic measures the “peakedness” of the distribution of the residuals. A kurtosis of 3 indicates a normal distribution, while higher values indicate a more peaked distribution and lower values indicate a flatter distribution.</p></li>
<li><p>‘Cond. No.’: this is the condition number, which measures the sensitivity of the model to small changes in the data. A high condition number can indicate multicollinearity (high correlation between predictors), which is more relevant in multiple regression.</p></li>
</ul>
<p>By examining these diagnostic tests, we can gain further insights into the adequacy of our model and potential areas for improvement. If any of these tests suggest significant deviations from the assumptions, we might need to consider transformations, different modeling approaches, or further investigation of the data.</p>
</section>
<section id="breusch-pagan">
<h4>Breusch-Pagan<a class="headerlink" href="#breusch-pagan" title="Link to this heading">#</a></h4>
<p>In addition to examining the scale-location plot, we can use the Breusch-Pagan test to formally test for heteroscedasticity. This statistical test assesses whether the variance of the errors is related to the predictor variables. While not directly shown in the statsmodels summary output, <a class="reference external" href="https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.het_breuschpagan.html">we can perform this test separately using the Statsmodels library</a>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">het_breuschpagan</span></code> function returns a tuple containing the Lagrange multiplier test statistic, the P value associated with the test statistic, the F-statistic for the auxiliary regression, and the P value associated with the F-statistic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.stats.diagnostic</span> <span class="kn">import</span> <span class="n">het_breuschpagan</span>

<span class="c1"># Perform the Breusch-Pagan test</span>
<span class="n">breusch_pagan_test</span> <span class="o">=</span> <span class="n">het_breuschpagan</span><span class="p">(</span>
    <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">resid</span><span class="p">,</span>
    <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">exog</span>  <span class="c1"># Provides the design matrix (exogenous variables) used in the model</span>
<span class="p">)</span>

<span class="c1"># Print the test results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Breusch-Pagan test results:&quot;</span><span class="p">,</span> <span class="n">breusch_pagan_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P value: </span><span class="si">{</span><span class="n">breusch_pagan_test</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Breusch-Pagan test results: (3.1971404515605957, 0.07376714520088247, 3.5875802150776823, 0.08479389740119875)
P value: 0.08479
</pre></div>
</div>
</div>
</div>
<p>The P value associated with the F-statistic is higher than 0.05, meaning we do not have enough evidence to reject the null hypothesis of homoscedasticity. In simpler terms, it suggests that the variability of the residuals is constant across the range of predictor values. This supports the assumption of homoscedasticity.</p>
</section>
<section id="exogeneity">
<h4>Exogeneity<a class="headerlink" href="#exogeneity" title="Link to this heading">#</a></h4>
<p>Recall our simple linear regression equation <span class="math notranslate nohighlight">\(y = f(x) + \epsilon\)</span>. The <strong>exogeneity</strong> assumption states that the predictor variable (<span class="math notranslate nohighlight">\(x\)</span>) should not be correlated with the error term (<span class="math notranslate nohighlight">\(\epsilon\)</span>). In other words, the factors that influence the error term should be independent of the factors that influence the predictor variable.</p>
<p>Exogeneity is crucial because the residuals we analyze are estimates of the error term. If the predictor variable is correlated with the error term, the <em>residuals will also exhibit a systematic relationship with the predictor</em>, potentially leading to misleading patterns in the residual plots.</p>
<p>Unfortunately, there’s no single statistical test to definitively assess exogeneity. It often requires careful consideration of the relationship between the variables and potential sources of endogeneity (violation of exogeneity), such as omitted variables or measurement error.</p>
<p>However, by examining residual plots and considering the context of our analysis, we can gain some insights into the plausibility of the exogeneity assumption. If the residuals show a clear non-random pattern related to the predictor variable, it might suggest a violation of exogeneity.</p>
</section>
<section id="other-diagnostic-tools">
<h4>Other diagnostic tools<a class="headerlink" href="#other-diagnostic-tools" title="Link to this heading">#</a></h4>
<p>While we’ve focused on visual diagnostics and the Statsmodels diagnostic table in this chapter, other tools can provide further insights into our regression model.</p>
<p>Recall from the chapter on normality and outlier detection that <strong>leverage</strong> measures how far an observation’s predictor values are from the others, while <strong>influence</strong> measures how much an observation affects the regression results. One common measure of influence is <strong>Cook’s distance</strong>, which we explored in detail previously.</p>
<p>Statsmodels provides ways to calculate these measures, allowing us to identify influential data points that might warrant further investigation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the influence summary frame from the fitted model result object</span>
<span class="n">influence_summary</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">get_influence</span><span class="p">()</span><span class="o">.</span><span class="n">summary_frame</span><span class="p">()</span>

<span class="c1"># Print the first few rows</span>
<span class="n">influence_summary</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dfb_Intercept</th>
      <th>dfb_per_C2022_fatacids</th>
      <th>cooks_d</th>
      <th>standard_resid</th>
      <th>hat_diag</th>
      <th>dffits_internal</th>
      <th>student_resid</th>
      <th>dffits</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.432524</td>
      <td>-0.399939</td>
      <td>0.130740</td>
      <td>1.036203</td>
      <td>0.195836</td>
      <td>0.511351</td>
      <td>1.040043</td>
      <td>0.513246</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.125778</td>
      <td>-0.114766</td>
      <td>0.013442</td>
      <td>0.369589</td>
      <td>0.164447</td>
      <td>0.163963</td>
      <td>0.354598</td>
      <td>0.157312</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.246360</td>
      <td>0.224792</td>
      <td>0.049815</td>
      <td>-0.711492</td>
      <td>0.164447</td>
      <td>-0.315643</td>
      <td>-0.694551</td>
      <td>-0.308127</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.414219</td>
      <td>0.376444</td>
      <td>0.132773</td>
      <td>-1.192503</td>
      <td>0.157350</td>
      <td>-0.515311</td>
      <td>-1.218495</td>
      <td>-0.526542</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.149938</td>
      <td>-0.136264</td>
      <td>0.019598</td>
      <td>0.458159</td>
      <td>0.157350</td>
      <td>0.197982</td>
      <td>0.441066</td>
      <td>0.190596</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="consequences-and-remedies">
<h3>Consequences and remedies<a class="headerlink" href="#consequences-and-remedies" title="Link to this heading">#</a></h3>
<p>Why are these assumptions important and what we can do if they are violated?</p>
<p>Violating the assumptions of linear regression can have several consequences:</p>
<ul class="simple">
<li><p>Biased coefficient estimates: if the linearity or exogeneity assumptions are violated, the estimated regression coefficients might be biased, meaning they don’t accurately reflect the true relationships between the variables.</p></li>
<li><p>Invalid inference: if the homoscedasticity or normality assumptions are violated, the standard errors, confidence intervals, and P values might be inaccurate, leading to incorrect conclusions about the statistical significance of the relationships.</p></li>
<li><p>Inefficient estimates: even if the coefficients are unbiased, violating assumptions can lead to less efficient estimates, meaning they have larger standard errors and are less precise.</p></li>
<li><p>Poor predictive accuracy: violations of assumptions can also affect the model’s ability to accurately predict new observations.</p></li>
</ul>
<p>Fortunately, we have several tools to address violations of assumptions:</p>
<ul>
<li><p>Transformations: transforming the predictor or response variable (e.g., taking the logarithm, square root, or reciprocal) can often help address non-linearity or heteroscedasticity.</p></li>
<li><p>Weighted least squares: if we detect heteroscedasticity, where the variability of the errors is not constant, we can use <strong>weighted least squares (WLS)</strong> regression. This method assigns different weights to the observations, giving more weight to those with smaller variances and less weight to those with larger variances. This can help to account for the unequal variances and improve the efficiency of the estimates.
In WLS, the function we minimize, previously denoted as <span class="math notranslate nohighlight">\(S(\boldsymbol{\beta})\)</span>, is modified to incorporate weights:</p>
<div class="math notranslate nohighlight">
\[S(\boldsymbol{\beta}) = \sum_{i=1}^{n} w_i [y_i - f(\mathbf{x}_i, \boldsymbol{\beta})]^2\]</div>
<p>where <span class="math notranslate nohighlight">\(w_i\)</span> represents the weight assigned to the <span class="math notranslate nohighlight">\(i\)</span>-th observation.</p>
</li>
<li><p>If, in addition to heteroscedasticity, there is also correlation between the error terms (<strong>autocorrelation</strong>), then <strong>generalized least squares (GLS)</strong> models can be used. These models are more complex but can account for both unequal variances and correlations in the errors.</p></li>
<li><p>Robust regression: if outliers are a concern, we can use robust regression techniques that are less sensitive to extreme values.</p></li>
<li><p>Non-linear models: if the relationship between the variables is clearly non-linear, we might need to consider non-linear regression models.</p></li>
</ul>
<p>Understanding the consequences of violating assumptions and the available remedies empowers us to take appropriate action and ensure the validity and reliability of our linear regression analysis. By addressing potential issues, we can build more accurate and robust models that provide meaningful insights into the relationships between variables.</p>
</section>
</section>
<section id="statistical-inference-and-uncertainty">
<h2>Statistical inference and uncertainty<a class="headerlink" href="#statistical-inference-and-uncertainty" title="Link to this heading">#</a></h2>
<p>In the previous sections, we explored how to estimate the coefficients of a linear regression model and assess its overall fit. Now, we’ll delve into the realm of inference and uncertainty. This involves quantifying the <strong>uncertainty</strong> associated with our estimates and drawing conclusions about the <strong>population</strong> based on our sample data.</p>
<section id="standard-error-of-the-regression">
<h3>Standard error of the regression<a class="headerlink" href="#standard-error-of-the-regression" title="Link to this heading">#</a></h3>
<p>A key measure of uncertainty in linear regression is the <strong>standard error of the regression (SER)</strong>, also known as the <em>standard error of the estimate (<span class="math notranslate nohighlight">\(S\)</span>)</em>. SER represents the average distance that the observed values fall from the regression line:</p>
<div class="math notranslate nohighlight">
\[S = \sqrt{\frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n - 2}} = \sqrt{\frac{\text{RSS}}{n-2}}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the observed value for the <span class="math notranslate nohighlight">\(i\)</span>-th observation</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value for the <span class="math notranslate nohighlight">\(i\)</span>-th observation</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of observations</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{RSS}\)</span> is the residual sum of squares</p></li>
</ul>
<p>Notice that we divide by <span class="math notranslate nohighlight">\(n - 2\)</span> in the formula, which is the <strong>degrees of freedom of the residuals</strong>. This is because we’ve estimated two parameters in the <em>simple linear regression</em> model: the intercept (<span class="math notranslate nohighlight">\(\beta_0\)</span>) and the slope (<span class="math notranslate nohighlight">\(\beta_1\)</span>). Estimating these parameters consumes two degrees of freedom from the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate S (SER)</span>
<span class="n">ser</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">rss</span> <span class="o">/</span> <span class="n">df_residuals</span><span class="p">)</span>
<span class="c1"># np.sqrt(results_statsmodels.mse_resid)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;S = </span><span class="si">{</span><span class="n">ser</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>S = 75.8955
</pre></div>
</div>
</div>
</div>
<p>SER provides a valuable measure of the <em>overall accuracy</em> of our model. It tells us <em>how much the observed values deviate from the regression line, on averag</em>e, in the same units as the response variable.</p>
<p>Here’s why SER is important and how we can use it:</p>
<ul class="simple">
<li><p>Assess the accuracy of the model: a lower SER generally indicates a better fit, where the observations are closer to the regression line. We can use SER to compare the accuracy of different models (we will discuss this topic in the next chapter about the comparison of models).</p></li>
<li><p>Visualize the uncertainty of the regression line: SER is a key component in calculating <em>confidence bands around the regression line</em>. These bands provide a visual representation of the uncertainty associated with the estimated relationship between the variables, as we will see later.</p></li>
<li><p>Quantify the uncertainty in our predictions: SER helps us understand how much our predictions might deviate from the actual values, on average. This is crucial for constructing prediction intervals, which we’ll discuss later in the chapter.</p></li>
<li><p>Identify potential outliers: we can use SER to calculate standardized residuals, which can help identify potential outliers.</p></li>
<li><p>Understand the overall fit: while R² provides a relative measure of the goodness of fit (the proportion of variance explained), SER provides an absolute measure of the variability around the regression line. Considering both metrics gives us a more complete picture of the model’s performance.</p></li>
</ul>
</section>
<section id="p-values-and-confidence-intervals">
<h3>P values and confidence intervals<a class="headerlink" href="#p-values-and-confidence-intervals" title="Link to this heading">#</a></h3>
<p>The Statsmodels output provides standard errors, t-statistics, P values and confidence intervals for each coefficient in the model. These statistics help us quantify the uncertainty associated with the estimated relationships between the predictors and the response variable.</p>
<p>Remind that a confidence interval provides a range of plausible values for the true <strong>population</strong> value of a coefficient. For example, a 95% confidence interval for the slope tells us that we can be 95% confident that the true slope of the relationship between the predictor and the response falls within that interval.</p>
<p>A P value tests the <em>null hypothesis</em> that the true coefficient is <em>zero</em>. A low P value (typically below 0.05) suggests that the predictor variable has a statistically significant effect on the response variable, meaning that the observed relationship is unlikely to be due to chance alone.</p>
<p>These statistics are essential for drawing meaningful conclusions from our regression analysis. They help us understand not only the magnitude and direction of the relationships but also the level of confidence we can have in our estimates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the coefficients table</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coef_table_statsmodels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>======================================================================================
                         coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------------
Intercept           -486.5420    193.716     -2.512      0.029    -912.908     -60.176
per_C2022_fatacids    37.2077      9.296      4.003      0.002      16.748      57.668
======================================================================================
</pre></div>
</div>
</div>
</div>
<section id="t-tests-for-the-coefficients">
<h4>T-tests for the coefficients<a class="headerlink" href="#t-tests-for-the-coefficients" title="Link to this heading">#</a></h4>
<p>Just as we used a t-test to assess the significance of the correlation coefficient, we can also use a t-test to evaluate the significance of each coefficient in our linear regression model. The formula for the t-statistic is similar, involving the ratio of the estimated coefficient to its standard error. However, in the context of regression, the t-test helps us determine whether a predictor variable has a statistically significant effect on the response variable:</p>
<div class="math notranslate nohighlight">
\[t = \frac{\hat{\beta}_j}{s_{\hat{\beta}_j}}\]</div>
<p>where <span class="math notranslate nohighlight">\(t\)</span> is the t-statistic, <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> is the estimated coefficient for the <span class="math notranslate nohighlight">\(j\)</span>-th predictor variable, and <span class="math notranslate nohighlight">\(s_{\hat{\beta}_j}\)</span> is the standard error of the estimated coefficient.</p>
<p>The standard error of a coefficient measures the variability or uncertainty associated with the estimated coefficient. It can be calculated using the following general formula:</p>
<div class="math notranslate nohighlight">
\[s_{\hat{\beta}_j} = \sqrt{s^2 [(\mathbf{X}^T \mathbf{X})^{-1}]_{j,j}}\]</div>
<p>where <span class="math notranslate nohighlight">\(s^2\)</span> is the estimated variance of the error term, i.e., the square of SER (<span class="math notranslate nohighlight">\(S^2\)</span>), and <span class="math notranslate nohighlight">\([(\mathbf{X}^T \mathbf{X})^{-1}]_{j,j}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th diagonal element of the inverted <span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span> matrix, which represents the variance of the <span class="math notranslate nohighlight">\(j\)</span>-th coefficient estimate.</p>
<p>This leads to the following formula:</p>
<div class="math notranslate nohighlight">
\[
s_{\hat{β}_0} = S \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum(x_i - \bar{x})^2}}
\qquad
s_{\hat{β}_1} = S \sqrt{\frac{1}{\sum(x_i - \bar{x})^2}} 
\]</div>
<p>While we won’t delve into the calculation of these standard errors, it’s important to understand that they are derived from the overall variability of the data and the specific structure of the model. Fortunately, Statsmodels provides the standard errors directly in its output, so we can readily access them without performing the calculations ourselves.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract the coefficients and standard errors from the statsmodels table</span>
<span class="n">intercept_coef</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">]</span>
<span class="n">intercept_stderr</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">]</span>  <span class="c1"># bse contains the standard errors</span>
<span class="n">slope_coef</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;per_C2022_fatacids&quot;</span><span class="p">]</span>
<span class="n">slope_stderr</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="s2">&quot;per_C2022_fatacids&quot;</span><span class="p">]</span>

<span class="c1"># Calculate the t-ratios</span>
<span class="n">t_intercept</span> <span class="o">=</span> <span class="n">intercept_coef</span> <span class="o">/</span> <span class="n">intercept_stderr</span>
<span class="n">t_slope</span> <span class="o">=</span> <span class="n">slope_coef</span> <span class="o">/</span> <span class="n">slope_stderr</span>

<span class="c1"># Print the t-ratios</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t-ratio for Intercept: </span><span class="si">{</span><span class="n">t_intercept</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t-ratio for Slope: </span><span class="si">{</span><span class="n">t_slope</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>t-ratio for Intercept: -2.512
t-ratio for Slope: 4.003
</pre></div>
</div>
</div>
</div>
<p>With the t-statistics and degrees of freedom in hand, we can now determine the P value for the correlation. We’ll use the cumulative distribution function (CDF) of the <strong>t-distribution</strong> to calculate this P value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">t</span> <span class="k">as</span> <span class="n">t_dist</span>

<span class="c1"># Calculate the P values (two-sided test)</span>
<span class="n">intercept_pvalue</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_intercept</span><span class="p">),</span> <span class="n">df_residuals</span><span class="p">))</span>
<span class="n">slope_pvalue</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_slope</span><span class="p">),</span> <span class="n">df_residuals</span><span class="p">))</span>

<span class="c1"># Print the P values</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P value for Intercept: </span><span class="si">{</span><span class="n">intercept_pvalue</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P value for Slope: </span><span class="si">{</span><span class="n">slope_pvalue</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P value for Intercept: 0.02890
P value for Slope: 0.00208
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualizing-t-critical-and-p-values">
<h4>Visualizing t, critical and P values<a class="headerlink" href="#visualizing-t-critical-and-p-values" title="Link to this heading">#</a></h4>
<p>As we’ve done in previous chapters, we can visualize how the P value and critical values are determined using the t-statistic and the t-distribution. This visualization helps us understand the relationship between the calculated t-statistic, the degrees of freedom, and the corresponding P value, providing a clearer picture of the hypothesis testing process.</p>
<p>Notice that the critical t-value (t*) is the same for both coefficients. This is because in simple linear regression, both coefficients share the same degrees of freedom (n - 2), which determine the shape of the t-distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract the t-values for the coefficients</span>
<span class="n">intercept_tvalue</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">tvalues</span><span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">]</span>  <span class="c1"># Extract from the table</span>
<span class="n">slope_tvalue</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">tvalues</span><span class="p">[</span><span class="s2">&quot;per_C2022_fatacids&quot;</span><span class="p">]</span>

<span class="c1"># Significance level (alpha)</span>
<span class="n">α</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="c1"># Calculate critical t-values (two-tailed test)</span>
<span class="n">t_crit</span> <span class="o">=</span> <span class="n">t_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">α</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">df_residuals</span><span class="p">)</span>

<span class="c1"># Generate x values for plotting</span>
<span class="n">x_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">hx_t</span> <span class="o">=</span> <span class="n">t_dist</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">df_residuals</span><span class="p">)</span>

<span class="c1"># Create subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Plot for Intercept</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x_t</span><span class="p">,</span>
    <span class="n">hx_t</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="c1"># Critical value</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=-</span><span class="n">t_crit</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orangered&#39;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">t_crit</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orangered&#39;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;t* (</span><span class="si">{</span><span class="n">t_crit</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Alpha area</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">t_crit</span><span class="p">],</span>
    <span class="n">hx_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">t_crit</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tomato&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;α (</span><span class="si">{</span><span class="n">α</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&gt;=</span> <span class="n">t_crit</span><span class="p">],</span>
    <span class="n">hx_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&gt;=</span> <span class="n">t_crit</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tomato&#39;</span><span class="p">)</span>

<span class="c1"># t-statistic</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">intercept_tvalue</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;limegreen&#39;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;t (</span><span class="si">{</span><span class="n">intercept_tvalue</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># P value area</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_intercept</span><span class="p">)],</span>
    <span class="n">hx_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_intercept</span><span class="p">)],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;greenyellow&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;P (</span><span class="si">{</span><span class="n">intercept_pvalue</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&gt;=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">t_intercept</span><span class="p">)],</span>
    <span class="n">hx_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&gt;=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">t_intercept</span><span class="p">)],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;greenyellow&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;t&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;t-distribution (DF=</span><span class="si">{</span><span class="n">df_residuals</span><span class="si">}</span><span class="s2">) | Intercept&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">margins</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>


<span class="c1"># Plot for Slope</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">hx_t</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=-</span><span class="n">t_crit</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orangered&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">t_crit</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orangered&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;t* (</span><span class="si">{</span><span class="n">t_crit</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">t_crit</span><span class="p">],</span> <span class="n">hx_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">t_crit</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tomato&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;α (</span><span class="si">{</span><span class="n">α</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&gt;=</span> <span class="n">t_crit</span><span class="p">],</span> <span class="n">hx_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&gt;=</span> <span class="n">t_crit</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tomato&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">slope_tvalue</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;limegreen&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;t (</span><span class="si">{</span><span class="n">slope_tvalue</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_slope</span><span class="p">)],</span> <span class="n">hx_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_slope</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;greenyellow&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;P (</span><span class="si">{</span><span class="n">slope_pvalue</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&gt;=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">t_slope</span><span class="p">)],</span> <span class="n">hx_t</span><span class="p">[</span><span class="n">x_t</span> <span class="o">&gt;=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">t_slope</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;greenyellow&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;t&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t-distribution (DF=</span><span class="si">{</span><span class="n">df_residuals</span><span class="si">}</span><span class="s2">) | Slope&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">margins</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cff47d4a7f13ea97c25908764c19faccda5213730838fc7ad8eaa590dfaa01c3.png" src="_images/cff47d4a7f13ea97c25908764c19faccda5213730838fc7ad8eaa590dfaa01c3.png" />
</div>
</div>
</section>
<section id="confidence-interval">
<h4>Confidence interval<a class="headerlink" href="#confidence-interval" title="Link to this heading">#</a></h4>
<p>To calculate the confidence interval, we need to consider the variability of the estimated coefficient, which is captured by its standard error. Here’s how it’s done:</p>
<ol class="arabic simple">
<li><p>Calculate the point estimate: this is the value of the estimated coefficient <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> obtained from the regression analysis.</p></li>
<li><p>Calculate the standard error: the standard error of the coefficient <span class="math notranslate nohighlight">\(s_{\hat \beta_j}\)</span> is a measure of its variability.</p></li>
<li><p>Determine the degrees of freedom: the degrees of freedom for the t-distribution used in constructing the confidence interval are <span class="math notranslate nohighlight">\(n - 2\)</span> in simple linear regression, where <span class="math notranslate nohighlight">\(n\)</span> is the number of observations.</p></li>
<li><p>Find the critical t-value: using the desired confidence level (e.g., 95%) and the calculated degrees of freedom, we can find the corresponding critical t-value (<span class="math notranslate nohighlight">\(t^\ast\)</span>) from the t-distribution table or using statistical software.</p></li>
<li><p>Calculate the margin of error: multiply the standard error by the critical t-value: <span class="math notranslate nohighlight">\(W_j = t^\ast \times s_{\hat \beta_j}\)</span>.</p></li>
<li><p>Construct the confidence interval: subtract and add the margin of error to the point estimate to obtain the lower and upper bounds of the confidence interval: <span class="math notranslate nohighlight">\(\mathrm{CI}_{\beta_j} = \hat{\beta}_j \pm W_j\)</span></p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the confidence interval (e.g., 95% confidence)</span>
<span class="n">confidence_level</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">margin_of_error_intercept</span> <span class="o">=</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">intercept_stderr</span>
<span class="n">ci_intercept</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">intercept_coef</span> <span class="o">-</span> <span class="n">margin_of_error_intercept</span><span class="p">,</span>
    <span class="n">intercept_coef</span> <span class="o">+</span> <span class="n">margin_of_error_intercept</span><span class="p">)</span>
<span class="n">margin_of_error_slope</span> <span class="o">=</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">slope_stderr</span>
<span class="n">ci_slope</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">slope_coef</span> <span class="o">-</span> <span class="n">margin_of_error_slope</span><span class="p">,</span>
    <span class="n">slope_coef</span> <span class="o">+</span> <span class="n">margin_of_error_slope</span><span class="p">)</span>

<span class="c1"># Print the results</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% confidence interval for the Intercept: </span><span class="se">\</span>
<span class="s2">[</span><span class="si">{</span><span class="n">ci_intercept</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_intercept</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% confidence interval for the Slope: </span><span class="se">\</span>
<span class="s2">[</span><span class="si">{</span><span class="n">ci_slope</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_slope</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>95% confidence interval for the Intercept: [-912.908, -60.176]
95% confidence interval for the Slope: [16.748, 57.668]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="visualizing-the-confidence-band">
<h3>Visualizing the confidence band<a class="headerlink" href="#visualizing-the-confidence-band" title="Link to this heading">#</a></h3>
<p>Recall that the SER (S) measures the overall variability of the observed values around the regression line. We can use this measure, together with the critical t-value for the desired confidence level, e.g., 95%, to construct a <strong>confidence band</strong> around the line. The <code class="docutils literal notranslate"><span class="pre">plot_ci_manual()</span></code> function  calculates the margin of error for a confidence interval around the <em>predicted mean response</em> at a <em>specific value of the predictor variable</em> <span class="math notranslate nohighlight">\(x_0\)</span>, as we predict <span class="math notranslate nohighlight">\(\hat y_0 = \hat \beta_0 + \hat \beta_1 x_0\)</span>. The following formula is derived from the standard error of the predicted mean, taking into account both the uncertainty in the estimated coefficients and the variability of the data around the regression line:</p>
<div class="math notranslate nohighlight">
\[\text{CI}(x_0) = \hat y_0 \pm t^\ast \times S \times \sqrt{\frac{1}{n} + \frac{(x_0 - \bar x)^2}{\sum(x_i - \bar x)^2}}\]</div>
<p>As we can see, a larger SER will result in a wider confidence band, reflecting greater uncertainty in the estimated relationship.</p>
<p>To further illustrate the concept of confidence intervals and their role in quantifying uncertainty, let’s visualize the confidence band around our regression line. This visualization will provide a clear and intuitive representation of the range of plausible values for the true relationship between the variables.</p>
<p>We can define a <code class="docutils literal notranslate"><span class="pre">plot_ci_manual()</span></code> function to calculate and plot the 95% confidence (‘ci’) band around the regression line obtained using <code class="docutils literal notranslate"><span class="pre">np.polyfit()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_ci_manual</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">ser</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return an axes of confidence bands using a simple approach.&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

    <span class="n">ci</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">ser</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span> <span class="o">+</span> <span class="p">(</span><span class="n">x0</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span> <span class="o">+</span> <span class="n">ci</span><span class="p">,</span> <span class="n">y0</span> <span class="o">-</span> <span class="n">ci</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;salmon&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">mpatches</span>

<span class="c1"># Plot the data and the fitted line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">slope_numpy</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">intercept_numpy</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;yellow&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Regression line&quot;</span><span class="p">)</span>

<span class="c1"># Calculate and plot the CIs using the previous values for t*, S, n and X</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">slope_numpy</span> <span class="o">*</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">intercept_numpy</span>  <span class="c1"># Calculate predicted values for x0</span>
<span class="n">plot_ci_manual</span><span class="p">(</span><span class="n">t_crit</span><span class="p">,</span> <span class="n">ser</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">)</span>  <span class="c1"># Plot CI on the current axes</span>

<span class="c1"># Create a custom legend handle for the confidence band</span>
<span class="n">confidence_band_patch</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;salmon&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95</span><span class="si">% c</span><span class="s1">onfidence band&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;%C20-22 fatty acids&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Insulin sensitivity (mg/m²/min)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear regression with 95</span><span class="si">% c</span><span class="s2">onfidence band&quot;</span><span class="p">)</span>

<span class="c1"># Get the handles and labels from the automatically generated legend</span>
<span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="c1"># Add the confidence band patch to the handles list</span>
<span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">confidence_band_patch</span><span class="p">)</span>
<span class="c1"># Add the legend to the plot, including all labels</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">handles</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c18f25ea9c7e7f941d3484644583faa871b81a8ebb65b746b75711f91b938583.png" src="_images/c18f25ea9c7e7f941d3484644583faa871b81a8ebb65b746b75711f91b938583.png" />
</div>
</div>
<p>We can achieve a similar visualization <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.get_prediction.html">using our fitted model and its <code class="docutils literal notranslate"><span class="pre">get_prediction</span></code> method</a>. By leveraging its built-in capabilities, we can generate the fitted values and confidence intervals directly, using the ‘mean_ci_lower’ and ‘mean_ci_upper’ values from the prediction results. This streamlined approach eliminates the need for a custom function and showcases the convenience of Statsmodels for visualizing uncertainty in regression analysis. However, the resulting confidence band might appear less smooth compared to the one generated with a denser set of points from <code class="docutils literal notranslate"><span class="pre">np.linspace</span></code>, because it uses only the x-values from the original dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the data and the fitted line (Statsmodels)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;yellow&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Regression line&quot;</span><span class="p">)</span>

<span class="c1"># Extract confidence intervals from Statsmodels predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">get_prediction</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">prediction_summary_frame</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">summary_frame</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="c1"># Plot the confidence intervals</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
    <span class="n">y1</span><span class="o">=</span><span class="n">prediction_summary_frame</span><span class="p">[</span><span class="s1">&#39;mean_ci_lower&#39;</span><span class="p">],</span>
    <span class="n">y2</span><span class="o">=</span><span class="n">prediction_summary_frame</span><span class="p">[</span><span class="s1">&#39;mean_ci_upper&#39;</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;salmon&#39;</span><span class="p">,</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Create a custom legend handle for the confidence band</span>
<span class="n">confidence_band_patch</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;salmon&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95</span><span class="si">% c</span><span class="s1">onfidence band&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;%C20-22 fatty acids&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Insulin sensitivity (mg/m²/min)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear regression with 95</span><span class="si">% c</span><span class="s2">onfidence band&quot;</span><span class="p">)</span>

<span class="c1"># Get the handles and labels from the automatically generated legend</span>
<span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="c1"># Add the confidence band patch to the handles list</span>
<span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">confidence_band_patch</span><span class="p">)</span>
<span class="c1"># Add the legend to the plot, including all labels</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">handles</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c38530ee475b598bd7c90ef700c65c0c26c54c86c925923fe211a22c52ba630b.png" src="_images/c38530ee475b598bd7c90ef700c65c0c26c54c86c925923fe211a22c52ba630b.png" />
</div>
</div>
</section>
<section id="bootstrapping-in-linear-regression">
<h3>Bootstrapping in linear regression<a class="headerlink" href="#bootstrapping-in-linear-regression" title="Link to this heading">#</a></h3>
<p>As we saw in the correlation chapter, bootstrapping is a powerful technique for statistical inference, especially when dealing with small sample sizes or when the assumptions of traditional methods might not be fully met. It allows us to estimate parameters and confidence intervals directly from the data without relying on strong distributional assumptions.</p>
<p>In the context of linear regression, we can use bootstrapping to:</p>
<ul class="simple">
<li><p>Estimate the sampling distribution of the coefficients: by repeatedly resampling the data and fitting the regression model, we can approximate the distribution of the estimated coefficients.</p></li>
<li><p>Calculate confidence intervals: we can use the bootstrap distribution to construct confidence intervals for the coefficients, providing a range of plausible values.</p></li>
<li><p>Visualize confidence bands: we can use bootstrapping to create confidence bands around the regression line, which provide a visual representation of the uncertainty in the estimated relationship between the variables.</p></li>
<li><p>Perform hypothesis tests: we can use the bootstrap distribution to calculate p-values and test hypotheses about the coefficients.</p></li>
</ul>
<section id="generating-bootstrap-samples">
<h4>Generating bootstrap samples<a class="headerlink" href="#generating-bootstrap-samples" title="Link to this heading">#</a></h4>
<p>Just as we did for correlation analysis, the core idea behind bootstrapping in linear regression is to treat our observed sample of paired data points (predictor and response values) as a miniature representation of the population. We resample these pairs <em>with replacement</em>, preserving the relationship between the predictor and response within each pair.</p>
<p>This involves creating many bootstrap samples by repeatedly resampling the original data with replacement. For each bootstrap sample, we fit the linear regression model and obtain the estimated coefficients. This process generates a collection of bootstrap estimates that we can use for inference.</p>
<p>Here’s a step-by-step breakdown of the bootstrapping procedure:</p>
<ol class="arabic simple">
<li><p>Resample the data: randomly sample the data points, i.e. <strong>pairs of predictor AND response values</strong>, with replacement to create a bootstrap sample. The bootstrap sample should have the same size as the original dataset.</p></li>
<li><p>Fit the model: fit the linear regression model to the bootstrap sample and obtain the estimated coefficients.</p></li>
<li><p>Repeat: repeat steps 1 and 2 many times (e.g., 1000 or more) to create a collection of bootstrap estimates for the coefficients.</p></li>
<li><p>Analyze the bootstrap distribution: use the distribution of the bootstrap estimates to calculate confidence intervals, p-values, or other inferential statistics.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">draw_bs_pairs_linreg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform pairs bootstrap for linear regression and return coefficients.&quot;&quot;&quot;</span>

    <span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">bs_intercept_reps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="n">bs_slope_reps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
        <span class="n">bs_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inds</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">bs_x</span><span class="p">,</span> <span class="n">bs_y</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">bs_inds</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bs_inds</span><span class="p">]</span>
        
        <span class="c1"># Fit the linear regression model</span>
        <span class="n">bs_slope_reps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">bs_intercept_reps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">bs_x</span><span class="p">,</span> <span class="n">bs_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">bs_intercept_reps</span><span class="p">,</span> <span class="n">bs_slope_reps</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Set the number of replicates</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="c1"># Generate bootstrap replicates of the intercept and slope</span>
<span class="n">bs_intercept_reps</span><span class="p">,</span> <span class="n">bs_slope_reps</span> <span class="o">=</span> <span class="n">draw_bs_pairs_linreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">B</span><span class="p">)</span>

<span class="c1"># Print the 10 first replicates for the intercept and slope</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept replicates:&quot;</span><span class="p">,</span> <span class="n">bs_intercept_reps</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Slope replicates:&quot;</span><span class="p">,</span> <span class="n">bs_slope_reps</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intercept replicates: [-444.98247966 -296.53975671 -566.17720503 -649.81442009 -546.64129263]
Slope replicates: [34.98583138 27.91417119 41.08623919 45.74051836 42.48376325]
</pre></div>
</div>
</div>
</div>
<p>Without a custom function, we can also implement a more “pythonic” snippet that leads to the same results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Set the number of replicates</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="c1"># Generate bootstrap replicates of the intercept and slope (more Pythonic)</span>
<span class="n">bs_slope_reps</span><span class="p">,</span> <span class="n">bs_intercept_reps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">bs_inds</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bs_inds</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">bs_inds</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">])</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># Transpose to separate intercepts and slopes</span>

<span class="c1"># Print the 10 first replicates for the intercept and slope</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept (β0) replicates:&quot;</span><span class="p">,</span> <span class="n">bs_intercept_reps</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Slope (β1) replicates:&quot;</span><span class="p">,</span> <span class="n">bs_slope_reps</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intercept (β0) replicates: [-444.98247966 -296.53975671 -566.17720503 -649.81442009 -546.64129263]
Slope (β1) replicates: [34.98583138 27.91417119 41.08623919 45.74051836 42.48376325]
</pre></div>
</div>
</div>
</div>
</section>
<section id="constructing-confidence-intervals-from-bootstrap-replicates">
<h4>Constructing confidence intervals from bootstrap replicates<a class="headerlink" href="#constructing-confidence-intervals-from-bootstrap-replicates" title="Link to this heading">#</a></h4>
<p>Now that we have generated a collection of bootstrap replicates for the intercept and slope, we can use them to construct confidence intervals. These intervals provide a range of plausible values for the true population values of the coefficients.</p>
<p>To estimate the confidence interval, we’ll use the percentiles of the bootstrap distribution. For example, to construct a 95% confidence interval, we’ll use the 2.5th and 97.5th percentiles of the bootstrap replicates. This means that 95% of the bootstrapped coefficients fall within this interval, providing a range of plausible values for the true population coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the means, SEs, and 95% confidence intervals</span>
<span class="n">bs_intercept_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bs_intercept_reps</span><span class="p">)</span>
<span class="n">bs_slope_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bs_slope_reps</span><span class="p">)</span>

<span class="n">bs_intercept_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">bs_intercept_reps</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">bs_slope_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">bs_slope_reps</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">bs_intercept_ci</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">bs_intercept_reps</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">]),</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">bs_slope_ci</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">bs_slope_reps</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">]),</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Print the results</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap mean (β0) = </span><span class="si">{</span><span class="n">bs_intercept_m</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap mean (β1) = </span><span class="si">{</span><span class="n">bs_slope_m</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap SE (β0) = </span><span class="si">{</span><span class="n">bs_intercept_s</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap SE (β1) = </span><span class="si">{</span><span class="n">bs_slope_s</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap </span><span class="si">{</span><span class="n">confidence_level</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% CI (β0) = </span><span class="si">{</span><span class="n">bs_intercept_ci</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap </span><span class="si">{</span><span class="n">confidence_level</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% CI (β1) = </span><span class="si">{</span><span class="n">bs_slope_ci</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Bootstrap mean (β0) = -487.47498
Bootstrap mean (β1) = 37.22945
---
Bootstrap SE (β0) = 199.24043
Bootstrap SE (β1) = 9.89711
---
Bootstrap 95% CI (β0) = [-887.886 -113.057]
Bootstrap 95% CI (β1) = [18.705 57.088]
</pre></div>
</div>
</div>
</div>
<p>We’re observing a common phenomenon when comparing confidence intervals obtained from traditional methods (like those in Statsmodels) and those calculated using bootstrapping. While both methods aim to quantify the uncertainty around the estimated coefficients, they can produce slightly different results due to their underlying approaches.</p>
<p>Traditional confidence intervals in Statsmodels are based on the assumption that the errors are normally distributed. If this assumption is not perfectly met, the calculated intervals might not be entirely accurate. Bootstrapping, on the other hand, is a non-parametric method that doesn’t rely on this assumption. It estimates the sampling distribution of the coefficients directly from the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Histogram bootstrap distribution for Intercept</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">bs_intercept_reps</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gold&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Bootstrap $\beta_0$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">intercept_coef</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;cornflowerblue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">fr</span><span class="s1">&#39;Observed $\beta_0$ (</span><span class="si">{</span><span class="n">intercept_coef</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">bs_intercept_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;2.5th (</span><span class="si">{</span><span class="n">bs_intercept_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">bs_intercept_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;97.5th (</span><span class="si">{</span><span class="n">bs_intercept_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_0$&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s2">&quot;Bootstrap distribution of $\beta_0$ (B=</span><span class="si">{</span><span class="n">B</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Histogram bootstrap distribution for Intercept</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">bs_slope_reps</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gold&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Bootstrap $\beta_1$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">slope_coef</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;cornflowerblue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">fr</span><span class="s1">&#39;Observed $\beta_1$ (</span><span class="si">{</span><span class="n">slope_coef</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">bs_slope_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;2.5th (</span><span class="si">{</span><span class="n">bs_slope_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">bs_slope_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;97.5th (</span><span class="si">{</span><span class="n">bs_slope_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_1$&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s2">&quot;Bootstrap distribution of $\beta_1$ (B=</span><span class="si">{</span><span class="n">B</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/aac10dcd507da90a01106303af504db77f03e50e91ce502fe7deddeb5df58a6e.png" src="_images/aac10dcd507da90a01106303af504db77f03e50e91ce502fe7deddeb5df58a6e.png" />
</div>
</div>
<p>As seen in the previous chapters, given the symmetrical distribution of our bootstrapped coefficients, we can also estimate the confidence intervals of the bootstrap distributions using the normal approximation and the bootstrap means and standard errors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Calculate the 95% normal margin of error</span>
<span class="n">z_crit</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">confidence_level</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="n">bs_intercept_w</span> <span class="o">=</span>  <span class="n">z_crit</span> <span class="o">*</span> <span class="n">bs_intercept_s</span>
<span class="n">bs_slope_w</span> <span class="o">=</span>  <span class="n">z_crit</span> <span class="o">*</span> <span class="n">bs_slope_s</span>

<span class="c1"># Calculate normal confidence interval</span>
<span class="n">bs_intercept_ci_normal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span>
    <span class="p">(</span><span class="n">bs_intercept_m</span> <span class="o">-</span> <span class="n">bs_intercept_w</span><span class="p">,</span> <span class="n">bs_intercept_m</span> <span class="o">+</span> <span class="n">bs_intercept_w</span><span class="p">),</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">bs_slope_ci_normal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span>
    <span class="p">(</span><span class="n">bs_slope_m</span> <span class="o">-</span> <span class="n">bs_slope_w</span><span class="p">,</span> <span class="n">bs_slope_m</span> <span class="o">+</span> <span class="n">bs_slope_w</span><span class="p">),</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Print the result</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% normal CI of bootstrap distribution (β0) = </span><span class="si">{</span><span class="n">bs_intercept_ci_normal</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% normal CI of bootstrap distribution (β1) = </span><span class="si">{</span><span class="n">bs_slope_ci_normal</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>95% normal CI of bootstrap distribution (β0) = [-877.97905  -96.97092]
95% normal CI of bootstrap distribution (β1) = [17.83147 56.62742]
</pre></div>
</div>
</div>
</div>
<p>With smaller sample sizes, the traditional confidence intervals might be more sensitive to deviations from normality. Bootstrapping can be more robust in such cases, as it directly captures the variability in the data. On the other side, the bootstrap confidence intervals themselves are subject to some variability due to the random resampling process. If we run the bootstrap with a different seed or a different number of iterations, we might get slightly different intervals.</p>
<p>In the current case, the bootstrap confidence intervals for both the intercept and the slope are slightly narrower than the traditional intervals from Statsmodels. This could suggest that the bootstrap method is capturing the variability in the data more accurately, or it could simply be due to random variation in the bootstrap process.</p>
</section>
<section id="bootstrapping-and-the-confidence-band">
<h4>Bootstrapping and the confidence band<a class="headerlink" href="#bootstrapping-and-the-confidence-band" title="Link to this heading">#</a></h4>
<p>Earlier, we visualized the 95% confidence band around the regression line. Now, let’s explore the concept further by examining how the regression line itself varies across different bootstrap samples. This will provide a more intuitive understanding of the uncertainty captured by the confidence band.</p>
<p>The following plot shows 200 bootstrapped regression lines, i.e., <em>calculated with the coefficients obtained from the previous bootstrap replicates</em>, each fitted to a different resampled dataset. The variability of these lines reflects the uncertainty in the estimated relationship between the variables. Notice how the spread of these lines is related to the width of the confidence band we visualized earlier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the variability of the regression line</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">bs_slope_reps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">bs_intercept_reps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s1">&#39;y-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.35</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orangered&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;%C20-22 fatty acids&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Insulin sensitivity (mg/m²/min)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Bootstrapped regression lines and the confidence band&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e59360ec069659f7cf782c6de774a93eca8a95a13f57fdead8b9c73ce9d312d8.png" src="_images/e59360ec069659f7cf782c6de774a93eca8a95a13f57fdead8b9c73ce9d312d8.png" />
</div>
</div>
<p>We can also use the <a class="reference external" href="https://seaborn.pydata.org/generated/seaborn.regplot.html"><code class="docutils literal notranslate"><span class="pre">sns.regplot()</span></code> function in Seaborn</a> to visualize the variability of the regression line based on bootstrapping. The ‘n_boot’ parameter controls the number of bootstrap iterations used to estimate the confidence interval.</p>
<p>Notice how the spread of the bootstrapped lines in the Seaborn plot is similar to the spread we observed in the previous plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the variability of the regression line (Seaborn)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="mi">95</span><span class="p">,</span> <span class="n">n_boot</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">111</span><span class="p">,)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;%C20-22 fatty acids&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Insulin sensitivity (mg/m²/min)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Visualizing the variability of the regression line (Seaborn)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/333e89ca9a59c3a7007a75245469027828082a37f64846a37cc7e8eb64e2130a.png" src="_images/333e89ca9a59c3a7007a75245469027828082a37f64846a37cc7e8eb64e2130a.png" />
</div>
</div>
<p>While the case resampling (empirical) bootstrap method we discussed throughout this subsection is useful for linear regression, it’s not always the best choice. For instance, when the data has influential observations (points with extreme x-values that strongly influence the regression line), this method might not perform well. In such cases, we can turn to alternative bootstrapping techniques:</p>
<ul class="simple">
<li><p>Residual bootstrap: this approach focuses on resampling the residuals (the differences between the actual and predicted y-values). We generate bootstrap replicates of the residuals and then add them to the predicted y-values to create new bootstrap samples. This helps address the impact of influential observations.</p></li>
<li><p>Wild bootstrap: this method is particularly helpful when the variability of the residuals isn’t constant across the range of predictor values (heteroscedasticity). It uses a more complex procedure to generate bootstrap samples that better account for this varying variability.</p></li>
<li><p>Bootstrap for logistic regression: bootstrapping can also be adapted for logistic regression, a type of regression used when the response variable is categorical (e.g., predicting whether a customer will make a purchase), that we will discussed in a future chapter.</p></li>
</ul>
<p>These are just a few examples of the diverse range of bootstrapping methods available for linear regression. The choice of method depends on the specific characteristics of your data and the goals of your analysis. For a deeper dive into these techniques, we can refer to resources like the <a class="reference external" href="https://faculty.washington.edu/yenchic/17Sp_403/Lec6-bootstrap_reg.pdf">lecture you linked from the University of Washington</a>, which provides detailed explanations and examples.</p>
</section>
<section id="permutation-test-for-p-values">
<h4>Permutation test for P values<a class="headerlink" href="#permutation-test-for-p-values" title="Link to this heading">#</a></h4>
<p>To calculate a P value for our linear regression model, we need a way to <em>simulate the null hypothesis</em>, i.e., the scenario where there’s <em>no relationship between the predictor and response variables</em>. Permutation testing provides a solution. Here’s how it works:</p>
<ol class="arabic simple">
<li><p><strong>Break the association:</strong> we randomly shuffle the values of the predictor variable (X) while keeping the response variable (y) fixed. This effectively eliminates any real relationship between them.</p></li>
<li><p>Fit the model: we fit the same linear regression model to this permuted data (shuffled X and original y) and calculate a test statistic that reflects the strength of the (now non-existent) relationship. We have several options for this test statistic:</p>
<ul class="simple">
<li><p>F-statistic: measures the overall model fit (we will introduce this statistic in the next chapter)</p></li>
<li><p>t-statistic for a specific coefficient: tests the significance of a particular predictor, similar to what we did in the chapters on comparing univariate and bivariate data, and using the formula defined in the previous section <span class="math notranslate nohighlight">\(t = \frac{\hat{\beta}_j}{s_{\hat{\beta}_j}}\)</span></p></li>
<li><p>R²: represents the proportion of variance in y explained by X, though its tendency to skew towards zero under the null hypothesis reduces the statistical power for detecting a true effect when one exists</p></li>
</ul>
</li>
<li><p>Repeat: we repeat steps 1 and 2 many times (e.g., 1,000 or more). Each time, we get a test statistic calculated under the null hypothesis (no real relationship).</p></li>
<li><p>Construct the null distribution: this collection of test statistics from the permuted datasets forms the <em>permutation distribution</em>. It shows us what kinds of test statistic values we’d expect to see if there were truly no relationship between X and y.</p></li>
<li><p>Calculate the P value: we compare the test statistic from our original data to this permutation distribution. The P value is the proportion of permuted test statistics that are as extreme as, or more extreme than, the observed statistic.</p></li>
</ol>
<p>This approach allows us to estimate the P value without relying on the standard assumptions of linear regression, making it a more robust method in many situations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">permute_for_linreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a permuted sample for linear regression </span>
<span class="sd">    under the null hypothesis of no relationship.</span>

<span class="sd">    Args:</span>
<span class="sd">      X: The predictor variable array.</span>
<span class="sd">      y: The response variable array.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A new array with the values of x permuted randomly,</span>
<span class="sd">      and the original y array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Permute the values of X</span>
    <span class="n">permuted_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">permuted_X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Generate 9999 permutation replicates of the t-statistic for slope coefficient</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># 99 rule</span>
<span class="n">perm_slope_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>

<span class="c1"># Generate permuted coefficients</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="c1"># Permute the data</span>
    <span class="n">X_perm</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">permute_for_linreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Fit the model to the permuted data and extract slope and its SE</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">linregress</span><span class="p">(</span><span class="n">X_perm</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">perm_slope_t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">slope</span> <span class="o">/</span> <span class="n">res</span><span class="o">.</span><span class="n">stderr</span> <span class="c1"># type: ignore</span>

<span class="c1"># Print the first 6 replicate bootstrapped t-statistic for the slope</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Permuted t-statistic for β1:&quot;</span><span class="p">,</span> <span class="n">perm_slope_t</span><span class="p">[:</span><span class="mi">6</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Permuted t-statistic for β1: [-0.30696831  0.34960778 -0.22120215 -1.72146729  2.21105111  0.1506495 ]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Same using a more pythonic approach</span>
<span class="n">perm_slope_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># T for last coeff (slope)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Print the first 6 replicate bootstrapped t-statistic for the slope</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Permuted t-statistic for β1:&quot;</span><span class="p">,</span> <span class="n">perm_slope_t</span><span class="p">[:</span><span class="mi">6</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Permuted t-statistic for β1: [-0.30696831  0.34960778 -0.22120215 -1.72146729  2.21105111  0.1506495 ]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the P value using the distribution of bootstrap t-statistic for β1</span>
<span class="c1"># in permuted pairs considering the direction of the observed t-statistic</span>
<span class="c1"># slope_tvalue = results_statsmodels.tvalues.iloc[-1]</span>
<span class="k">if</span> <span class="n">slope_tvalue</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">p_value_slope_t_perm_up</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perm_slope_t</span> <span class="o">&gt;=</span> <span class="n">slope_tvalue</span><span class="p">)</span>
    <span class="n">p_value_slope_t_perm_low</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perm_slope_t</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">slope_tvalue</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">p_value_slope_t_perm_low</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perm_slope_t</span> <span class="o">&lt;=</span> <span class="n">slope_tvalue</span><span class="p">)</span>
    <span class="n">p_value_slope_t_perm_up</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perm_slope_t</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="n">slope_tvalue</span><span class="p">)</span>

<span class="n">p_value_slope_t_perm_1t</span> <span class="o">=</span> <span class="n">p_value_slope_t_perm_up</span> <span class="k">if</span> <span class="n">slope_tvalue</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">p_value_slope_t_perm_low</span>

<span class="c1"># Print the P values</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;One-sided P value for permutation β1 t-statistic = </span><span class="si">{</span><span class="n">p_value_slope_t_perm_1t</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>One-sided P value for permutation β1 t-statistic = 0.00120
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the histogram of the permuted β1 t-statistics</span>
<span class="n">hist</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">perm_slope_t</span><span class="p">,</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">B</span><span class="o">**</span><span class="mf">.5</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gold&#39;</span><span class="p">,</span>
    <span class="c1"># alpha=0.75,</span>
<span class="p">)</span>

<span class="c1"># Annotate the observed mean difference</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">slope_tvalue</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;limegreen&#39;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;Observed $t_{\beta_1}$ &quot;</span><span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">slope_tvalue</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Determine the direction of the observed difference and plot accordingly</span>
<span class="k">if</span> <span class="n">r_squared_scipy</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># type: ignore</span>
    <span class="c1"># Plot the histogram of the mean differences &gt;= observed mean difference </span>
    <span class="n">extreme_diffs</span> <span class="o">=</span> <span class="n">perm_slope_t</span><span class="p">[</span><span class="n">perm_slope_t</span> <span class="o">&gt;=</span> <span class="n">slope_tvalue</span><span class="p">]</span>
    <span class="c1"># Change the color of the bars based on the direction parameter</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">bin_edge</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">bin_edge</span> <span class="o">&gt;=</span> <span class="n">extreme_diffs</span><span class="p">):</span>
            <span class="n">patches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;lime&#39;</span><span class="p">)</span> <span class="c1"># type: ignore</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Plot the histogram of the mean differences &lt;= observed mean difference</span>
    <span class="n">extreme_diffs</span> <span class="o">=</span> <span class="n">perm_slope_t</span><span class="p">[</span><span class="n">perm_slope_t</span> <span class="o">&lt;=</span> <span class="n">slope_tvalue</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">bin_edge</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">bin_edge</span> <span class="o">&lt;=</span> <span class="n">extreme_diffs</span><span class="p">):</span>
            <span class="n">patches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;lime&#39;</span><span class="p">)</span> <span class="c1"># type: ignore</span>

<span class="c1"># Add labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$t_{\beta_1}$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Distribution of $t_{\beta_1}$ under H0 &quot;</span><span class="sa">f</span><span class="s2">&quot;(B=</span><span class="si">{</span><span class="n">B</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Create a copy of the original patch for the legend</span>
<span class="n">original_patch</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;gold&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Permuted $t_{\beta_1}$&#39;</span><span class="p">)</span>
<span class="c1"># Create a patch for the legend</span>
<span class="n">p_value_patch</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lime&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">fr</span><span class="s1">&#39;One-tailed P (</span><span class="si">{</span><span class="n">p_value_slope_t_perm_1t</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="c1"># Add the patches to the legend</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="p">[</span><span class="n">original_patch</span><span class="p">,</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">lines</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p_value_patch</span><span class="p">]);</span> <span class="c1"># type: ignore</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ff864d27383d925090357b9223b9c7f6812256be2a6695e0bf8ce0be19e10fe9.png" src="_images/ff864d27383d925090357b9223b9c7f6812256be2a6695e0bf8ce0be19e10fe9.png" />
</div>
</div>
<p>As we’ve seen in previous chapters on univariate and bivariate analysis, and correlation analysis, we can calculate a two-tailed P value for the t-statistic in our bootstrap analysis. To do this, we sum the probabilities in both tails of the distribution. Given that the bootstrap distribution of the t-statistic for the slope coefficient (<span class="math notranslate nohighlight">\(t_{\beta_0}\)</span>) under the null hypothesis is symmetrical, we can also approximate the two-sided P value by doubling the more conservative of the two one-tailed P values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Maximum one-tail</span>
<span class="n">p_value_permut_2t_max</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="n">p_value_slope_t_perm_low</span><span class="p">,</span> <span class="n">p_value_slope_t_perm_up</span><span class="p">)</span>

<span class="c1"># Sum of the tails</span>
<span class="n">p_value_permut_2t_sum</span> <span class="o">=</span> <span class="n">p_value_slope_t_perm_low</span> <span class="o">+</span> <span class="n">p_value_slope_t_perm_up</span>

<span class="c1"># Print the results</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Two-tailed P value of permutation slope t (conservative) = </span><span class="si">{</span><span class="n">p_value_permut_2t_max</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Two-tailed P value of permutation slope t (both tails) = </span><span class="si">{</span><span class="n">p_value_permut_2t_sum</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Two-tailed P value of permutation slope t (conservative) = 0.00240
Two-tailed P value of permutation slope t (both tails) = 0.00200
</pre></div>
</div>
</div>
</div>
<p>Based on the low P value and its consistency with the Statsmodels and Pingouin output, we can confidently conclude that there is a statistically significant relationship between the predictor and response variables in the data. The bootstrapping approach provides additional support for this conclusion and offers insights into the uncertainty associated with the estimated relationship.</p>
</section>
</section>
</section>
<section id="prediction">
<h2>Prediction<a class="headerlink" href="#prediction" title="Link to this heading">#</a></h2>
<p>We’ve explored how to estimate the coefficients of a linear regression model, assess its assumptions, and perform inference on the coefficients. Now, let’s delve into one of the primary applications of regression analysis: <strong>prediction</strong>.</p>
<p>Recall that we’ve already used the estimated coefficients to predict values and visualize confidence bands around the regression line. We also used predictions within the bootstrap method to assess the variability of the coefficients.</p>
<p>In this section, we’ll expand on these ideas and explore how to use the fitted linear regression model to predict the response variable for new values of the predictor variable. We’ll also discuss how to quantify the uncertainty associated with these predictions using prediction intervals.</p>
<section id="making-predictions">
<h3>Making predictions<a class="headerlink" href="#making-predictions" title="Link to this heading">#</a></h3>
<p>One of the primary goals of building a linear regression model is to use it for prediction. With our fitted model, we can predict the value of the response variable for a new observation, given its value on the predictor variable. Recall the equation of our simple linear regression model: <span class="math notranslate nohighlight">\(y = \hat \beta_0 + \hat \beta_1 x\)</span>. To make a prediction, we simply plug in the new value of <span class="math notranslate nohighlight">\(x\)</span> into the equation, along with the estimated coefficients we obtained from fitting the model!</p>
<p>It’s important to distinguish between two types of prediction:</p>
<ol class="arabic simple">
<li><p><strong>Interpolation:</strong> this refers to predicting the response variable for a new observation where the predictor value falls <em>within</em> the range of the original data. We’re essentially <em>using the fitted line</em> to estimate the response value within the observed range of the predictor.</p></li>
<li><p><strong>Extrapolation:</strong> this refers to predicting the response variable for a new observation where the predictor value falls <em>outside</em> the range of the original data. We should be cautious about extrapolation, as the model’s accuracy might decrease significantly when predicting outside the observed range. The relationship between the variables might not hold beyond the range of our data.</p></li>
</ol>
</section>
<section id="prediction-with-python">
<h3>Prediction with Python<a class="headerlink" href="#prediction-with-python" title="Link to this heading">#</a></h3>
<p>Now that we have a fitted linear regression model, we can use it to predict the response variable for new values of the predictor variable. We can use the Statsmodels model generated ealier to make predictions by providing a new value for the predictor variable, <a class="reference external" href="https://www.statsmodels.org/dev/examples/notebooks/generated/predict.html">as explained brighlty elsewhere</a>. However, we’ll take this opportunity to introduce a <a class="reference external" href="https://scikit-learn.org/stable/">widely used machine learning library in Python: <strong>scikit-learn (sklearn)</strong></a>. It provides a comprehensive and user-friendly set of tools for various <strong>machine learning</strong> tasks, including regression analysis. We can therefore gain experience with a popular machine learning library, expanding our toolkit for data analysis, and learning a standardized approach used for different machine learning algorithms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Create a LinearRegression object</span>
<span class="n">model_sklearn</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># scikit-learn requires a specific data format, think it uses vectors </span>
<span class="c1"># and matrices, so either extract DataFrame, transform Series to DataFrame</span>
<span class="n">X_matrix</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;per_C2022_fatacids&#39;</span><span class="p">]]</span>  <span class="c1"># same as X.to_frame()</span>
<span class="c1"># or reshape X Series values to a 2D NumPy array</span>
<span class="c1"># X_matrix = X.values.reshape(-1, 1) </span>

<span class="c1"># Fit the model to the data</span>
<span class="n">model_sklearn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_matrix</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Extract the coefficients</span>
<span class="n">intercept_sklearn</span> <span class="o">=</span> <span class="n">model_sklearn</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">slope_sklearn</span> <span class="o">=</span> <span class="n">model_sklearn</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Print the coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept (sklearn):&quot;</span><span class="p">,</span> <span class="n">intercept_sklearn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Slope (sklearn):&quot;</span><span class="p">,</span> <span class="n">slope_sklearn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intercept (sklearn): -486.54199459921057
Slope (sklearn): 37.2077457474554
</pre></div>
</div>
</div>
</div>
<p>Remind we previously plotted the regression line using the coefficients obtained from the NumPy linear regression model for each x-value (<code class="docutils literal notranslate"><span class="pre">slope_numpy</span> <span class="pre">*</span> <span class="pre">X</span> <span class="pre">+</span> <span class="pre">intercept_numpy</span></code>), here we use the fitted sklearn model <code class="docutils literal notranslate"><span class="pre">model_sklearn</span></code> to predict the corresponding y-values (<code class="docutils literal notranslate"><span class="pre">y_line</span></code>) for the generated x-values (<code class="docutils literal notranslate"><span class="pre">x_line</span></code>) that finally consist of the two extremity points <span class="math notranslate nohighlight">\([\min(X), \max(X)]\)</span>. The model applies the estimated coefficients to the new x-values to calculate the predicted y-values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span>

<span class="c1"># Predict y-values using the fitted model</span>
<span class="n">y_line</span> <span class="o">=</span> <span class="n">model_sklearn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_line</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># Reshape for sklearn</span>

<span class="c1"># Plot the data and the fitted line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">y_line</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># Plot line using predicted values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;%C20-22 fatty acids&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Insulin sensitivity (mg/m²/min)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear regression using scikit-learn regression model&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0da030f84b482a4809fadcc00ccf135f280be61246d526937d06652affcf113f.png" src="_images/0da030f84b482a4809fadcc00ccf135f280be61246d526937d06652affcf113f.png" />
</div>
</div>
<p>We can use the fitted sklearn model to predict the insulin sensitivity for new values of the predictor variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create new/unseen data points for prediction</span>
<span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">24</span><span class="p">])</span>
<span class="n">X_new_matrix</span> <span class="o">=</span> <span class="n">X_new</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Reshape for sklearn</span>

<span class="c1"># Predict the response variable for the new data points</span>
<span class="n">y_pred_new</span> <span class="o">=</span> <span class="n">model_sklearn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new_matrix</span><span class="p">)</span>

<span class="c1"># Print the predictions</span>
<span class="k">for</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_new_matrix</span><span class="p">,</span> <span class="n">y_pred_new</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted insulin sensitivity for </span><span class="se">\</span>
<span class="si">{</span><span class="n">x_i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">%C20-22 fatty acids: </span><span class="si">{</span><span class="n">y_i</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> mg/m²/min&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted insulin sensitivity for 20%C20-22 fatty acids: 257.61 mg/m²/min
Predicted insulin sensitivity for 24%C20-22 fatty acids: 406.44 mg/m²/min
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-performance">
<h3>Model performance<a class="headerlink" href="#model-performance" title="Link to this heading">#</a></h3>
<p>In machine learning, we often evaluate the performance of a model by comparing its predictions to the actual values. This evaluation helps us understand how well the model generalizes to new, unseen data and identify areas for improvement.</p>
<p>We can evaluate the performance of our <code class="docutils literal notranslate"><span class="pre">model_sklearn</span></code> model using metrics like R², MSE, and RMSE, which we explored earlier in this chapter, and mean absolute error (MAE), sometimes referred to as the “Manhattan norm” because it calculates the average absolute vertical or horizontal distances between the data points and the fitted line.</p>
<p>These metrics offer valuable insights into how well our model fits the data and how accurately it predicts new observations. Calculating these metrics is straightforward and follows the same basic principle for all of them: we simply compare the actual values of the response variable (<code class="docutils literal notranslate"><span class="pre">y</span></code>) to the predicted values (<code class="docutils literal notranslate"><span class="pre">y_pred</span></code>) generated by the model. This comparison allows us to quantify the model’s performance and understand its strengths and weaknesses.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">,</span> <span class="n">mean_absolute_error</span>

<span class="c1"># Predict y-values for all initial x-vlaues using the fitted model</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_sklearn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_matrix</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Calculate evaluation metrics</span>
<span class="n">mse_sklearn</span>  <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">rmse_sklearn</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
<span class="n">r2_sklearn</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Print the metrics with clear labels and formatting</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE</span><span class="se">\t</span><span class="s2">: </span><span class="si">{</span><span class="n">mse_sklearn</span><span class="w"> </span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RMSE</span><span class="se">\t</span><span class="s2">: </span><span class="si">{</span><span class="n">rmse_sklearn</span><span class="w"> </span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R²</span><span class="se">\t</span><span class="s2">: </span><span class="si">{</span><span class="n">r2_sklearn</span><span class="w"> </span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MAE</span><span class="se">\t</span><span class="s2">: </span><span class="si">{</span><span class="n">mae</span><span class="w"> </span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE	: 4873.95184
RMSE	: 75.89549
R²	: 0.59290
MAE	: 65.00429
</pre></div>
</div>
</div>
</div>
<p>We observe a slight difference in the MSE values between <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> and <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>. This might be due to how each library calculates MSE, particularly regarding the use of degrees of freedom. However, the R-squared values are essentially the same, suggesting that the overall goodness of fit is consistent across the two libraries. In fact scikit-learn calculate MSE directly as the average of the squared residuals without explicitely considering degrees of freedom: <span class="math notranslate nohighlight">\(\text{MSE}^\prime = \frac{1}{n} \sum_{i=1}^n (y_i - \hat y_i)^2\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of pairs in the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate and print RSS/n</span>
<span class="n">rss_over_n</span> <span class="o">=</span> <span class="n">rss</span> <span class="o">/</span> <span class="n">n</span>  <span class="c1"># results_statsmodels.nobs</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RSS/n (Statsmodels): </span><span class="si">{</span><span class="n">rss_over_n</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RSS/n (Statsmodels): 4873.95184
</pre></div>
</div>
</div>
</div>
</section>
<section id="prediction-intervals">
<h3>Prediction intervals<a class="headerlink" href="#prediction-intervals" title="Link to this heading">#</a></h3>
<p>We’ve seen how to use our linear regression model to make predictions for new observations. However, it’s important to recognize that these predictions are not perfect. There will always be some uncertainty associated with them due to the inherent variability in the data and the limitations of the model.</p>
<p>To quantify this uncertainty, we can use <strong>prediction intervals</strong>. A prediction interval provides a range of plausible values within which we expect a future observation to fall with a certain level of confidence.</p>
<p>It’s crucial to distinguish between <em>prediction intervals</em> and <em>confidence intervals</em>, which we discussed earlier. Confidence intervals quantify the uncertainty around the estimated <em>mean</em> response at a given predictor value. They tell us how confident we are in the estimated relationship between the variables. Prediction intervals quantify the uncertainty around the predicted value for a <em>single new observation</em>. They tell us how much variability we can expect in individual predictions.</p>
<p>The formula for calculating a prediction interval is similar to that of a confidence interval, but it includes an extra term to account for the variability of individual observations:</p>
<div class="math notranslate nohighlight">
\[
\text{PI}(x_0) = \hat y_0 \pm t^\ast \times S \times \sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar x)^2}{\sum{(x_i - \bar x)^2}}}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat y_0\)</span> is the predicted value at <span class="math notranslate nohighlight">\(x_0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(t^\ast\)</span> is the critical t-value for the desired confidence level</p></li>
<li><p><span class="math notranslate nohighlight">\(S\)</span> is the standard error of the regression (SER)</p></li>
<li><p><span class="math notranslate nohighlight">\(s\)</span> is the number of observations</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i\)</span> are the original predictor values</p></li>
<li><p><span class="math notranslate nohighlight">\(\bar x\)</span> is the mean of the predictor values</p></li>
</ul>
<p>Remind the formula of the confidence interval and notice the extra <span class="math notranslate nohighlight">\(1\)</span> under the square root in the prediction interval formula:</p>
<div class="math notranslate nohighlight">
\[\text{CI}(x_0) = \hat y_0 \pm t^\ast \times S \times \sqrt{\frac{1}{n} + \frac{(x_0 - \bar x)^2}{\sum(x_i - \bar x)^2}}\]</div>
<p>This extra term accounts for the variability of individual observations around the predicted mean.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the data and the fitted line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">y_line</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;yellow&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Regression line&quot;</span><span class="p">)</span>  <span class="c1"># From sklearn</span>

<span class="c1"># Calculate and plot the Cis using the previous values for t*, S, n and X</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">model_sklearn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x0</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># Predict values for x0</span>
<span class="n">plot_ci_manual</span><span class="p">(</span><span class="n">t_crit</span><span class="p">,</span> <span class="n">ser</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">)</span>  <span class="c1"># Plot CI on the current axes</span>

<span class="c1"># Create a custom legend handle for the confidence band</span>
<span class="n">confidence_band_patch</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;salmon&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95</span><span class="si">% c</span><span class="s2">onfidence band&quot;</span><span class="p">)</span>

<span class="c1"># Calculate and plot prediction intervals</span>
<span class="n">pi</span> <span class="o">=</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">ser</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span> <span class="o">+</span> <span class="p">(</span><span class="n">x0</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span> <span class="o">-</span> <span class="n">pi</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;forestgreen&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95% prediction limits&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span> <span class="o">+</span> <span class="n">pi</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;forestgreen&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;%C20-22 fatty acids&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Insulin sensitivity (mg/m²/min)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear regression of insulin sensitivity on %C20-22 fatty acids&quot;</span><span class="p">)</span>

<span class="c1"># Get the handles and labels from the automatically generated legend</span>
<span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="c1"># Add the confidence band patch to the handles list</span>
<span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">confidence_band_patch</span><span class="p">)</span>
<span class="c1"># Add the legend to the plot, including all labels</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">handles</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6f8b77a6b866e906fc1e056092e39ec5b562723b7b76c367cf40cf375e190c24.png" src="_images/6f8b77a6b866e906fc1e056092e39ec5b562723b7b76c367cf40cf375e190c24.png" />
</div>
</div>
<p>Similarly to how we plotted confidence bands, we can visualize prediction interval limits using our fitted model. By leveraging its built-in capabilities, we can generate the predicted values and prediction intervals directly, using the ‘obs_ci_lower’ and ‘obs_ci_upper’ values from the prediction results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the data and the fitted line (Statsmodels)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;yellow&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Regression line&quot;</span><span class="p">)</span>

<span class="c1"># Extract confidence intervals from</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">get_prediction</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">prediction_summary_frame</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">summary_frame</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="c1"># Plot the confidence intervals</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">prediction_summary_frame</span><span class="p">[</span><span class="s1">&#39;mean_ci_lower&#39;</span><span class="p">],</span>
    <span class="n">prediction_summary_frame</span><span class="p">[</span><span class="s1">&#39;mean_ci_upper&#39;</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;salmon&#39;</span><span class="p">,</span>
    <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Create a custom legend handle for the confidence band</span>
<span class="n">confidence_band_patch</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;salmon&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95</span><span class="si">% c</span><span class="s2">onfidence band&quot;</span><span class="p">)</span>

<span class="c1"># Calculate and plot prediction intervals</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">prediction_summary_frame</span><span class="p">[</span><span class="s1">&#39;obs_ci_lower&#39;</span><span class="p">],</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;forestgreen&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95% prediction limits&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">prediction_summary_frame</span><span class="p">[</span><span class="s1">&#39;obs_ci_upper&#39;</span><span class="p">],</span>
    <span class="s2">&quot;--&quot;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;forestgreen&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;%C20-22 fatty acids&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Insulin sensitivity (mg/m²/min)&quot;</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Linear regression of insulin sensitivity on %C20-22 fatty acids&quot;</span><span class="p">)</span>

<span class="c1"># Get the handles and labels from the automatically generated legend</span>
<span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="c1"># Add the confidence band patch to the handles list</span>
<span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">confidence_band_patch</span><span class="p">)</span>
<span class="c1"># Add the legend to the plot, including all labels</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">handles</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fb467427cdf9949baf8e301333a8737359f80bc489768080aa0a3f6c859500db.png" src="_images/fb467427cdf9949baf8e301333a8737359f80bc489768080aa0a3f6c859500db.png" />
</div>
</div>
<p>Prediction intervals are always wider than confidence intervals for the same predictor value and confidence level. This is because prediction intervals account for two sources of uncertainty:</p>
<ol class="arabic simple">
<li><p>Uncertainty in the estimated relationship: this is the same uncertainty captured by the confidence interval</p></li>
<li><p>Random variability of individual observations: even if we knew the true relationship perfectly, individual data points would still vary around the regression line due to random factors</p></li>
</ol>
<p>Prediction intervals incorporate both of these sources of uncertainty, making them wider than confidence intervals, which only account for the first source.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the prediction intervals for each x-value</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X_new</span><span class="p">):</span>
    <span class="c1"># Calculate the prediction interval for the current x-value (x_i)</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">ser</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    
    <span class="c1"># Extract the predicted y-value for the current x-value (from sklearn predictions)</span>
    <span class="n">y_pred_new_i</span> <span class="o">=</span> <span class="n">y_pred_new</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  
    
    <span class="c1"># Print the prediction interval information</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction interval for </span><span class="si">{</span><span class="n">x_i</span><span class="si">:</span><span class="s2">n</span><span class="si">}</span><span class="s2">% C20-22 fatty acids:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - Predicted value: </span><span class="si">{</span><span class="n">y_pred_new_i</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> mg/m²/min&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - Prediction interval: [</span><span class="si">{</span><span class="n">y_pred_new_i</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pi</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">y_pred_new_i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">pi</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">] mg/m²/min&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prediction interval for 20% C20-22 fatty acids:
  - Predicted value: 257.61292 mg/m²/min
  - Prediction interval: [83.6455, 431.5804] mg/m²/min
Prediction interval for 24% C20-22 fatty acids:
  - Predicted value: 406.44390 mg/m²/min
  - Prediction interval: [220.5224, 592.3654] mg/m²/min
</pre></div>
</div>
</div>
</div>
<p>We can also leverage the Statsmodels library to calculate prediction intervals. As we saw for the confidence interval, the <code class="docutils literal notranslate"><span class="pre">get_prediction</span></code> method provides comprehensive prediction results, including prediction intervals. We can extract these intervals and the predicted values using the <code class="docutils literal notranslate"><span class="pre">summary_frame</span></code> method, and store them in a DataFrame with clear column names for the predicted values (‘y_pred’) and the lower and upper bounds of the prediction intervals (‘pi_lo’ and ‘pi_up’).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate prediction intervals and extract relevant values</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">get_prediction</span><span class="p">(</span><span class="n">exog</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">per_C2022_fatacids</span><span class="o">=</span><span class="n">X_new</span><span class="p">))</span>
<span class="n">pred_intervals</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">summary_frame</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)[[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;obs_ci_lower&#39;</span><span class="p">,</span> <span class="s1">&#39;obs_ci_upper&#39;</span><span class="p">]]</span>
<span class="n">pred_intervals</span> <span class="o">=</span> <span class="n">pred_intervals</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span>
    <span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="s2">&quot;y_pred&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;obs_ci_lower&quot;</span><span class="p">:</span> <span class="s2">&quot;pi_lo&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;obs_ci_upper&quot;</span><span class="p">:</span> <span class="s2">&quot;pi_up&quot;</span>
<span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pred_intervals</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>       y_pred       pi_lo       pi_up
0  257.612920   83.645457  431.580383
1  406.443903  220.522381  592.365426
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="advanced-techniques">
<h2>Advanced techniques<a class="headerlink" href="#advanced-techniques" title="Link to this heading">#</a></h2>
<section id="maximum-likelihood-estimation">
<h3>Maximum likelihood estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Link to this heading">#</a></h3>
<p>Throughout this chapter, we’ve focused on the ordinary least squares (OLS) method for estimating the coefficients in our simple linear regression model.  Recall that we briefly mentioned another powerful method for coefficient estimation: <strong>maximum likelihood estimation (MLE)</strong>.</p>
<p>While OLS is intuitive and widely used, MLE offers some advantages, particularly when we have specific knowledge about the distribution of the errors. In this final section, we’ll explore the concept of MLE and how it can be applied in linear regression.</p>
<section id="how-mle-works">
<h4>How MLE works<a class="headerlink" href="#how-mle-works" title="Link to this heading">#</a></h4>
<p>MLE is a powerful method used to <em>estimate the parameters of a probability distribution</em>. It’s particularly useful for choosing between different models and making inferences about the population from which the data is sampled.</p>
<p>Mathematically, MLE works by maximizing the <strong>likelihood function (<span class="math notranslate nohighlight">\(\ell\)</span>)</strong>. This function represents the probability of observing our data, given a set of parameters for the underlying probability distribution. In the case of independent and identically distributed (i.i.d.) data, the likelihood function is simply the <em>product of the probability density function (PDF) values</em> over the entire sample.</p>
<p>Since MLE involves a product of probabilities, it’s often more convenient to work with the <strong>log-likelihood function</strong>. This transforms the product into a sum, making it easier to maximize. We can express the log-likelihood function as:</p>
<div class="math notranslate nohighlight">
\[\ell (\Theta; Y) = \sum \log p(y_i | \Theta)\]</div>
<p>where <span class="math notranslate nohighlight">\(\Theta\)</span> represents the set of parameters we want to estimate, <span class="math notranslate nohighlight">\(Y\)</span> is the random variable representing our data, <span class="math notranslate nohighlight">\(p\)</span> is the probability density function associated with <span class="math notranslate nohighlight">\(Y\)</span>, and <span class="math notranslate nohighlight">\(y_i\)</span> are the individual observed data points.</p>
<p>MLE aims to find the parameter values <span class="math notranslate nohighlight">\(\hat \Theta\)</span> that maximize this log-likelihood function:</p>
<div class="math notranslate nohighlight">
\[\hat \Theta = \operatorname{arg max}_\Theta l(\Theta; Y)\]</div>
</section>
<section id="mle-and-linear-regression">
<h4>MLE and linear regression<a class="headerlink" href="#mle-and-linear-regression" title="Link to this heading">#</a></h4>
<p>Interestingly, in the case of linear regression with the assumption of normally distributed errors, the regression line determined by the OLS method is identical to the line determined by MLE. This means that minimizing the sum of squared errors (OLS) is equivalent to finding the parameter values that maximize the likelihood of observing the data (MLE), given the normality assumption. MLE is actually using the observed y-values and the assumed distribution of the errors (residuals) to estimate the coefficients.</p>
<p>MLE is a method that is used to estimate parameters of a probablililty distribution, and is usefull for model choosing. It is done by maximizing the likelihood function. In the case that we are interested in (i.e. independant identically distributed) this likelihood function is simply the product of a density function values over the entire sample. It is a parametric method since it needs to have an a priory about the density function for it to work. Since it is a product, most of the time we would rather work with the log likelihood function which transforms this product into a sum.</p>
</section>
<section id="visualization-of-mle">
<h4>Visualization of MLE<a class="headerlink" href="#visualization-of-mle" title="Link to this heading">#</a></h4>
<p>To illustrate this concept, we’ll keep the intercept constant and vary the slope of the regression line. This will generate different sets of residuals, which we’ll then use to demonstrate the MLE process.</p>
<p>For each set of residuals, we’ll plot a normal distribution centered at zero. This represents the ideal scenario in linear regression where the errors are normally distributed with a mean of zero. We’ll also use the standard deviation calculated from each set of residuals to define the spread of the normal distribution.</p>
<p>This visualization will help us understand how the likelihood of observing the residuals changes as we vary the slope of the regression line. It will also demonstrate how MLE aims to find the slope value that maximizes this likelihood, effectively minimizing the residuals and achieving the best fit for our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">collections</span> <span class="k">as</span> <span class="n">mc</span>

<span class="c1"># Define different slopes based on the coefficient obtained with OLS</span>
<span class="n">slopes</span> <span class="o">=</span> <span class="p">[</span><span class="n">slope_sklearn</span><span class="p">,</span> <span class="n">slope_sklearn</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">slope_sklearn</span> <span class="o">*</span> <span class="mf">1.35</span><span class="p">]</span>

<span class="c1"># Create subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">slopes</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Estimate the standard deviation of the residuals</span>
<span class="c1"># Calculate the residuals from the scikit-learn model and its predictions</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span>
<span class="n">resid_std</span> <span class="o">=</span> <span class="n">residuals</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="c1"># Optimize the x-range based on the OLS residuals</span>
<span class="n">x_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span> <span class="o">*</span> <span class="n">resid_std</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">resid_std</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Iterate through different slopes</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">slope</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">slopes</span><span class="p">):</span>
    <span class="c1"># Fit the model with the current slope</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">intercept_sklearn</span>  <span class="c1"># Keep the intercept constant</span>
    <span class="n">model</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">slope</span><span class="p">])</span>  <span class="c1"># Set the new slope</span>
    <span class="n">y_pred_i</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_matrix</span><span class="p">)</span>
    <span class="n">residuals_mle</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_pred_i</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="c1"># Plot predicted values as crosses at the bottom</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">residuals_mle</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">residuals_mle</span><span class="p">),</span> <span class="s1">&#39;k+&#39;</span><span class="p">)</span>
    <span class="c1"># Plot the normal distribution</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">resid_std</span><span class="p">),</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>

    <span class="c1"># Calculate predicted likelihoods</span>
    <span class="n">predicted_likelihoods</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">residuals_mle</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">resid_std</span><span class="p">)</span>
    <span class="n">predicted_coords</span> <span class="o">=</span> <span class="p">[[</span><span class="n">v</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">residuals_mle</span><span class="p">,</span> <span class="n">predicted_likelihoods</span><span class="p">)]</span>

    <span class="c1"># Plot segments</span>
    <span class="n">segments</span> <span class="o">=</span> <span class="p">[[[</span><span class="n">v</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">p</span><span class="p">]]</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">residuals_mle</span><span class="p">,</span> <span class="n">predicted_likelihoods</span><span class="p">)]</span>
    <span class="n">lc</span> <span class="o">=</span> <span class="n">mc</span><span class="o">.</span><span class="n">LineCollection</span><span class="p">(</span>
        <span class="n">segments</span><span class="p">,</span>
        <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
        <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted likelihood&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_collection</span><span class="p">(</span><span class="n">lc</span><span class="p">)</span>

    <span class="c1"># Calculate and display log-likelihood</span>
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predicted_likelihoods</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
        <span class="sa">r</span><span class="s2">&quot;$\beta_1$: &quot;</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">slope</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, Log-Likelihood=</span><span class="si">{</span><span class="n">log_likelihood</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Residuals insulin sensitivity&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Likelihood&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d5629637912dbb7e7ed2c4532e8a8afa83c74944deef9c137d93e77e5cf163d7.png" src="_images/d5629637912dbb7e7ed2c4532e8a8afa83c74944deef9c137d93e77e5cf163d7.png" />
</div>
</div>
<p>In the previous visualization, we kept the intercept of our regression model constant and varied the slope. This generated different sets of residuals, effectively shifting them left or right along the x-axis.</p>
<p>For each set of residuals, we calculated the likelihood of observing those residuals under a normal distribution with a mean of zero and the same standard deviation. This demonstrated how the likelihood changed as we shifted the residuals, highlighting the principle behind MLE.</p>
<p>Typically, the MLE algorithm would search for the best values of both the intercept and the slope coefficients, along with the standard deviation of the errors, to find the combination of parameters that maximizes the likelihood of observing the data. In this specific case, where we kept the intercept constant and the errors are assumed to be normally distributed, it has been mathematically proven that the MLE solution for the slope coefficient is identical to the one obtained using the OLS method.</p>
<p>Multiplying these likelihoods together (or, equivalently, summing their logarithms) was exactly what MLE did. It tried different parameter values (in this case, the slope, which indirectly affected the mean of the residuals) to find the combination that maximized the likelihood of observing the data.</p>
<p>While we focus on the case where the residuals are assumed to have a mean of zero in linear regression, it’s important to remember that this is not a general requirement of MLE. MLE can be applied to various distributions and models, where the mean might not be zero. However, the zero-mean assumption is common in linear regression due to its benefits in terms of unbiasedness, interpretability, and mathematical convenience.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the log-likelihood from statsmodels</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log-likelihood (Statsmodels):</span><span class="se">\t</span><span class="si">{</span><span class="n">results_statsmodels</span><span class="o">.</span><span class="n">llf</span><span class="si">:</span><span class="s2">.9f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Print the log-likelihood from the previous visualization</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log-likelihood (MLE):</span><span class="se">\t\t</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
<span class="w">    </span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span><span class="w"> </span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">scale</span><span class="o">=</span><span class="n">resid_std</span><span class="p">)))</span><span class="si">:</span><span class="s2">.9f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Log-likelihood (Statsmodels):	-73.641993232
Log-likelihood (MLE):		-73.662270832
</pre></div>
</div>
</div>
</div>
<p>Recall the Statsmodels results table, which includes the log-likelihood value. We can extract this value directly from the results object. This log-likelihood is very similar to the one we obtained using the coefficients estimated with the OLS method, confirming the close relationship between OLS and MLE in the context of linear regression with normally distributed errors.</p>
</section>
</section>
<section id="generalized-linear-models">
<h3>Generalized linear models<a class="headerlink" href="#generalized-linear-models" title="Link to this heading">#</a></h3>
<p>Throughout this chapter, we’ve focused on simple linear regression where we modeled the relationship between the predictor variable and the <em>mean</em> of the response variable using a linear function.  We’ve also assumed that the errors (the deviations of the observed values from the mean) are normally distributed.</p>
<p>However, there are situations where these assumptions might not hold. The response variable might not be continuous or normally distributed, or the relationship between the variables might not be strictly linear. In such cases, <strong>generalized linear models (GLMs)</strong> offer a more flexible framework.</p>
<p>GLMs extend the concepts of linear regression to <a class="reference external" href="https://en.wikipedia.org/wiki/Generalized_linear_model">accommodate a wider range of response variables and relationships</a>. They allow us to:</p>
<ul class="simple">
<li><p>Model non-normal response variables: we can use GLMs to model response variables that follow different distributions, such as count data (Poisson regression) or binary data (logistic regression).</p></li>
<li><p>Handle non-linear relationships: GLMs can incorporate link functions that transform the relationship between the predictors and the response, allowing us to model non-linear patterns.</p></li>
</ul>
<p>GLM relies on three key components: a <strong>random component</strong> that specifies the probability distribution of the response variable, a <strong>systematic component</strong> that defines the linear predictor, which is a combination of the predictor variables, and a <strong>link function</strong> that connects the random and systematic components, specifying the relationship between the mean of the response variable and the linear predictor.</p>
<p>GLMs offer a flexible and interpretable framework with well-established methods for handling various response variables and relationships, extending the concepts of simple linear regression to a wider range of real-world problems. They open up possibilities for modeling more complex data and relationships, providing a powerful tool for tackling diverse research questions.</p>
<p>While a detailed exploration of GLMs is beyond the scope of this chapter on simple linear regression, it’s essential to be aware of their existence and potential applications. As we progress in our statistical journey, we’ll encounter GLMs and other advanced techniques that build upon the foundation laid by simple linear regression.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In this chapter, we embarked on a journey through the core concepts of simple linear regression. We started by establishing the foundational principles of this powerful technique, learning how to represent relationships between variables using a linear model and the method of Ordinary Least Squares (OLS). We then dove into the practicalities of building and assessing these models in Python, exploring various libraries such as SciPy, Statsmodels, Pingouin, and scikit-learn (our first contact with machine learning), each offering unique advantages and functionalities.</p>
<p>We learned how to estimate the coefficients of the linear equation, interpret their meaning in the context of our data (the relationship between insulin sensitivity and the percentage of C20-22 fatty acids in muscle phospholipids), and assess the overall fit of the model using metrics like R², MSE, and RMSE. We discovered how these metrics provide different perspectives on the model’s performance, with R² capturing the proportion of variance explained and MSE/RMSE quantifying the prediction error.</p>
<p>We then delved into the crucial assumptions that underpin linear regression, recognizing the importance of linearity, homoscedasticity, and normality of errors. We used different diagnostic tools, including residual analysis, visualizations like histograms and Q-Q plots of the residuals, and statistical tests, to critically evaluate our models and identify potential areas for improvement. We discovered how to interpret the diagnostic table provided by <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> to gain further insights into the model’s validity.</p>
<p>We also discussed statistical inference, learning how to quantify the uncertainty associated with our estimated coefficients and predictions. Confidence intervals and P values appeared to be essential in assessing the significance of relationships and drawing meaningful conclusions from the analyses. We explored how to calculate these statistics using both traditional methods and bootstrapping, a powerful resampling technique that allows us to estimate the sampling distribution of the coefficients and perform inference without relying on strong distributional assumptions.</p>
<p>Furthermore, we harnessed the predictive power of linear regression, using our models to forecast outcomes for new observations and understanding the limitations of extrapolation. We learned how to construct prediction intervals, acknowledging the inherent uncertainty in predicting individual values. We also discovered how to visualize the variability of the regression line and the confidence band using bootstrapping, gaining a deeper intuition for the uncertainty in our estimates.</p>
<p>Finally, we glimpsed the broader landscape of regression analysis, touching upon advanced techniques like Maximum Likelihood Estimation (MLE) and Generalized Linear Models (GLMs). We explored the principle of MLE, understanding how it aims to find the parameter values that maximize the likelihood of observing the data. We also briefly introduced GLMs as a more flexible framework for handling different types of response variables and relationships, paving the way for future explorations of more complex models and data structures.</p>
<p>This chapter provided a solid foundation in building and interpreting a single linear regression model. However, we often encounter situations where multiple models compete to best represent the data. In the next chapter, we’ll explore techniques to compare these models, evaluate their performance, and select the best one based on accuracy, interpretability, and generalizability. This will equip us with the tools and knowledge for effective model selection in our data analysis.</p>
</section>
<section id="cheat-sheet">
<h2>Cheat sheet<a class="headerlink" href="#cheat-sheet" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>Fitting model to data<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># With NumPy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">deg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># With SciPy</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">linregress</span>
<span class="n">linregress</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># With Pingouin</span>
<span class="kn">import</span> <span class="nn">pingouin</span> <span class="k">as</span> <span class="nn">pg</span>
<span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># With Statsmodels</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="n">model_statsmodels</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;y ~ X&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">results_statsmodels</span> <span class="o">=</span> <span class="n">model_statsmodels</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="c1"># Print the whole results</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">summary2</span><span class="p">()</span>

<span class="c1"># Extract the coefficient table</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Print the parameters</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>Assessing model fit<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># All of the parameters are extracted from Statsmodels</span>
<span class="c1"># TSS and RSS (SSR)</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">centered_tss</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">ssr</span>

<span class="c1"># R²</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">rsquared</span>

<span class="c1"># Adjusted-R²</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">rsquared_adj</span>

<span class="c1"># MSE and RMSE</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">mse_resid</span>
<span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">results_statsmodels</span><span class="o">.</span><span class="n">mse_resid</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3>Assumptions and diagnostics<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<section id="id4">
<h4>Residual analysis<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Residual plot</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">resid</span>
<span class="n">fitted_values</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">fittedvalues</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fitted_values</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>

<span class="c1"># Histrogram of residuals</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>

<span class="c1"># Q-Q plot</span>
<span class="kn">from</span> <span class="nn">statsmodels.api</span> <span class="kn">import</span> <span class="n">qqplot</span>
<span class="n">qqplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">)</span>

<span class="c1"># Scale-location plot</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">fitted_values</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span>
            <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">get_influence</span><span class="p">()</span><span class="o">.</span><span class="n">resid_studentized_internal</span>
        <span class="p">)</span>
    <span class="p">),</span>
    <span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lowess</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id5">
<h4>Diagnostic tests<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract the diagnostic table from Statsmodel results</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># Breusch-Pagan</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.diagnostic</span> <span class="kn">import</span> <span class="n">het_breuschpagan</span>
<span class="n">het_breuschpagan</span><span class="p">(</span>
    <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">resid</span><span class="p">,</span>
    <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">exog</span><span class="p">)</span>

<span class="c1"># Influence summary frame</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">get_influence</span><span class="p">()</span><span class="o">.</span><span class="n">summary_frame</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="id6">
<h3>Statistical inference and uncertainty<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<section id="p-value-and-confidence-intervals">
<h4>P value and confidence intervals<a class="headerlink" href="#p-value-and-confidence-intervals" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># SER = sqrt(RSS / Df_residuals)</span>
<span class="n">ser</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
    <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">ssr</span> <span class="o">/</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">df_resid</span>
<span class="p">)</span>

<span class="c1"># Extract the coefficient table with P and CI</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Extract confidence interval limits from Statsmodels predictions</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">get_prediction</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">summary_frame</span><span class="p">()[</span>
    <span class="p">[</span><span class="s1">&#39;mean_ci_lower&#39;</span><span class="p">,</span> <span class="s1">&#39;mean_ci_upper&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="bootstrapping">
<h4>Bootstrapping<a class="headerlink" href="#bootstrapping" title="Link to this heading">#</a></h4>
<section id="resampling">
<h5>Resampling<a class="headerlink" href="#resampling" title="Link to this heading">#</a></h5>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the number of replicates</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="c1"># Generate bootstrap replicates of the intercept and slope</span>
<span class="n">bs_slope_reps</span><span class="p">,</span> <span class="n">bs_intercept_reps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">bs_inds</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bs_inds</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">bs_inds</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">])</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># Transpose to separate intercepts and slopes</span>

<span class="c1"># Calculate the means, standard errors, and 95% CIs</span>
<span class="c1"># for the slope coefficient (samme applied to the intercept coefficient)</span>
<span class="n">bs_slope_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bs_slope_reps</span><span class="p">)</span>
<span class="n">bs_slope_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">bs_slope_reps</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">bs_slope_reps</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">])</span>

<span class="c1"># Calculate the 95% normal bootstrap CI for the slope</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="n">z_crit</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">.95</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 95% confidence level</span>
<span class="p">(</span>
    <span class="n">bs_slope_m</span> <span class="o">-</span> <span class="n">z_crit</span> <span class="o">*</span> <span class="n">bs_slope_s</span><span class="p">,</span>
    <span class="n">bs_slope_m</span> <span class="o">+</span> <span class="n">z_crit</span> <span class="o">*</span> <span class="n">bs_slope_s</span>
<span class="p">)</span>

<span class="c1"># Visualizing the variability of the regression line</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="mi">95</span><span class="p">,</span> <span class="n">n_boot</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="permutation">
<h5>Permutation<a class="headerlink" href="#permutation" title="Link to this heading">#</a></h5>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate permutation replicates of the t-statistic for slope coefficient</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># 99 rule</span>
<span class="n">perm_slope_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># T for last coeff (slope)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Compute one-tailed (1t) and two-tailed (2t) P values</span>
<span class="n">observed_t_slope</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">tvalues</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="k">if</span> <span class="n">observed_t_slope</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">p_value_slope_t_perm_up</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perm_slope_t</span> <span class="o">&gt;=</span> <span class="n">observed_t_slope</span><span class="p">)</span>
    <span class="n">p_value_slope_t_perm_low</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perm_slope_t</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">observed_t_slope</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">p_value_slope_t_perm_low</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perm_slope_t</span> <span class="o">&lt;=</span> <span class="n">observed_t_slope</span><span class="p">)</span>
    <span class="n">p_value_slope_t_perm_up</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perm_slope_t</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="n">observed_t_slope</span><span class="p">)</span>

<span class="n">p_value_slope_t_perm_1t</span> <span class="o">=</span> <span class="n">p_value_slope_t_perm_up</span> <span class="k">if</span> <span class="n">slope_tvalue</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">p_value_slope_t_perm_low</span>

<span class="n">p_value_permut_2t_max</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="n">p_value_slope_t_perm_low</span><span class="p">,</span> <span class="n">p_value_slope_t_perm_up</span><span class="p">)</span>  <span class="c1"># Conservative</span>
<span class="n">p_value_permut_2t_sum</span> <span class="o">=</span> <span class="n">p_value_slope_t_perm_low</span> <span class="o">+</span> <span class="n">p_value_slope_t_perm_up</span>  <span class="c1"># Sum of the tails</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="id7">
<h3>Prediction<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">model_sklearn</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">X_matrix</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;per_C2022_fatacids&#39;</span><span class="p">]]</span>  <span class="c1"># Matrix format requirement for sklearn</span>
<span class="n">model_sklearn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_matrix</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Extract the coefficients</span>
<span class="n">intercept_sklearn</span> <span class="o">=</span> <span class="n">model_sklearn</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">slope_sklearn</span> <span class="o">=</span> <span class="n">model_sklearn</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Create new/unseen data points for prediction</span>
<span class="n">X_new_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">24</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Predict</span>
<span class="n">model_sklearn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new_matrix</span><span class="p">)</span>

<span class="c1"># Model performance</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">,</span> <span class="n">mean_absolute_error</span>

<span class="c1"># Predict y-values for all initial x-vlaues using the fitted model</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_sklearn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_matrix</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Calculate MSE, RMSE, R² and MAE metrics</span>
<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  <span class="c1"># DF=n</span>
<span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
<span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Calculate prediction intervals and extract relevant values</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">results_statsmodels</span><span class="o">.</span><span class="n">get_prediction</span><span class="p">(</span><span class="n">exog</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">24</span><span class="p">])))</span>
<span class="n">predictions</span><span class="o">.</span><span class="n">summary_frame</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)[</span>
    <span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;obs_ci_lower&#39;</span><span class="p">,</span> <span class="s1">&#39;obs_ci_upper&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>Advanced techniques<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Log-likelihood</span>
<span class="n">results_statsmodels</span><span class="o">.</span><span class="n">llf</span>
</pre></div>
</div>
</section>
</section>
<section id="session-information">
<h2>Session information<a class="headerlink" href="#session-information" title="Link to this heading">#</a></h2>
<p>The output below details all packages and version necessary to reproduce the results in this report.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>python<span class="w"> </span>--version
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">importlib.metadata</span> <span class="kn">import</span> <span class="n">version</span>

<span class="c1"># List of packages we want to check the version</span>
<span class="n">packages</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;numpy&#39;</span><span class="p">,</span> <span class="s1">&#39;pandas&#39;</span><span class="p">,</span> <span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;seaborn&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy&#39;</span><span class="p">,</span> <span class="s1">&#39;pingouin&#39;</span><span class="p">,</span> <span class="s1">&#39;statsmodels&#39;</span><span class="p">,</span> <span class="s1">&#39;scikit-learn&#39;</span><span class="p">]</span>

<span class="c1"># Initialize an empty list to store the versions</span>
<span class="n">versions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop over the packages</span>
<span class="k">for</span> <span class="n">package</span> <span class="ow">in</span> <span class="n">packages</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Get the version of the package</span>
        <span class="n">package_version</span> <span class="o">=</span> <span class="n">version</span><span class="p">(</span><span class="n">package</span><span class="p">)</span>
        <span class="c1"># Append the version to the list</span>
        <span class="n">versions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">package_version</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>  <span class="c1"># Use a more general exception for broader compatibility</span>
        <span class="n">versions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;Not installed&#39;</span><span class="p">)</span>

<span class="c1"># Print the versions</span>
<span class="k">for</span> <span class="n">package</span><span class="p">,</span> <span class="n">version</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">packages</span><span class="p">,</span> <span class="n">versions</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">package</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Python 3.12.7
-------------
numpy: 1.26.4
pandas: 2.2.2
matplotlib: 3.9.2
seaborn: 0.13.2
scipy: 1.14.1
pingouin: 0.5.5
statsmodels: 0.14.2
scikit-learn: 1.5.1
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="32%20-%20Correlation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Correlation</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-model-to-data">Fitting model to data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#theory-and-definitions">Theory and definitions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares">Least squares</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-coefficients-in-python">Calculating coefficients in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scipy">SciPy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pingouin">Pingouin</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statsmodels">Statsmodels</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#numpy">NumPy</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-model-fit">Assessing model fit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared">R-squared</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusted-r-squared">Adjusted R-squared</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error">Mean squared error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error">Root mean squared error</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-and-diagnostics">Assumptions and diagnostics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-for-linear-regression">Assumptions for linear regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-analysis">Residual analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-plots">Residual plots</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#histograms">Histograms</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#q-q-plots">Q-Q plots</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scale-location-plot">Scale-location plot</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostic-tests">Diagnostic tests</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statsmodels-diagnostic-table">Statsmodels diagnostic table</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#breusch-pagan">Breusch-Pagan</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exogeneity">Exogeneity</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#other-diagnostic-tools">Other diagnostic tools</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#consequences-and-remedies">Consequences and remedies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-inference-and-uncertainty">Statistical inference and uncertainty</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-error-of-the-regression">Standard error of the regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values-and-confidence-intervals">P values and confidence intervals</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#t-tests-for-the-coefficients">T-tests for the coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-t-critical-and-p-values">Visualizing t, critical and P values</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-interval">Confidence interval</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-confidence-band">Visualizing the confidence band</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping-in-linear-regression">Bootstrapping in linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-bootstrap-samples">Generating bootstrap samples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#constructing-confidence-intervals-from-bootstrap-replicates">Constructing confidence intervals from bootstrap replicates</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping-and-the-confidence-band">Bootstrapping and the confidence band</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#permutation-test-for-p-values">Permutation test for P values</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction">Prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#making-predictions">Making predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-with-python">Prediction with Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-performance">Model performance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-intervals">Prediction intervals</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-techniques">Advanced techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum likelihood estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-mle-works">How MLE works</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-and-linear-regression">MLE and linear regression</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-of-mle">Visualization of MLE</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-linear-models">Generalized linear models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cheat-sheet">Cheat sheet</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Fitting model to data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Assessing model fit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Assumptions and diagnostics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Residual analysis</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Diagnostic tests</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Statistical inference and uncertainty</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#p-value-and-confidence-intervals">P value and confidence intervals</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping">Bootstrapping</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling">Resampling</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#permutation">Permutation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Advanced techniques</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#session-information">Session information</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sébastien Wieckowski
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>