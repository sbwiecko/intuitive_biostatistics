
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Multiple regression &#8212; The Python Companion of Intuitive Biostatistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-QQY9FLLPJ8"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-QQY9FLLPJ8');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-QQY9FLLPJ8');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '37 - Multiple regression';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Logistic regression" href="38%20-%20Logistic%20regression.html" />
    <link rel="prev" title="Nonlinear regression" href="36%20-%20Nonlinear%20regression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.jpg" class="logo__image only-light" alt="The Python Companion of Intuitive Biostatistics - Home"/>
    <script>document.write(`<img src="_static/logo.jpg" class="logo__image only-dark" alt="The Python Companion of Intuitive Biostatistics - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    Python Companion Guide to “Intuitive Biostatistics”
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Continuous variables</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09%20-%20Quantifying%20scatter%20of%20continuous%20data.html">Quantifying scatter of continuous data</a></li>
<li class="toctree-l1"><a class="reference internal" href="10%20-%20Gaussian%20distribution.html">The Gaussian distribution</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Confidence intervals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12%20-%20Confidence%20interval%20of%20a%20mean.html">Confidence interval of a mean</a></li>
<li class="toctree-l1"><a class="reference internal" href="04%20-%20Confidence%20interval%20of%20a%20proportion.html">Confidence interval of a proportion</a></li>
<li class="toctree-l1"><a class="reference internal" href="05%20-%20Confidence%20interval%20of%20survival%20data.html">Confidence interval of survival data</a></li>
<li class="toctree-l1"><a class="reference internal" href="06%20-%20Confidence%20interval%20of%20counted%20data.html">Confidence interval of counted data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Statistical significance and data assumptions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="15%20-%20Statistical%20significance.html">Statistical significance</a></li>
<li class="toctree-l1"><a class="reference internal" href="20%20-%20Statistical%20power%20and%20sample%20size.html">Statistical power and sample size</a></li>
<li class="toctree-l1"><a class="reference internal" href="24%20-%20Normality%20tests%20and%20outliers.html">Normality tests and outliers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Statistical tests</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="27%20-%20Comparing%20proportions.html">Comparing proportions</a></li>
<li class="toctree-l1"><a class="reference internal" href="29%20-%20Comparing%20survival%20curves.html">Comparing survival curves</a></li>
<li class="toctree-l1"><a class="reference internal" href="30%20-%20Comparing%20two%20unpaired%20means.html">Comparing two unpaired means</a></li>
<li class="toctree-l1"><a class="reference internal" href="31%20-%20Comparing%20paired%20data.html">Comparing paired data</a></li>
<li class="toctree-l1"><a class="reference internal" href="32%20-%20Correlation.html">Correlation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Fitting models to data</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="33%20-%20Simple%20linear%20regression.html">Simple linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="35%20-%20Comparing%20models.html">Comparing models</a></li>
<li class="toctree-l1"><a class="reference internal" href="36%20-%20Nonlinear%20regression.html">Nonlinear regression</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Multiple regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="38%20-%20Logistic%20regression.html">Logistic regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">The rest of statistics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="39%20-%20ANOVA.html">ANOVA</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sbwiecko/intuitive_biostatistics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sbwiecko/intuitive_biostatistics/edit/master/37 - Multiple regression.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/37 - Multiple regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Multiple regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">Definitions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-multiple-regression">What is multiple regression?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-terminology">Key terminology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mathematical-model">The mathematical model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation">Estimation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-coefficients">Interpreting the coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions">Assumptions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linearity">Linearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-of-errors">Independence of errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#homoscedasticity">Homoscedasticity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normality-of-errors">Normality of errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-perfect-multicollinearity">No perfect multicollinearity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-example">Real-world example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-the-data">Getting the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-regression-model">Building the regression model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-report">Interpretation of the report</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-model-fit">Overall model fit</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficients">Coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#other-diagnostics">Other diagnostics</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-regression-relationships">Visualizing regression relationships</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-and-adjusted-r-squared">R-squared and adjusted R-squared</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-techniques">Advanced techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improvement-of-the-fit">Improvement of the fit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection">Variable selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collinearity-and-multicollinearity">Collinearity and multicollinearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-dimensional-visualization">Three-dimensional visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactions">Interactions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-and-higher-order-polynomial-terms">Quadratic and higher-order polynomial terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-variables">Special variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#removal-of-intercept">Removal of intercept</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-variables">Categorical variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cheat-sheet">Cheat sheet</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Building the regression model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostics">Diagnostics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Visualizing regression relationships</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictions">Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Three-dimensional visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Interactions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Quadratic and higher-order polynomial terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Special variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Removal of intercept</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Categorical variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#session-information">Session information</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="multiple-regression">
<h1>Multiple regression<a class="headerlink" href="#multiple-regression" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In biology, outcomes rarely depend on a single factor. From plant growth to disease risk, multiple variables interact in complex ways. While simple linear regression examines one predictor’s effect on a response, it’s inadequate for understanding these multifaceted scenarios. This is where <strong>multiple regression</strong> becomes essential, providing a statistical framework to disentangle the simultaneous effects of multiple factors.</p>
<p>This chapter will equip us with the tools to apply multiple regression in biological research. We’ll explore how to build models that incorporate the combined influence of <em>several independent variables</em>, such as predicting species abundance based on temperature, rainfall, altitude, and competition, or modeling gene expression as a function of various factors.</p>
<p>We’ll start by defining the mathematical model underlying multiple regression and learn how to use Python’s <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> library to fit these models to our data. We’ll delve into model interpretation, understanding the magnitude, direction, and significance of each predictor’s effect.</p>
<p>We’ll also explore how to evaluate model performance using R-squared, select relevant variables, assess interactions, and incorporate non-linear relationships. Furthermore, we’ll visualize our models and address multicollinearity, where predictors are highly correlated.</p>
<p>Building on the concepts from the previous chapter on simple linear regression, we’ll focus on the practical application of multiple regression using real-world examples, and highlight the key differences and new considerations that arise when dealing with multiple predictors. We’ll streamline our discussion of familiar aspects and encourage you to refer back to the simple linear regression chapter for a more detailed review of those topics.</p>
</section>
<section id="definitions">
<h2>Definitions<a class="headerlink" href="#definitions" title="Link to this heading">#</a></h2>
<section id="what-is-multiple-regression">
<h3>What is multiple regression?<a class="headerlink" href="#what-is-multiple-regression" title="Link to this heading">#</a></h3>
<p>Multiple regression is a statistical technique that allows us to examine the relationship between a <strong>single, continuous dependent variable (Y)</strong> and <strong>two or more independent variables (X)</strong>. It extends the concept of simple linear regression, where only one predictor is used, to a more realistic scenario where multiple factors likely influence the outcome. Biologists often use multiple regression to:</p>
<ol class="arabic simple">
<li><p>Assess the impact of one variable after accounting for others: for instance, is a particular gene associated with increased disease risk <em>even after controlling for</em> age, lifestyle, and other known risk factors?</p></li>
<li><p>Create predictive models: multiple regression can generate an equation to predict the value of the outcome variable based on the values of the independent variables. This could be used to predict crop yield based on rainfall, temperature, and fertilizer use, or to estimate the probability of a species’ occurrence in a new habitat.</p></li>
<li><p>Understand the relative importance of multiple factors: by comparing the strength of the relationship between each independent variable and the dependent variable, multiple regression helps to determine which factors contribute most significantly to explaining the outcome. For example, we could determine whether nutrient availability or light intensity is a stronger driver of plant growth.</p></li>
</ol>
</section>
<section id="key-terminology">
<h3>Key terminology<a class="headerlink" href="#key-terminology" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Dependent variable (Y): also known as the <strong>response variable</strong> or <strong>outcome variable</strong>, this is the variable we are trying to predict or explain. In multiple <em>linear</em> regression, it must be a continuous variable (e.g., plant height, blood pressure, gene expression level).</p></li>
<li><p>Independent variables (X): also known as <strong>explanatory variables</strong> or <strong>predictor variables</strong>, these are the variables we believe influence the dependent variable. They can be:</p>
<ul>
<li><p><strong>Continuous</strong>: variables measured on a continuous scale (e.g., temperature, concentration, body mass).</p></li>
<li><p><strong>Binary</strong>: variables representing two categories, typically coded as 0 and 1 (e.g., presence/absence of a disease, treatment/control group).</p></li>
<li><p><strong>Categorical</strong>: variables representing three or more categories (e.g., habitat type, blood type). These will need to be converted into a set of <em>dummy variables</em> for use in the model. For a variable with <span class="math notranslate nohighlight">\(k\)</span> categories, we create <span class="math notranslate nohighlight">\(k-1\)</span> dummy variables, each coded as 0 or 1. One category is chosen as the <em>reference category</em>, and the coefficients of the dummy variables represent the <em>difference</em> in the predicted value of Y between that category and the reference category.</p></li>
</ul>
</li>
</ul>
<p>While multiple regression deals with <em>multiple</em> X variables (hence sometimes being called “multivariable regression”), it is still a <strong>univariate</strong> technique because it only analyzes a <em>single</em> Y variable. Multivariate methods, such as <em>principal component analysis (PCA)</em> or canonical correlation analysis, analyze multiple dependent variables simultaneously, and is out of scope here.</p>
</section>
<section id="the-mathematical-model">
<h3>The mathematical model<a class="headerlink" href="#the-mathematical-model" title="Link to this heading">#</a></h3>
<p>The multiple linear regression model expresses the dependent variable as a linear combination of the independent variables, plus an error term:</p>
<div class="math notranslate nohighlight">
\[Y_{i} = β_{0} + β_{1}X_{1i} + β_{2}X_{2i} + ... + β_{p}X_{pi} + ε_{i}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y_{i}\)</span> is the value of the dependent variable for the i<sup>th</sup> individual or observation.</p></li>
<li><p><span class="math notranslate nohighlight">\(X_{1i}\)</span>, <span class="math notranslate nohighlight">\(X_{2i}\)</span>, …, <span class="math notranslate nohighlight">\(X_{pi}\)</span> are the values of the independent variables for the i<sup>th</sup> individual.</p></li>
<li><p><span class="math notranslate nohighlight">\(β_{0}\)</span> is the <strong>intercept</strong>, representing the predicted value of Y when all X variables are equal to zero. It’s the baseline value of the outcome.</p></li>
<li><p><span class="math notranslate nohighlight">\(β_{1}\)</span>, <span class="math notranslate nohighlight">\(β_{2}\)</span>, …, <span class="math notranslate nohighlight">\(β_{p}\)</span> are the <strong>regression coefficients</strong> (also called <strong>parameters</strong> or <strong>slopes</strong>). Each coefficient represents the change in the predicted value of Y associated with a one-unit increase in the corresponding X variable, <em>holding all other X variables constant</em>.</p></li>
<li><p><span class="math notranslate nohighlight">\(ε_{i}\)</span> is the <strong>error term</strong> (or <strong>residual</strong>) for the i<sup>th</sup> individual, representing the difference between the observed value of Y and the value predicted by the model. It captures random variability and the influence of unmeasured factors.</p></li>
</ul>
<section id="estimation">
<h4>Estimation<a class="headerlink" href="#estimation" title="Link to this heading">#</a></h4>
<p>As we saw in simple linear regression, the regression coefficients (the <span class="math notranslate nohighlight">\(β\)</span>’s) in multiple regression are also estimated using the method of <strong>Ordinary Least Squares (OLS)</strong>. Recall that OLS seeks to find the coefficients that minimize the sum of the squared differences between the observed values of Y and the values predicted by the model. In essence, it finds the “best-fitting” line (or <em>hyperplane</em> in multiple dimensions) through the data. Fortunately, we don’t have to perform these calculations manually. We’ll use Python’s <code class="docutils literal notranslate"><span class="pre">statsmodels.OLS</span></code> to efficiently estimate the coefficients for our multiple regression models.</p>
</section>
<section id="interpreting-the-coefficients">
<h4>Interpreting the coefficients<a class="headerlink" href="#interpreting-the-coefficients" title="Link to this heading">#</a></h4>
<p>The regression coefficients (<span class="math notranslate nohighlight">\(β\)</span> values) are the heart of the model’s interpretation. Let’s break down what they mean:</p>
<ul class="simple">
<li><p><em>Magnitude:</em> the absolute value of a coefficient indicates the strength of the relationship between that X variable and Y. A larger magnitude suggests a stronger effect.</p></li>
<li><p><em>Sign:</em> the sign (+ or -) of a coefficient indicates the direction of the relationship:</p>
<ul>
<li><p><em>Positive coefficient:</em> as the X variable increases, the predicted value of Y also increases (holding other X’s constant).</p></li>
<li><p><em>Negative coefficient:</em> as the X variable increases, the predicted value of Y decreases (holding other X’s constant).</p></li>
</ul>
</li>
<li><p><em>Units:</em> a coefficient’s units are “units of Y per unit of X”. For example, if <span class="math notranslate nohighlight">\(β_{1}\)</span> = 2.5 for <span class="math notranslate nohighlight">\(X_{1}\)</span> = temperature in degrees Celsius and Y = plant growth in cm, then a 1°C increase in temperature is associated with a 2.5 cm increase in predicted plant growth, holding all other variables constant.</p></li>
<li><p><em>“Holding other variables constant” or “adjusting for”:</em> this phrase is crucial. It highlights that a coefficient reflects the <em>unique</em> contribution of that X variable, after accounting for the effects of all other variables in the model.</p></li>
</ul>
</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Link to this heading">#</a></h4>
<p>Let’s say we model plant growth (<span class="math notranslate nohighlight">\(Y\)</span>) as a function of temperature (<span class="math notranslate nohighlight">\(X_{1}\)</span>) and rainfall (<span class="math notranslate nohighlight">\(X_{2}\)</span>):</p>
<div class="math notranslate nohighlight">
\[\mathrm{Growth}_{i} = 5 + 2.5 \times \mathrm{Temperature}_{i} - 0.1 \times \mathrm{Rainfall}_{i} + ε_{i}\]</div>
<ul class="simple">
<li><p>Intercept (<span class="math notranslate nohighlight">\(\beta_0 = 5\)</span>): a plant is predicted to grow 5 cm even with zero temperature and zero rainfall (this might be extrapolated beyond the observed data range, so be cautious).</p></li>
<li><p>Temperature coefficient (<span class="math notranslate nohighlight">\(\beta_1 = 2.5\)</span>): for every 1°C increase in temperature, plant growth is predicted to increase by 2.5 cm, <em>holding rainfall constant</em>.</p></li>
<li><p>Rainfall coefficient (<span class="math notranslate nohighlight">\(\beta_2 = -0.1\)</span>): for every 1 mm increase in rainfall, plant growth is predicted to <em>decrease</em> by 0.1 cm, <em>holding temperature constant</em>.</p></li>
</ul>
</section>
</section>
</section>
<section id="assumptions">
<h2>Assumptions<a class="headerlink" href="#assumptions" title="Link to this heading">#</a></h2>
<p>Just like simple linear regression, multiple regression relies on certain assumptions about the data and the model. Violating these assumptions can lead to biased or inefficient estimates of the regression coefficients, invalid P values, and ultimately, incorrect conclusions about the relationships between your variables. Therefore, it’s crucial to understand these assumptions and to check them before interpreting your model.</p>
<section id="linearity">
<h3>Linearity<a class="headerlink" href="#linearity" title="Link to this heading">#</a></h3>
<p>The relationship between each independent variable and the dependent variable needs to be linear, <em>after accounting for the effects of other predictors</em>. This means a one-unit change in an X variable is associated with a <strong>constant change</strong> in the predicted value of Y, regardless of the values of other predictors.</p>
<p>We can explore this assumption visually. Scatterplots of the dependent variable against each independent variable can be a starting point. However, these plots only reveal the <em>bivariate</em> relationship and don’t account for the influence of other predictors. Therefore, they might not accurately reflect the linearity assumption in a multiple regression context.</p>
<p>Residual plots offer a more informative view. By plotting the residuals (the differences between observed and predicted Y values) against each independent variable and the predicted Y values, we can look for non-linear patterns. A U-shape or curve in the residuals suggests a non-linear relationship. Also, be aware of increasing or decreasing spread in the residuals, which can indicate non-linearity and violation of another assumption (homoscedasticity).</p>
<p><em>Component-component plus residual (CCPR)</em> plots, also known as added variable plots, are specialized plots that help assess linearity more effectively. They isolate the relationship between each predictor and the response after adjusting for the effects of all other predictors.</p>
<p>Violating the linearity assumption can lead to biased estimates of regression coefficients, inaccurate predictions, and misleading conclusions.</p>
<p>To address non-linearity, we can apply <strong>transformations</strong> to the independent or dependent variables. For example, we might use a logarithmic transformation for skewed data or a square root transformation for count data. Another approach is to <em>include polynomial terms</em> in the model, such as squared or cubed terms of the independent variables, to capture curvature. This allows us to model non-linear relationships within the framework of linear regression.</p>
<p>We will delve deeper into residual plots and CCPR plots in the upcoming sections.</p>
</section>
<section id="independence-of-errors">
<h3>Independence of errors<a class="headerlink" href="#independence-of-errors" title="Link to this heading">#</a></h3>
<p>The independence of errors assumption states that the error terms (residuals) should be independent of each other. This means that the error associated with one observation does not influence the error associated with any other observation.</p>
<p>Violating this assumption can lead to underestimated standard errors of the coefficients, inflated t-statistics, and artificially low P values, increasing the risk of Type I errors (false positives). Why? Because if errors are correlated, the model underestimates the true variability of the estimates.</p>
<p>To assess the independence of errors, we first need to consider the study design. Non-independence often occurs in data with inherent structure, such as:</p>
<ul class="simple">
<li><p><em>Time series data:</em> observations collected over time may exhibit autocorrelation, where errors are correlated with those of neighboring observations.</p></li>
<li><p><em>Repeated measures data:</em> multiple measurements from the same subjects can lead to correlated errors.</p></li>
<li><p><em>Clustered data:</em> observations within the same group (e.g., students in a classroom, animals in a litter) may be more similar to each other and have correlated errors.</p></li>
</ul>
<p>We can also use visual diagnostics to detect non-independence. Plotting residuals against the order of data collection can reveal patterns such as clusters or trends. Autocorrelation plots can help visualize correlations between errors at different time lags. Additionally, the <strong>Durbin-Watson test</strong> can be used to formally test for autocorrelation in time series data. Statsmodels automatically calculates the Durbin-Watson statistic and includes it in the summary output of the OLS regression. We can then interpret this statistic to assess the presence of autocorrelation in the residuals, with a value around 2 suggesting no autocorrelation, while values closer to 0 indicate positive autocorrelation and values closer to 4 indicate negative autocorrelation.”</p>
<p>Addressing non-independence requires using appropriate models that account for the data structure:</p>
<ul class="simple">
<li><p><em>Time series data:</em> Time series models, like ARIMA models, explicitly model the autocorrelation in the data.</p></li>
<li><p><em>Repeated measures/clustered data:</em> Mixed-effects models or <em>generalized estimating equations (GEEs)</em> account for the correlation within subjects or clusters.</p></li>
<li><p><em>Generalized Least Squares:</em> GLS can be used in various situations to account for different correlation structures in the errors.</p></li>
</ul>
<p>While a detailed exploration of these methods is beyond the scope of this book, it’s important to be aware of them. If your data involves time series, repeated measures, or clustered observations, we recommend consulting resources specializing in these types of analyses to ensure you’re using the most appropriate techniques.”</p>
</section>
<section id="homoscedasticity">
<h3>Homoscedasticity<a class="headerlink" href="#homoscedasticity" title="Link to this heading">#</a></h3>
<p>The homoscedasticity assumption states that the variability of the errors should be consistent across all levels of the independent variables. Imagine plotting the residuals against the predicted values - we should see a roughly even spread of points, like a random cloud, without any noticeable patterns. If the spread of the residuals widens or narrows as the predicted values change (forming a cone or fan shape), then the homoscedasticity assumption might be violated. This is called heteroscedasticity.</p>
<p>When heteroscedasticity is present, our standard errors for the regression coefficients might be inaccurate. Although the coefficients themselves are still unbiased (meaning they tend to be centered around the true values), the model’s uncertainty about those coefficients is misrepresented. This can lead to misleading P values and confidence intervals, potentially affecting our conclusions about which predictors are statistically significant.</p>
<p>How can we address heteroscedasticity?</p>
<ul class="simple">
<li><p><em>Transformations:</em> sometimes, transforming the dependent variable (e.g., using a log or square root transformation) can help stabilize the variance.</p></li>
<li><p><em>Weighted Least Squares (WLS):</em> this method assigns different weights to observations based on their estimated variance, giving more weight to observations with smaller variance.</p></li>
<li><p><em>Robust Standard Errors:</em> these methods provide more reliable standard errors even when heteroscedasticity is present.</p></li>
</ul>
<p>While statistical tests like the Breusch-Pagan test and White test can formally assess homoscedasticity, we’ll primarily rely on visual inspection of residual plots in this book. If you encounter potential heteroscedasticity, we encourage you to explore the more advanced techniques mentioned above.</p>
</section>
<section id="normality-of-errors">
<h3>Normality of errors<a class="headerlink" href="#normality-of-errors" title="Link to this heading">#</a></h3>
<p>The normality of errors assumption states that the residuals from our model should follow a normal distribution. We can visually check this by examining a <strong>histogram</strong> of the residuals or by creating a <strong>Q-Q plot</strong> (Quantile-Quantile plot). In a Q-Q plot, if the residuals are normally distributed, the points should fall approximately along a straight diagonal line.</p>
<p>While formal statistical tests like the <em>Shapiro-Wilk</em> test or <em>Kolmogorov-Smirnov</em> test can be used to assess normality, these tests can sometimes be overly sensitive, especially with larger sample sizes. In practice, visual inspection of the histogram and Q-Q plot often provides sufficient information for our purposes.</p>
<p>Why is normality important? If the errors are not normally distributed, the P values and confidence intervals associated with our regression coefficients might be inaccurate, particularly when dealing with small sample sizes. However, multiple regression is generally <strong>robust</strong> to moderate departures from normality, especially when we have larger datasets. This robustness is thanks to the Central Limit Theorem, which states that the sampling distribution of the coefficients tends towards normality as the sample size increases.</p>
<p>If we encounter substantial deviations from normality, here are a few options:</p>
<ul class="simple">
<li><p><em>Transformations:</em>  transforming the dependent variable (e.g., using a log or square root transformation) can sometimes help normalize the residuals.</p></li>
<li><p><em>Bootstrapping:</em> this resampling technique can provide more accurate confidence intervals when normality is violated.</p></li>
<li><p><em>Non-parametric methods:</em>  if transformations and bootstrapping don’t suffice, we might consider non-parametric regression methods that don’t rely on the normality assumption.</p></li>
</ul>
<p>In many cases, simply increasing the sample size can also mitigate the impact of non-normality.</p>
</section>
<section id="no-perfect-multicollinearity">
<h3>No perfect multicollinearity<a class="headerlink" href="#no-perfect-multicollinearity" title="Link to this heading">#</a></h3>
<p>The no perfect multicollinearity assumption means that none of our independent variables should be a perfect linear combination of the others. In simpler terms, we shouldn’t be able to perfectly predict one predictor just by using the others.</p>
<p>Think of it like this: if we included “total cholesterol” and also “HDL cholesterol” and “LDL cholesterol” as separate predictors in a model, we’d run into a problem. Since total cholesterol is simply the sum of HDL and LDL, there’s redundant information. This redundancy makes it difficult for the model to determine the independent effect of each type of cholesterol.</p>
<p>What happens when multicollinearity is present?</p>
<ul class="simple">
<li><p><em>Unstable coefficients:</em> the estimated coefficients become very sensitive to slight changes in the data, making them unreliable.</p></li>
<li><p><em>Inflated standard errors:</em> the standard errors of the coefficients become much larger, making it harder to detect statistically significant effects.</p></li>
<li><p><em>Difficult interpretation:</em> It becomes challenging to isolate the unique contribution of each predictor to the outcome.</p></li>
</ul>
<p>While perfect multicollinearity is rare in practice, high degrees of correlation between predictors can still cause problems. We’ll discuss how to detect and address multicollinearity later in this chapter.</p>
</section>
</section>
<section id="real-world-example">
<h2>Real-world example<a class="headerlink" href="#real-world-example" title="Link to this heading">#</a></h2>
<section id="getting-the-data">
<h3>Getting the data<a class="headerlink" href="#getting-the-data" title="Link to this heading">#</a></h3>
<p>We’ll explore a study examining the prevalence of mental disorders among male prisoners in France. This will allow us to see how multiple regression can be used to analyze complex relationships between variables in a practical setting.</p>
<p>The study we’ll be working with is titled <a class="reference external" href="https://bmcpsychiatry.biomedcentral.com/articles/10.1186/1471-244X-6-33">“Prevalence of mental disorders in French prisons for men” by Falissard et al. (2006)</a>. This research investigated the mental health of 800 male prisoners randomly sampled from French prisons. Here’s a summary of their key findings:</p>
<ul class="simple">
<li><p><strong>Diagnostic challenges:</strong> the authors highlight the difficulties of accurately diagnosing mental disorders within the prison context, where traditional standardized interviews might not be fully adequate.</p></li>
<li><p><strong>Two-clinician approach:</strong> to enhance diagnostic accuracy, they used a unique approach where each prisoner was interviewed by two clinicians, including a senior psychiatrist. One clinician used a structured clinical interview (MINI plus) for DSM-IV diagnoses, while the other conducted an open clinical interview.</p></li>
<li><p><strong>High prevalence rates:</strong> the study revealed high prevalence rates for several mental disorders:</p>
<ul>
<li><p>Schizophrenia: 3.8% (independent diagnoses) to 6.2% (consensual diagnosis)</p></li>
<li><p>Major depressive disorder: 17.9% to 24%</p></li>
<li><p>Generalized anxiety disorder: 12.0% to 17.7%</p></li>
<li><p>Drug dependence: 10.8% to 14.6%</p></li>
</ul>
</li>
<li><p><strong>Importance of clinical judgment:</strong> the authors concluded that combining structured interviews with open clinical assessments provides a more reliable and nuanced understanding of mental health in prisoners.</p></li>
</ul>
<p>The data from this study has been obtained from a MOOC (Massive Open Online Course) on biostatistics using R. This MOOC was offered by Bruno Falissard, one of the authors of the study we’re analyzing. The dataset, available through the MOOC, provides a rich source of information for our analysis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># Load the dataset from the URL of the MOOC</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;https://lms.fun-mooc.fr/c4x/Paris11/15001/asset/smp2.csv&quot;</span><span class="p">,</span>
    <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;;&#39;</span><span class="p">,</span>  <span class="c1"># Specify the delimiter as &#39;;&#39;</span>
<span class="p">)</span>

<span class="c1"># Display the first 5 rows of the DataFrame</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>prof</th>
      <th>duree</th>
      <th>discip</th>
      <th>n.enfant</th>
      <th>n.fratrie</th>
      <th>ecole</th>
      <th>separation</th>
      <th>juge.enfant</th>
      <th>place</th>
      <th>...</th>
      <th>subst.cons</th>
      <th>scz.cons</th>
      <th>char</th>
      <th>rs</th>
      <th>ed</th>
      <th>dr</th>
      <th>suicide.s</th>
      <th>suicide.hr</th>
      <th>suicide.past</th>
      <th>dur.interv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>31.0</td>
      <td>autre</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>4</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>49.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>7.0</td>
      <td>3</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>70.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>50.0</td>
      <td>prof.interm?diaire</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>47.0</td>
      <td>ouvrier</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>6</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>105.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>23.0</td>
      <td>sans emploi</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>6</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 26 columns</p>
</div></div></div>
</div>
<p>To gain a better understanding of the dataset, we’ll use the <code class="docutils literal notranslate"><span class="pre">data.info()</span></code> function. This will provide us with valuable information about the data, including the number of rows and columns, the data type of each variable, and whether there are any missing values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 799 entries, 0 to 798
Data columns (total 26 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   age           797 non-null    float64
 1   prof          793 non-null    object 
 2   duree         576 non-null    float64
 3   discip        793 non-null    float64
 4   n.enfant      773 non-null    float64
 5   n.fratrie     799 non-null    int64  
 6   ecole         794 non-null    float64
 7   separation    788 non-null    float64
 8   juge.enfant   794 non-null    float64
 9   place         792 non-null    float64
 10  abus          792 non-null    float64
 11  grav.cons     795 non-null    float64
 12  dep.cons      799 non-null    int64  
 13  ago.cons      799 non-null    int64  
 14  ptsd.cons     799 non-null    int64  
 15  alc.cons      799 non-null    int64  
 16  subst.cons    799 non-null    int64  
 17  scz.cons      799 non-null    int64  
 18  char          703 non-null    float64
 19  rs            696 non-null    float64
 20  ed            692 non-null    float64
 21  dr            688 non-null    float64
 22  suicide.s     758 non-null    float64
 23  suicide.hr    760 non-null    float64
 24  suicide.past  785 non-null    float64
 25  dur.interv    749 non-null    float64
dtypes: float64(18), int64(7), object(1)
memory usage: 162.4+ KB
</pre></div>
</div>
</div>
</div>
<p>While the variable names in the dataset are in French, a <a class="reference external" href="https://lms.fun-mooc.fr/c4x/Paris11/15001/asset/Pr_sentation_variables__tude_SMP_MOOC_R.pdf">comprehensive PDF document</a> explains each variable in detail. The summary table below will help us understand the data and conduct our analysis effectively.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Variable</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Units</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>age</p></td>
<td><p>Age</p></td>
<td><p>Years</p></td>
</tr>
<tr class="row-odd"><td><p>prof</p></td>
<td><p>Profession (agriculteur, artisan, cadre, profession intermédiaire, employé, ouvrier, autre, sans emploi)</p></td>
<td><p>Categorical</p></td>
</tr>
<tr class="row-even"><td><p>dep.cons</p></td>
<td><p>Presence of depressive disorder (diagnosed by consensus of two clinicians)</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-odd"><td><p>scz.cons</p></td>
<td><p>Presence of schizophrenia (diagnosed by consensus of two clinicians)</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-even"><td><p>grav.cons</p></td>
<td><p>Severity of the inmate’s psychopathology (1: normal, 2: borderline, 3: mild, 4: moderate, 5: manifest, 6: severe, 7: among the most ill)</p></td>
<td><p>Ordinal (1-7)</p></td>
</tr>
<tr class="row-odd"><td><p>n.enfant</p></td>
<td><p>Number of children</p></td>
<td><p>Count</p></td>
</tr>
<tr class="row-even"><td><p>rs</p></td>
<td><p>Novelty seeking (1: low, 2: moderate, 3: high)</p></td>
<td><p>Ordinal (1-3)</p></td>
</tr>
<tr class="row-odd"><td><p>ed</p></td>
<td><p>Harm avoidance (1: low, 2: moderate, 3: high)</p></td>
<td><p>Ordinal (1-3)</p></td>
</tr>
<tr class="row-even"><td><p>dr</p></td>
<td><p>Reward dependence (1: low, 2: moderate, 3: high)</p></td>
<td><p>Ordinal (1-3)</p></td>
</tr>
<tr class="row-odd"><td><p>duree</p></td>
<td><p>Duration of incarceration (1: Less than 1 month, 2: 1 to 6 months, 3: 6 months to 1 year, 4: 1 to 5 years, 5: 5 years or more)</p></td>
<td><p>Ordinal (1-5)</p></td>
</tr>
<tr class="row-even"><td><p>discip</p></td>
<td><p>Disciplinary action since incarceration</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-odd"><td><p>n.fratrie</p></td>
<td><p>Number of siblings</p></td>
<td><p>Count</p></td>
</tr>
<tr class="row-even"><td><p>ecole</p></td>
<td><p>Education level (1: no diploma, 2: middle school, 3: CAP, BEP, 4: high school, 5: university)</p></td>
<td><p>Ordinal (1-5)</p></td>
</tr>
<tr class="row-odd"><td><p>separation</p></td>
<td><p>Separation from parents for at least 6 months during childhood</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-even"><td><p>juge.enfant</p></td>
<td><p>Followed by a children’s judge before age 18</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-odd"><td><p>place</p></td>
<td><p>Placement in a home or foster care before age 18</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-even"><td><p>abus</p></td>
<td><p>History of childhood abuse (physical, psychological, or sexual)</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-odd"><td><p>ago.cons</p></td>
<td><p>Presence of agoraphobia</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-even"><td><p>ptsd.cons</p></td>
<td><p>Presence of post-traumatic stress disorder</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-odd"><td><p>alc.cons</p></td>
<td><p>Presence of alcohol abuse</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-even"><td><p>subst.cons</p></td>
<td><p>Presence of substance abuse</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-odd"><td><p>char</p></td>
<td><p>Intensity of personality disorder (1: absent, 2: mild, 3: moderate, 4: severe)</p></td>
<td><p>Ordinal (1-4)</p></td>
</tr>
<tr class="row-even"><td><p>suicide.s</p></td>
<td><p>Suicide risk score</p></td>
<td><p>Score (1-6)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://suicide.hr">suicide.hr</a></p></td>
<td><p>High suicide risk</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-even"><td><p>suicide.past</p></td>
<td><p>History of suicide attempt</p></td>
<td><p>Binary (0: No, 1: Yes)</p></td>
</tr>
<tr class="row-odd"><td><p>dur.interv</p></td>
<td><p>Duration of the interview</p></td>
<td><p>Minutes</p></td>
</tr>
</tbody>
</table>
</div>
<p>To get a preliminary overview of the data, we can utilize the <code class="docutils literal notranslate"><span class="pre">data.describe()</span></code> function. This function provides descriptive statistics for each of the columns in the dataset, including the mean, standard deviation, minimum, and maximum values. This overview can be useful for understanding the distribution of the variables and identifying any potential outliers or anomalies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>duree</th>
      <th>discip</th>
      <th>n.enfant</th>
      <th>n.fratrie</th>
      <th>ecole</th>
      <th>separation</th>
      <th>juge.enfant</th>
      <th>place</th>
      <th>abus</th>
      <th>...</th>
      <th>subst.cons</th>
      <th>scz.cons</th>
      <th>char</th>
      <th>rs</th>
      <th>ed</th>
      <th>dr</th>
      <th>suicide.s</th>
      <th>suicide.hr</th>
      <th>suicide.past</th>
      <th>dur.interv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>797.000000</td>
      <td>576.000000</td>
      <td>793.000000</td>
      <td>773.000000</td>
      <td>799.000000</td>
      <td>794.000000</td>
      <td>788.000000</td>
      <td>794.000000</td>
      <td>792.000000</td>
      <td>792.000000</td>
      <td>...</td>
      <td>799.000000</td>
      <td>799.000000</td>
      <td>703.000000</td>
      <td>696.000000</td>
      <td>692.000000</td>
      <td>688.000000</td>
      <td>758.000000</td>
      <td>760.000000</td>
      <td>785.000000</td>
      <td>749.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>38.899624</td>
      <td>4.302083</td>
      <td>0.232030</td>
      <td>1.755498</td>
      <td>4.286608</td>
      <td>1.866499</td>
      <td>0.422589</td>
      <td>0.277078</td>
      <td>0.228535</td>
      <td>0.277778</td>
      <td>...</td>
      <td>0.265332</td>
      <td>0.082603</td>
      <td>1.512091</td>
      <td>2.057471</td>
      <td>1.865607</td>
      <td>2.152616</td>
      <td>0.794195</td>
      <td>0.201316</td>
      <td>0.284076</td>
      <td>61.891856</td>
    </tr>
    <tr>
      <th>std</th>
      <td>13.280978</td>
      <td>0.868219</td>
      <td>0.422395</td>
      <td>1.834044</td>
      <td>3.441485</td>
      <td>0.977585</td>
      <td>0.494285</td>
      <td>0.447837</td>
      <td>0.420155</td>
      <td>0.448186</td>
      <td>...</td>
      <td>0.441786</td>
      <td>0.275454</td>
      <td>0.853723</td>
      <td>0.877948</td>
      <td>0.871233</td>
      <td>0.829738</td>
      <td>1.435488</td>
      <td>0.401248</td>
      <td>0.451261</td>
      <td>19.669605</td>
    </tr>
    <tr>
      <th>min</th>
      <td>19.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>28.000000</td>
      <td>4.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>48.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>37.000000</td>
      <td>5.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>60.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>48.000000</td>
      <td>5.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>6.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>75.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>83.000000</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>13.000000</td>
      <td>21.000000</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>120.000000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 25 columns</p>
</div></div></div>
</div>
<p>We can retrieve some of the statistics that were highlighted in the original paper, such as the prevalence of different mental disorders. For example, we can calculate the median age (<code class="docutils literal notranslate"><span class="pre">age</span></code>), the percentage of prisoners with a diagnosis of depression (<code class="docutils literal notranslate"><span class="pre">dep.cons</span></code>) or schizophrenia (<code class="docutils literal notranslate"><span class="pre">scz.cons</span></code>), of the frequency of prisoners that had seen a children’s judge before the age of 18 (<code class="docutils literal notranslate"><span class="pre">juge.enfant</span></code>). This will allow us to compare our findings with those reported in the paper and ensure that our dataset is consistent with the original study.</p>
</section>
<section id="building-the-regression-model">
<h3>Building the regression model<a class="headerlink" href="#building-the-regression-model" title="Link to this heading">#</a></h3>
<p>Let’s construct a multiple regression model to analyze the relationship between the duration of the interview (<code class="docutils literal notranslate"><span class="pre">dur.interv</span></code>) and several independent variables: <code class="docutils literal notranslate"><span class="pre">age</span></code>, <code class="docutils literal notranslate"><span class="pre">dep.cons</span></code>, <code class="docutils literal notranslate"><span class="pre">subst.cons</span></code>, and <code class="docutils literal notranslate"><span class="pre">scz.cons</span></code>.</p>
<p>Our model can be expressed as:</p>
<div class="math notranslate nohighlight">
\[Y_i = \beta_0 + \beta_\mathrm{age} X_{i, \mathrm{age}} + \beta_\mathrm{dep.cons} X_{i, \mathrm{dep.cons}} + \beta_\mathrm{subst.cons} X_{i, \mathrm{subst.cons}} + \beta_\mathrm{scz.cons} X_{i, \mathrm{scz.cons}} + \epsilon_i\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y_i\)</span> represents the duration of the interview for the <span class="math notranslate nohighlight">\(i\)</span>-th participant.</p></li>
<li><p><span class="math notranslate nohighlight">\(X_{i, \mathrm{age}}\)</span> is the age of the <span class="math notranslate nohighlight">\(i\)</span>-th participant.</p></li>
<li><p><span class="math notranslate nohighlight">\(X_{i, \mathrm{dep.cons}}\)</span>, <span class="math notranslate nohighlight">\(X_{i, \mathrm{subst.cons}}\)</span>, and <span class="math notranslate nohighlight">\(X_{i, \mathrm{scz.cons}}\)</span> are dummy variables indicating the presence (1) or absence (0) of depression, substance abuse, and schizophrenia, respectively, for the <span class="math notranslate nohighlight">\(i\)</span>-th participant.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept, representing the predicted average duration of the interview when all independent variables are 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_\mathrm{age}\)</span>, <span class="math notranslate nohighlight">\(\beta_\mathrm{dep.cons}\)</span>, <span class="math notranslate nohighlight">\(\beta_\mathrm{subst.cons}\)</span>, and <span class="math notranslate nohighlight">\(\beta_\mathrm{scz.cons}\)</span> are the regression coefficients, representing the <em>change in the average</em> interview duration associated with a one-unit change in the corresponding independent variable, while holding other variables constant.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon_i\)</span> is the error term for the <span class="math notranslate nohighlight">\(i\)</span>-th participant, representing the difference between the observed and predicted interview duration.</p></li>
</ul>
<p>The coefficients will be interpreted as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_\mathrm{age}\)</span> represents the average change in interview duration for a one-year increase in age, assuming the other variables remain constant.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_\mathrm{dep.cons}\)</span> represents the average difference in interview duration between those with depression (<code class="docutils literal notranslate"><span class="pre">dep.cons</span></code> = 1) and those without depression (<code class="docutils literal notranslate"><span class="pre">dep.cons</span></code> = 0), holding other variables constant.</p></li>
<li><p>Similarly, <span class="math notranslate nohighlight">\(\beta_\mathrm{subst.cons}\)</span> and <span class="math notranslate nohighlight">\(\beta_\mathrm{scz.cons}\)</span> represent the average differences in interview duration associated with substance abuse and schizophrenia, respectively, after adjusting for the effects of other variables in the model.</p></li>
<li><p>The Intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> represents the predicted average interview duration when all independent variables are 0. In this specific case, it might not have a direct practical interpretation since it would imply a participant with an age of 0 and no presence of the mental health conditions included in the model. However, the intercept is crucial for the mathematical definition of the regression line.</p></li>
</ul>
<p>We’ll now use the <code class="docutils literal notranslate"><span class="pre">statsmodels.formula.api</span></code> module to fit our multiple regression model. The process is very similar to how we used <code class="docutils literal notranslate"><span class="pre">OLS</span></code> for simple linear regression, so we won’t go into the specifics again here. Instead, let’s focus on applying the method and interpreting the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="c1"># Create a new DataFrame with only the relevant variables</span>
<span class="n">analysis_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;dep.cons&#39;</span><span class="p">,</span> <span class="s1">&#39;subst.cons&#39;</span><span class="p">,</span> <span class="s1">&#39;scz.cons&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Drop rows with missing values in any of the selected columns</span>
<span class="n">analysis_data</span> <span class="o">=</span> <span class="n">analysis_data</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Define the dependent variable (Y)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">analysis_data</span><span class="p">[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">]</span>

<span class="c1"># Define the independent variables (X)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">analysis_data</span><span class="p">[[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;dep.cons&#39;</span><span class="p">,</span> <span class="s1">&#39;subst.cons&#39;</span><span class="p">,</span> <span class="s1">&#39;scz.cons&#39;</span><span class="p">]]</span>

<span class="c1"># Add a constant term to the independent variables matrix for the intercept</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Create and fit the OLS model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:             dur.interv   R-squared:                       0.058
Model:                            OLS   Adj. R-squared:                  0.053
Method:                 Least Squares   F-statistic:                     11.49
Date:                Tue, 14 Jan 2025   Prob (F-statistic):           4.69e-09
Time:                        09:03:34   Log-Likelihood:                -3260.7
No. Observations:                 747   AIC:                             6531.
Df Residuals:                     742   BIC:                             6554.
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         48.9011      2.622     18.649      0.000      43.753      54.049
age            0.2210      0.057      3.871      0.000       0.109       0.333
dep.cons       7.3893      1.448      5.104      0.000       4.547      10.232
subst.cons     5.2516      1.743      3.013      0.003       1.829       8.674
scz.cons       2.2726      2.523      0.901      0.368      -2.681       7.226
==============================================================================
Omnibus:                       28.567   Durbin-Watson:                   1.072
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               31.557
Skew:                           0.452   Prob(JB):                     1.40e-07
Kurtosis:                       3.445   Cond. No.                         167.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>Patsy formulas allow us to express the model in a more compact and readable way.</p>
<p>In Patsy formulas, variable names need to follow standard Python variable naming conventions. This means they cannot contain special characters like periods (<code class="docutils literal notranslate"><span class="pre">.</span></code>) or hyphens (<code class="docutils literal notranslate"><span class="pre">-</span></code>), because they have a statistical meaning. Since our dataset includes variable names with periods, such as <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code>, <code class="docutils literal notranslate"><span class="pre">dep.cons</span></code>, and <code class="docutils literal notranslate"><span class="pre">subst.cons</span></code>, we’ll use Patsy’s <code class="docutils literal notranslate"><span class="pre">Q()</span></code> function to <em>quote</em> these names, ensuring Patsy interprets them correctly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.formula.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">smf</span>

<span class="c1"># Define the model formula using patsy syntax with the interaction term</span>
<span class="n">formula</span> <span class="o">=</span> <span class="s2">&quot;Q(&#39;dur.interv&#39;) ~ age + Q(&#39;dep.cons&#39;) + Q(&#39;subst.cons&#39;) + Q(&#39;scz.cons&#39;)&quot;</span>

<span class="c1"># Fit the OLS model using the formula and data</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">analysis_data</span><span class="p">)</span>

<span class="c1"># Fit the model</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results with a little different method</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary2</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                 Results: Ordinary least squares
==================================================================
Model:              OLS              Adj. R-squared:     0.053    
Dependent Variable: Q(&#39;dur.interv&#39;)  AIC:                6531.3226
Date:               2025-01-14 09:03 BIC:                6554.4030
No. Observations:   747              Log-Likelihood:     -3260.7  
Df Model:           4                F-statistic:        11.49    
Df Residuals:       742              Prob (F-statistic): 4.69e-09 
R-squared:          0.058            Scale:              364.62   
------------------------------------------------------------------
                    Coef.  Std.Err.    t    P&gt;|t|   [0.025  0.975]
------------------------------------------------------------------
Intercept          48.9011   2.6221 18.6493 0.0000 43.7534 54.0487
age                 0.2210   0.0571  3.8707 0.0001  0.1089  0.3330
Q(&#39;dep.cons&#39;)       7.3893   1.4478  5.1037 0.0000  4.5470 10.2316
Q(&#39;subst.cons&#39;)     5.2516   1.7432  3.0126 0.0027  1.8294  8.6737
Q(&#39;scz.cons&#39;)       2.2726   2.5232  0.9007 0.3681 -2.6809  7.2261
------------------------------------------------------------------
Omnibus:               28.567       Durbin-Watson:          1.072 
Prob(Omnibus):         0.000        Jarque-Bera (JB):       31.557
Skew:                  0.452        Prob(JB):               0.000 
Kurtosis:              3.445        Condition No.:          167   
==================================================================
Notes:
[1] Standard Errors assume that the covariance matrix of the
errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>We can achieve similar results using the <a class="reference external" href="https://pingouin-stats.org/build/html/generated/pingouin.linear_regression.html"><code class="docutils literal notranslate"><span class="pre">pingouin</span></code> library</a>, but it will return less detailed diagnostic values. Additionally, it is limited to handling numerical variables, which might be restrictive for some analyses. Therefore, we’ll primarily use <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> for our multiple regression analyses, as it provides more comprehensive diagnostics and can handle both numerical and categorical variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pingouin</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pg</span>

<span class="c1"># Fit the linear regression model using Pingouin</span>
<span class="n">results_pingouin</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Print the regression results summary</span>
<span class="n">results_pingouin</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>names</th>
      <th>coef</th>
      <th>se</th>
      <th>T</th>
      <th>pval</th>
      <th>r2</th>
      <th>adj_r2</th>
      <th>CI[2.5%]</th>
      <th>CI[97.5%]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>48.901</td>
      <td>2.622</td>
      <td>18.649</td>
      <td>0.000</td>
      <td>0.058</td>
      <td>0.053</td>
      <td>43.753</td>
      <td>54.049</td>
    </tr>
    <tr>
      <th>1</th>
      <td>age</td>
      <td>0.221</td>
      <td>0.057</td>
      <td>3.871</td>
      <td>0.000</td>
      <td>0.058</td>
      <td>0.053</td>
      <td>0.109</td>
      <td>0.333</td>
    </tr>
    <tr>
      <th>2</th>
      <td>dep.cons</td>
      <td>7.389</td>
      <td>1.448</td>
      <td>5.104</td>
      <td>0.000</td>
      <td>0.058</td>
      <td>0.053</td>
      <td>4.547</td>
      <td>10.232</td>
    </tr>
    <tr>
      <th>3</th>
      <td>subst.cons</td>
      <td>5.252</td>
      <td>1.743</td>
      <td>3.013</td>
      <td>0.003</td>
      <td>0.058</td>
      <td>0.053</td>
      <td>1.829</td>
      <td>8.674</td>
    </tr>
    <tr>
      <th>4</th>
      <td>scz.cons</td>
      <td>2.273</td>
      <td>2.523</td>
      <td>0.901</td>
      <td>0.368</td>
      <td>0.058</td>
      <td>0.053</td>
      <td>-2.681</td>
      <td>7.226</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="interpretation-of-the-report">
<h3>Interpretation of the report<a class="headerlink" href="#interpretation-of-the-report" title="Link to this heading">#</a></h3>
<section id="overall-model-fit">
<h4>Overall model fit<a class="headerlink" href="#overall-model-fit" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><em>Adj. R-squared (0.053):</em> this tells us that the model explains approximately 5.3% of the variance in the duration of the interview (<code class="docutils literal notranslate"><span class="pre">dur.interv</span></code>). This is quite low, suggesting that the predictors in the model don’t account for much of the variability in interview length. We will talk about the <em>R-squared</em> and <em>adjusted R-squared</em> in more details later in the chapter.</p></li>
<li><p><em>F-statistic (11.49) P value (4.69e-09):</em> the F-statistic tests the overall significance of the model. The very low P value indicates that the model is statistically significant, meaning <em>at least one of the predictors is significantly related to the outcome</em>, as it tests the hypothesis that all the coefficients are 0.</p></li>
</ul>
</section>
<section id="coefficients">
<h4>Coefficients<a class="headerlink" href="#coefficients" title="Link to this heading">#</a></h4>
<p>This table shows the estimated coefficients or <em>best-fit values</em> for each predictor, along with their standard errors, t-statistics, P values, and 95% confidence intervals.</p>
<ul class="simple">
<li><p><em>Intercept (48.9011):</em> this is the estimated average <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code> when all other predictors are 0. In this context, it doesn’t have a meaningful real-world interpretation, as it’s not possible to have an age of 0 or the absence of all the mental health conditions.</p></li>
<li><p><em>age (0.2210; P = 0.0001):</em> for each one-year increase in age, the interview duration is estimated to increase by 0.2210 minutes, on average, holding other factors constant. The 95% CI ranges from 0.109 to 0.333. This effect is statistically significant.</p></li>
<li><p><em>Q(‘dep.cons’) (7.3893; P = 0.0000):</em> inmates with depression (<code class="docutils literal notranslate"><span class="pre">dep.cons</span></code> = 1) have, on average, interviews that are 7.3893 minutes longer than those without depression, controlling for other factors. This effect is also statistically significant.</p></li>
<li><p><em>Q(‘subst.cons’) (5.2516; P = 0.0027):</em> inmates with a substance abuse disorder have interviews estimated to be 5.2516 minutes longer than those without, holding other factors constant. This is statistically significant.</p></li>
<li><p><em>Q(‘scz.cons’) (2.2726; P = 0.3681):</em> the coefficient for schizophrenia is not statistically significant (P &gt; 0.05). This suggests that, after accounting for age, depression, and substance abuse, schizophrenia is not a significant predictor of interview duration in this model.</p></li>
</ul>
</section>
<section id="other-diagnostics">
<h4>Other diagnostics<a class="headerlink" href="#other-diagnostics" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><em>Omnibus, Prob(Omnibus), Skew, Kurtosis, Jarque-Bera (JB), Prob(JB):</em> these tests assess the <strong>normality</strong> of the residuals. The significant results here suggest that the residuals may not be normally distributed.</p></li>
<li><p><em>Durbin-Watson (1.072):</em> this statistic tests for <strong>autocorrelation</strong> in the residuals. A value this low suggests potential positive autocorrelation, which might be expected in some types of data where observations are not completely independent (though we don’t have enough information about the data collection to say for sure).</p></li>
<li><p><em>Condition No. (167):</em> this is a measure of <strong>multicollinearity</strong>. While not extremely high, it’s something to keep in mind, especially if we were to add more predictors to the model.</p></li>
</ul>
<p>To further assess the normality of the residuals, we can visualize their distribution using a histogram and a Q-Q plot. These plots will provide a visual complement to the statistical tests reported in the regression output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="c1"># Get the residuals from the model</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">resid</span>

<span class="c1"># Create subplots for the histogram and Q-Q plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot the histogram of residuals</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Residuals&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Histogram of residuals&quot;</span><span class="p">)</span>

<span class="c1"># Plot the Q-Q plot of residuals</span>
<span class="n">sm</span><span class="o">.</span><span class="n">qqplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Q-Q plot of residuals&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3599d037902b414bd41a51794f8f297617acd20d7edef7bbdae53dcdf65c171c.png" src="_images/3599d037902b414bd41a51794f8f297617acd20d7edef7bbdae53dcdf65c171c.png" />
</div>
</div>
</section>
</section>
<section id="visualizing-regression-relationships">
<h3>Visualizing regression relationships<a class="headerlink" href="#visualizing-regression-relationships" title="Link to this heading">#</a></h3>
<p>Visualizing the relationships between our variables can help us understand the model and assess its assumptions. One useful tool is the <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.graphics.regressionplots.plot_partregress_grid.html"><em>partial regression plot</em></a>, which displays the relationship between the response variable and a given predictor after removing the effects of all other predictors in the model.</p>
<p>In essence, a partial regression plot isolates the relationship between the response and a specific predictor, allowing us to see the unique contribution of that predictor after accounting for the influence of other variables. This can be helpful for identifying non-linear patterns or unusual observations that might be affecting the model.</p>
<p>The code below creates a grid of partial regression plots for all the independent variables in the regression model, using the <code class="docutils literal notranslate"><span class="pre">results</span></code> object returned by the fitted regression model, which contains the necessary information for generating the plots.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the partial regression plot grid</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.5</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_partregress_grid</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5e2fa26be546ce65318bfb4ee11c1db94c75466c8a90096666951324304c2cf1.png" src="_images/5e2fa26be546ce65318bfb4ee11c1db94c75466c8a90096666951324304c2cf1.png" />
</div>
</div>
<p>A partial regression plot aims to isolate the relationship between a specific independent variable and the dependent variable, while controlling for the effects of other independent variables in the model. The slope of the line in the partial regression plot represents the change in the response variable associated with a one-unit change in the predictor, after controlling for the other variables. This is exactly what the regression coefficient represents in the table.</p>
<p>Furtheremore, the dependent variable (<code class="docutils literal notranslate"><span class="pre">dur.interv</span></code> in this case) is regressed on all the independent variables <em>except</em> the one for which the partial regression plot is being created. Then, the specific independent variable, e.g., <code class="docutils literal notranslate"><span class="pre">age</span></code>, is also regressed on all the other independent variables. The residuals from the first regression (involving the dependent variable) are plotted against the residuals from the second regression (involving the independent variable).</p>
<p>The values on both the x-axis and y-axis of a partial regression plot represent <em>residuals</em> from the two regressions described above. Since residuals can be positive or negative, it’s perfectly normal to see negative values on either axis of a partial regression plot. A negative value simply means that the actual observed value was lower than the value predicted by the regression model involving the other predictors.</p>
<p>Another function <a class="reference external" href="https://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.plot_regress_exog.html"><code class="docutils literal notranslate"><span class="pre">sm.graphics.plot_regress_exog</span></code></a> generates a collection of plots that illustrate the relationship between the dependent variable and a chosen independent variable within a regression model. It helps visualize how well the model fits the data, identify potential problems like non-linearity or uneven variance, and isolate the impact of the chosen independent variable on the dependent variable while accounting for other factors. For example, we can show the relationship between <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code> and <code class="docutils literal notranslate"><span class="pre">age</span></code> after adjusting for the effects of <code class="docutils literal notranslate"><span class="pre">dep.cons</span></code>, <code class="docutils literal notranslate"><span class="pre">subst.cons</span></code>, and <code class="docutils literal notranslate"><span class="pre">scz.cons</span></code> as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the partial regression plot grid</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_regress_exog</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cb03a3acc255ece78728e6f627faa56a08ec431d495fe11719ad12f52c7a5ea2.png" src="_images/cb03a3acc255ece78728e6f627faa56a08ec431d495fe11719ad12f52c7a5ea2.png" />
</div>
</div>
<p>The grid presents a set of different plots:</p>
<ul class="simple">
<li><p><em>Y and Fitted vs. X</em>: this plot displays the relationship between the dependent variable and the independent variable “age”. It shows both the <em>observed</em> values (Y) and the <em>fitted</em> values from the regression model against “age”. This helps to visualize how well the model fits the data.</p></li>
<li><p><em>Residuals vs. X</em>: this plot shows the residuals against the independent variable “age”. It helps to detect any patterns in the residuals that might indicate violations of the model’s assumptions, such as <em>non-linearity</em> or <em>heteroscedasticity</em>.</p></li>
<li><p><strong>Partial regression plot</strong>: this plot displays the relationship between the dependent variable and the independent variable “age” after removing the linear effects of all other independent variables in the model. This helps to isolate the effect of “age” on the dependent variable, controlling for other factors.</p></li>
<li><p><strong>CCPR plot</strong>: this is a component-component plus residual plot, which is similar to the partial regression plot but also includes the residuals. It’s useful for identifying non-linearity and <em>influential</em> points.</p></li>
</ul>
</section>
<section id="r-squared-and-adjusted-r-squared">
<h3>R-squared and adjusted R-squared<a class="headerlink" href="#r-squared-and-adjusted-r-squared" title="Link to this heading">#</a></h3>
<p>The R-squared value in our regression model is 0.058. This means that only 5.8% of the variability in <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code> (interview duration) is explained by the independent variables included in our model (<code class="docutils literal notranslate"><span class="pre">age</span></code>, <code class="docutils literal notranslate"><span class="pre">dep.cons</span></code>, <code class="docutils literal notranslate"><span class="pre">subst.cons</span></code>, and <code class="docutils literal notranslate"><span class="pre">scz.cons</span></code>).</p>
<p>In simple linear regression with one predictor, we can easily visualize the best-fit line superimposed on a scatter plot of the data. However, with multiple regression models that have more than two independent variables, direct visualization of the best-fit line is not possible.</p>
<p>So how can we visualize the model fit in multiple regression? One approach is to plot the <em>predicted</em> values against the <em>actual</em> values of the <em>dependent variable</em>. Let’s first calculate the predicted <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code> values for each participant using our fitted model, and then create a scatter plot of the predicted values against the actual <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code> values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate predicted values</span>
<span class="n">analysis_data</span><span class="p">[</span><span class="s1">&#39;pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Plot predicted vs. actual Y values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;pred&#39;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">analysis_data</span><span class="p">,</span>
    <span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span>
    <span class="n">x_jitter</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">y_jitter</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;brown&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Add a 45-degree line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="p">[</span><span class="n">analysis_data</span><span class="p">[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span><span class="n">analysis_data</span><span class="p">[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()],</span>
    <span class="p">[</span><span class="n">analysis_data</span><span class="p">[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">analysis_data</span><span class="p">[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Perfect prediction&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actual dur.interv&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Predicted dur.interv&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">120</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">120</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. predicted values of the dependent variable&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/345d158614bce2e4c8461bc879f86ef71b51780ea5def9c7e5d9e69e59b83eb5.png" src="_images/345d158614bce2e4c8461bc879f86ef71b51780ea5def9c7e5d9e69e59b83eb5.png" />
</div>
</div>
<p>This plot visually represents how well our model predicts the actual <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code> values. If the prediction were perfect, all the points would align perfectly on a <em>45-degree line</em>, with the predicted values exactly matching the actual values. In our case, the points are scattered around the 45-degree line, indicating that our model’s predictions are not perfect. The R-squared value of 0.058, as seen in the regression output, quantifies this imperfection.</p>
<p>Even if the independent variables are completely unable to predict the dependent variable, the R-squared value will typically be greater than zero. This limits the usefulness of R-squared as a sole measure of goodness-of-fit, especially with <em>small sample sizes</em> where the model might overfit the data.</p>
<p><strong>Adjusted R-squared</strong> (<span class="math notranslate nohighlight">\(R_a^2\)</span>) provides a more realistic estimate of how well the model is expected to fit new, unseen data. It accounts for the number of independent variables in the model, penalizing the addition of unnecessary predictors that don’t contribute meaningfully to explaining the variance in the dependent variable. Adjusted R-squared is always smaller than the unadjusted R-squared.</p>
<p>Remind the formula for adjusted R-squared <span class="math notranslate nohighlight">\(R_a^2 = 1 - (1 - R^2) \frac{n - 1}{n - p - 1}\)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of observations (participants)</p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span> is the number of independent variables</p></li>
</ul>
<p>Rules of thumb suggest that the number of participants (n) should be somewhere between <em>10-40 times</em> the number of variables (p). With approximately 800 participants in our study, we could potentially analyze up to 20 independent variables while maintaining a reasonable ratio for reliable model estimation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the number of observations from the fitted model object</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">nobs</span>

<span class="c1"># Get the degrees of freedom of the residuals</span>
<span class="n">df_residuals</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">df_resid</span>

<span class="c1"># Calculate adjusted R-squared manually</span>
<span class="n">adjusted_r_squared</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">results</span><span class="o">.</span><span class="n">rsquared</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">df_residuals</span>

<span class="c1"># Print adjusted R-squared from the manual calculation and from Statsmodels</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted R-squared (manual):</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">adjusted_r_squared</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted R-squared (Statsmodels): </span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">rsquared_adj</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adjusted R-squared (manual):	 0.0533
Adjusted R-squared (Statsmodels): 0.0533
</pre></div>
</div>
</div>
</div>
<p>In this study, we have far more observation (747, after the removal of NaN values) than independent variables (4), so the adjusted R-squared is only slightly smaller than the unadjusted R-squared.</p>
</section>
</section>
<section id="advanced-techniques">
<h2>Advanced techniques<a class="headerlink" href="#advanced-techniques" title="Link to this heading">#</a></h2>
<section id="improvement-of-the-fit">
<h3>Improvement of the fit<a class="headerlink" href="#improvement-of-the-fit" title="Link to this heading">#</a></h3>
<p>In any research study, we often collect data on numerous variables. However, including all of them in a multiple regression model might not always improve the model’s fit or interpretability. Sometimes, adding more variables can even lead to <em>overfitting</em>, where the model performs well on the current data but fails to generalize to new data. Let’s now add a few variables and see if it improves the fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># List of variables to include in the second model</span>
<span class="n">variables</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;age&#39;</span><span class="p">,</span>
    <span class="s1">&#39;n.enfant&#39;</span><span class="p">,</span>
    <span class="s1">&#39;grav.cons&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dep.cons&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ago.cons&#39;</span><span class="p">,</span>
    <span class="s1">&#39;alc.cons&#39;</span><span class="p">,</span>
    <span class="s1">&#39;subst.cons&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scz.cons&#39;</span>
<span class="p">]</span>

<span class="c1"># Create a new DataFrame with only the relevant variables</span>
<span class="n">analysis_data_overfit</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">variables</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Drop rows with missing values in any of the selected columns</span>
<span class="n">analysis_data_overfit</span> <span class="o">=</span> <span class="n">analysis_data_overfit</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Define the dependent variable (Y)</span>
<span class="n">y_overfit</span> <span class="o">=</span> <span class="n">analysis_data_overfit</span><span class="p">[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">]</span>

<span class="c1"># Define the independent variables (X)</span>
<span class="n">X_overfit</span> <span class="o">=</span> <span class="n">analysis_data_overfit</span><span class="p">[</span><span class="n">variables</span><span class="p">]</span>

<span class="c1"># Add a constant term to the independent variables matrix for the intercept</span>
<span class="n">X_overfit</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_overfit</span><span class="p">)</span>

<span class="c1"># Create and fit the OLS model</span>
<span class="n">model_overfit</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_overfit</span><span class="p">,</span> <span class="n">X_overfit</span><span class="p">)</span>
<span class="n">results_overfit</span> <span class="o">=</span> <span class="n">model_overfit</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_overfit</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:             dur.interv   R-squared:                       0.081
Model:                            OLS   Adj. R-squared:                  0.071
Method:                 Least Squares   F-statistic:                     7.891
Date:                Tue, 14 Jan 2025   Prob (F-statistic):           3.33e-10
Time:                        09:03:36   Log-Likelihood:                -3146.2
No. Observations:                 722   AIC:                             6310.
Df Residuals:                     713   BIC:                             6352.
Df Model:                           8                                         
Covariance Type:            nonrobust                                         
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>The fact that the adjusted R-squared increased from 0.058 in the simpler model to 0.071 in the more complex model tells us a couple of things:</p>
<ol class="arabic simple">
<li><p><em>Improved model fit:</em> the more complex model, with its additional variables, explains a greater proportion of the variance in <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code>. This suggests that the added variables (<code class="docutils literal notranslate"><span class="pre">n.enfant</span></code>, <code class="docutils literal notranslate"><span class="pre">grav.cons</span></code>, <code class="docutils literal notranslate"><span class="pre">ago.cons</span></code>, and <code class="docutils literal notranslate"><span class="pre">alc.cons</span></code>) do contribute meaningfully to predicting interview duration.</p></li>
<li><p><em>Not just overfitting:</em> the fact that the <em>adjusted</em> R-squared increased is particularly important. It indicates that the improvement in model fit isn’t simply due to adding more variables and potentially overfitting the data. The adjusted R-squared penalizes the inclusion of unnecessary predictors, so its increase suggests that the added variables genuinely improve the model’s ability to generalize to new data.</p></li>
</ol>
<p>However, it’s important to consider that, while an increase from 0.058 to 0.071 is notable, it’s still a relatively low R-squared value overall. The model still explains only a small portion of the variability in <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code>. Consider whether the increase in explained variance has practical or clinical significance in the context of the study. Does this improvement in predictive power translate to meaningful insights or actionable information It might be worthwhile to further analyze the more complex model, examining the coefficients, P values, and diagnostic plots to understand the specific contributions of the added variables and ensure the model meets the assumptions of multiple regression.</p>
</section>
<section id="variable-selection">
<h3>Variable selection<a class="headerlink" href="#variable-selection" title="Link to this heading">#</a></h3>
<p>Deciding which variables to include in a model is a complex issue known as <strong>variable selection</strong>. While a full treatment of variable selection techniques is beyond the scope of this book (often falling more into the realm of <em>machine learning</em>), we’ll briefly touch upon the concept here.</p>
<p>One common approach to variable selection is <em>backward-stepwise selection</em> (or step-down selection). In this method, we start with a model that includes all potential predictors. We then iteratively remove the variable with the highest P value (i.e., the least statistically significant predictor) and refit the model until all remaining variables have P values below a certain threshold (often 0.05).</p>
<p>It’s important to note that while stepwise selection methods can be useful, they have limitations and should be used with caution. They don’t always guarantee the best possible model, and the resulting P values might not be entirely accurate. More advanced variable selection techniques, such as <em>regularization methods</em> (LASSO, Ridge), offer more robust approaches and are often preferred in modern statistical modeling.</p>
</section>
<section id="collinearity-and-multicollinearity">
<h3>Collinearity and multicollinearity<a class="headerlink" href="#collinearity-and-multicollinearity" title="Link to this heading">#</a></h3>
<p>In multiple linear regression, the various input variables used can be considered ‘dimensions’ of the problem or model. In theory, we ideally expect dimensions to be independent and uncorrelated. Practically speaking, however, it’s very challenging in large data sets to ensure that every input variable is completely uncorrelated from another.</p>
<p>While some correlation between input variables can be expected and tolerated in linear regression models, high levels of correlation can result in significant inflation of coefficients and inaccurate estimates of P values of coefficients.</p>
<p><em>Collinearity</em> means that two input variables are highly correlated. The definition of ‘high correlation’ is a matter of judgment, but as a rule of thumb correlations greater than 0.5 might be considered high and greater than 0.7 might be considered extreme. Creating a simple correlation matrix or a pairplot can immediately surface high or extreme collinearity.</p>
<p><em>Multicollinearity</em> means that there is a linear relationship between more than two of the input variables. This may not always present itself in the form of high correlations between pairs of input variables, but may be seen by identifying ‘clusters’ of moderately correlated variables, or by calculating a variance inflation factor (VIF) for each input variable where VIFs greater than 5 indicate high multicollinearity. Easy-to-use tests also exist in statistical software for identifying multicollinearity (for example the <code class="docutils literal notranslate"><span class="pre">mctest</span></code> package in R; see also <a class="reference external" href="https://www.statsmodels.org/dev/examples/notebooks/generated/regression_diagnostics.html#Multicollinearity">Regression diagnostics in statsmodels</a>).</p>
<p>Note that collinearity and multicollinearity only affect the coefficients of the variables impacted, and do not affect other variables or the overall statistics and fit of a model. Therefore, if a model is being developed primarily to make predictions and there is little interest in using the model to explain a phenomenon, there may not be any need to address this issue at all. However, in <em>inferential modeling</em> the accuracy of the coefficients is very important, and so testing of multicollinearity is essential. In general, the best way to deal with collinear variables is to remove one of them from the model (usually the one that has the least significance in explaining the outcome).</p>
</section>
<section id="three-dimensional-visualization">
<h3>Three-dimensional visualization<a class="headerlink" href="#three-dimensional-visualization" title="Link to this heading">#</a></h3>
<p>With one or two independent variables, we can visualize the relationship between the dependent and independent variables. For example, let’s study the relationship between <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code> and two specific predictors: <code class="docutils literal notranslate"><span class="pre">age</span></code> and <code class="docutils literal notranslate"><span class="pre">subst.cons</span></code>. This will allow us to demonstrate how to create a 3D plot of a <strong>hyperplane</strong>, which is a way to visualize a multiple regression model with two independent variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>

<span class="c1"># Create a new DataFrame with only the relevant variables</span>
<span class="n">hyperplane_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;subst.cons&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Drop rows with missing values in any of the selected columns</span>
<span class="n">hyperplane_data</span> <span class="o">=</span> <span class="n">hyperplane_data</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Define the model formula using patsy syntax</span>
<span class="n">formula</span> <span class="o">=</span> <span class="s2">&quot;Q(&#39;dur.interv&#39;) ~ age + Q(&#39;subst.cons&#39;)&quot;</span>

<span class="c1"># Fit the OLS model using the formula and data</span>
<span class="n">model_hyperplane</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">hyperplane_data</span><span class="p">)</span>
<span class="n">results_hyperplane</span> <span class="o">=</span> <span class="n">model_hyperplane</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_hyperplane</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Create the 3D plot</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># Scatter plot of the data points</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">hyperplane_data</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">],</span>
    <span class="n">hyperplane_data</span><span class="p">[</span><span class="s1">&#39;subst.cons&#39;</span><span class="p">],</span>
    <span class="n">hyperplane_data</span><span class="p">[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>

<span class="c1"># Create the hyperplane</span>
<span class="n">x_surf</span><span class="p">,</span> <span class="n">y_surf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">hyperplane_data</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">hyperplane_data</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">hyperplane_data</span><span class="p">[</span><span class="s1">&#39;subst.cons&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">hyperplane_data</span><span class="p">[</span><span class="s1">&#39;subst.cons&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">exog</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="n">x_surf</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="s1">&#39;subst.cons&#39;</span><span class="p">:</span> <span class="n">y_surf</span><span class="o">.</span><span class="n">ravel</span><span class="p">()})</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">results_hyperplane</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">exog</span><span class="o">=</span><span class="n">exog</span><span class="p">)</span>
<span class="n">z_surf</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_surf</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x_surf</span><span class="p">,</span> <span class="n">y_surf</span><span class="p">,</span> <span class="n">z_surf</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># type: ignore</span>

<span class="c1"># Set axis labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Age&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Substance abuse (subst.cons)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;Interview duration (dur.interv)&quot;</span><span class="p">)</span> <span class="c1"># type: ignore</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;3D hyperplane of interview duration vs. age and substance abuse&quot;</span><span class="p">)</span>

<span class="c1"># Set the view angle</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">325</span><span class="p">);</span> <span class="c1"># type: ignore</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>===================================================================================
                      coef    std err          t      P&gt;|t|      [0.025      0.975]
-----------------------------------------------------------------------------------
Intercept          52.3586      2.581     20.288      0.000      47.292      57.425
age                 0.2054      0.058      3.542      0.000       0.092       0.319
Q(&#39;subst.cons&#39;)     6.1505      1.764      3.487      0.001       2.688       9.613
===================================================================================
</pre></div>
</div>
<img alt="_images/61561f9bc0e65d74da7e6f45c22dd75f79e160c299ab3216dd857932764b64ae.png" src="_images/61561f9bc0e65d74da7e6f45c22dd75f79e160c299ab3216dd857932764b64ae.png" />
</div>
</div>
</section>
<section id="interactions">
<h3>Interactions<a class="headerlink" href="#interactions" title="Link to this heading">#</a></h3>
<p>In biological systems, it’s common for multiple variables to interact and influence an outcome together. For instance, the impact of temperature on plant growth might be different depending on the availability of water. This interplay between variables can be modeled using <strong>interaction</strong> terms in multiple regression.</p>
<p>Interaction terms are essentially new variables created by <em>multiplying</em> two or more original independent variables. They allow us to capture how the effect of one variable might change depending on the level of another variable.</p>
<p>By including interaction terms, we can build more nuanced and realistic models that capture the complex interplay between variables in biological systems. Imagine we have a model predicting a student’s final exam score (<span class="math notranslate nohighlight">\(\mathrm{Final}\)</span>) based on their Year 2 (<span class="math notranslate nohighlight">\(\mathrm{Yr2}\)</span>) and Year 3 (<span class="math notranslate nohighlight">\(\mathrm{Yr3}\)</span>) exam scores, including an interaction term: <span class="math notranslate nohighlight">\(\mathrm{Final} = \beta_0 + \beta_1\mathrm{Yr3} + \beta_2\mathrm{Yr2} + \beta_3\mathrm{Yr3}\mathrm{Yr2}\)</span>. Let’s say the significant interaction term indicates that the effect of Year 3 score on the final exam score is different depending on the Year 2 score. We can rewrite the model equation to highlight this:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{Final} &amp;= \beta_0 + \beta_1\mathrm{Yr3} + \beta_2\mathrm{Yr2} + \beta_3\mathrm{Yr3}\mathrm{Yr2} \\
&amp;= \beta_0 + (\beta_1 + \beta_3\mathrm{Yr2})\mathrm{Yr3} + \beta_2\mathrm{Yr2} \\
&amp;= \beta_0 + \gamma\mathrm{Yr3} + \beta_2\mathrm{Yr2}
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma = \beta_1 + \beta_3 \mathrm{Yr2}\)</span>.</p>
<p>This shows that the coefficient for <span class="math notranslate nohighlight">\(\mathrm{Yr3}\)</span> is not constant but changes with the value of <span class="math notranslate nohighlight">\(\mathrm{Yr2}\)</span>. In other words, the effect of an extra point in the Year 3 exam on the final exam score will be different depending on how the student performed in Year 2.</p>
<p>Let’s consider our model for predicting interview duration. We might hypothesize that the effect of depression (<code class="docutils literal notranslate"><span class="pre">dep.cons</span></code>) on interview duration could be different depending on whether the individual also has a substance abuse disorder (<code class="docutils literal notranslate"><span class="pre">subst.cons</span></code>). To model this interaction, we can add a new term to our equation:</p>
<div class="math notranslate nohighlight">
\[Y_i = \beta_0 + \beta_1 X_{i, \mathrm{age}} + \beta_2 X_{i, \mathrm{dep.cons}} + \beta_3 X_{i, \mathrm{subst.cons}} + \beta_4 X_{i, \mathrm{scz.cons}} + \beta_{2,3} \pmb{X_{i, \mathrm{dep.cons}} X_{i, \mathrm{subst.cons}}} + \epsilon_i\]</div>
<p>In this equation, the term <span class="math notranslate nohighlight">\(\beta_{2,3} \pmb{X_{i, \mathrm{dep.cons}} X_{i, \mathrm{subst.cons}}}\)</span> represents the interaction between <code class="docutils literal notranslate"><span class="pre">dep.cons</span></code> and <code class="docutils literal notranslate"><span class="pre">subst.cons</span></code>. The coefficient <span class="math notranslate nohighlight">\(\beta_{2,3}\)</span> quantifies how the effect of depression on interview duration changes depending on the presence or absence of substance abuse.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># List of variables to include in the model</span>
<span class="n">variables</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;dep.cons&#39;</span><span class="p">,</span> <span class="s1">&#39;subst.cons&#39;</span><span class="p">,</span> <span class="s1">&#39;scz.cons&#39;</span><span class="p">]</span>

<span class="c1"># Create a new DataFrame with only the relevant variables</span>
<span class="n">analysis_data_interaction</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">variables</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Drop rows with missing values in any of the selected columns</span>
<span class="n">analysis_data_interaction</span> <span class="o">=</span> <span class="n">analysis_data_interaction</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Create the interaction term, it&#39;s a simple mathematical multiplication</span>
<span class="n">analysis_data_interaction</span><span class="p">[</span><span class="s1">&#39;interaction&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">analysis_data_interaction</span><span class="p">[</span><span class="s1">&#39;dep.cons&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">analysis_data_interaction</span><span class="p">[</span><span class="s1">&#39;subst.cons&#39;</span><span class="p">]</span>

<span class="c1"># Define the dependent variable (Y)</span>
<span class="n">y_interaction</span> <span class="o">=</span> <span class="n">analysis_data_interaction</span><span class="p">[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">]</span>

<span class="c1"># Define the independent variables (X), including the interaction term</span>
<span class="n">X_interaction</span> <span class="o">=</span> <span class="n">analysis_data_interaction</span><span class="p">[</span><span class="n">variables</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;interaction&#39;</span><span class="p">]]</span>

<span class="c1"># Add a constant term to the independent variables matrix for the intercept</span>
<span class="n">X_interaction</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_interaction</span><span class="p">)</span>

<span class="c1"># Create and fit the OLS model</span>
<span class="n">model_interaction</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_interaction</span><span class="p">,</span> <span class="n">X_interaction</span><span class="p">)</span>
<span class="n">results_interaction</span> <span class="o">=</span> <span class="n">model_interaction</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_interaction</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>===============================================================================
                  coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------
const          49.5169      2.658     18.630      0.000      44.299      54.735
age             0.2173      0.057      3.805      0.000       0.105       0.329
dep.cons        6.1578      1.698      3.627      0.000       2.825       9.491
subst.cons      3.1724      2.298      1.380      0.168      -1.340       7.685
scz.cons        1.9723      2.531      0.779      0.436      -2.996       6.941
interaction     4.4969      3.243      1.387      0.166      -1.870      10.863
===============================================================================
</pre></div>
</div>
</div>
</div>
<p>If the confidence interval (CI) of the interaction term includes zero, we can conclude that there is no statistically significant interaction between <code class="docutils literal notranslate"><span class="pre">dep.cons</span></code> (depression) and <code class="docutils literal notranslate"><span class="pre">subst.cons</span></code> (substance abuse). In other words, the effect of depression on the outcome variable (e.g., interview duration) does not significantly depend on whether the individual also has a substance abuse disorder.</p>
<p>Note that Patsy provides two primary ways to specify interaction terms in formulas:</p>
<ol class="arabic simple">
<li><p>The asterisk <code class="docutils literal notranslate"><span class="pre">*</span></code> indicates that we want to include both the main effects of the variables and their interaction. For example, <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">*</span> <span class="pre">B</span></code> would include <code class="docutils literal notranslate"><span class="pre">A</span></code>, <code class="docutils literal notranslate"><span class="pre">B</span></code>, and the interacThe colon <code class="docutils literal notranslate"><span class="pre">:</span></code> only includes the interaction term itself. For example, <code class="docutils literal notranslate"><span class="pre">A:B</span></code> would only include the interaction effect, not the main effects of <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code>.tion <code class="docutils literal notranslate"><span class="pre">A:B</span></code> in the model.</p></li>
<li></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with interaction using &#39;*&#39;</span>
<span class="n">formula_star</span> <span class="o">=</span> <span class="s2">&quot;Q(&#39;dur.interv&#39;) ~ age + Q(&#39;scz.cons&#39;) + Q(&#39;dep.cons&#39;) * Q(&#39;subst.cons&#39;)&quot;</span>

<span class="c1"># Fit the model</span>
<span class="n">model_star</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula_star</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">analysis_data_interaction</span><span class="p">)</span>
<span class="n">results_star</span> <span class="o">=</span> <span class="n">model_star</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_star</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=================================================================================================
                                    coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------------
Intercept                        49.5169      2.658     18.630      0.000      44.299      54.735
age                               0.2173      0.057      3.805      0.000       0.105       0.329
Q(&#39;scz.cons&#39;)                     1.9723      2.531      0.779      0.436      -2.996       6.941
Q(&#39;dep.cons&#39;)                     6.1578      1.698      3.627      0.000       2.825       9.491
Q(&#39;subst.cons&#39;)                   3.1724      2.298      1.380      0.168      -1.340       7.685
Q(&#39;dep.cons&#39;):Q(&#39;subst.cons&#39;)     4.4969      3.243      1.387      0.166      -1.870      10.863
=================================================================================================
</pre></div>
</div>
</div>
</div>
<p>The output table from <code class="docutils literal notranslate"><span class="pre">results_star.summary2()</span></code> (using the <code class="docutils literal notranslate"><span class="pre">*</span></code> notation) will include coefficients for <code class="docutils literal notranslate"><span class="pre">dep.cons</span></code>, <code class="docutils literal notranslate"><span class="pre">subst.cons</span></code>, and their interaction <code class="docutils literal notranslate"><span class="pre">dep.cons:subst.cons</span></code>. It gives the same results as <code class="docutils literal notranslate"><span class="pre">results_interaction</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with interaction using &#39;:&#39;</span>
<span class="n">formula_colon</span> <span class="o">=</span> <span class="s2">&quot;Q(&#39;dur.interv&#39;) ~ age + Q(&#39;scz.cons&#39;) + Q(&#39;dep.cons&#39;):Q(&#39;subst.cons&#39;)&quot;</span>

<span class="c1"># Fit the model</span>
<span class="n">model_colon</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula_colon</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">analysis_data_interaction</span><span class="p">)</span>
<span class="n">results_colon</span> <span class="o">=</span> <span class="n">model_colon</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_colon</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=================================================================================================
                                    coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------------
Intercept                        52.6832      2.331     22.602      0.000      48.107      57.259
age                               0.1966      0.054      3.608      0.000       0.090       0.304
Q(&#39;scz.cons&#39;)                     2.0508      2.549      0.805      0.421      -2.953       7.054
Q(&#39;dep.cons&#39;):Q(&#39;subst.cons&#39;)    11.2797      2.192      5.147      0.000       6.977      15.582
=================================================================================================
</pre></div>
</div>
</div>
</div>
<p>The output table from <code class="docutils literal notranslate"><span class="pre">results_colon.summary2()</span></code> (using the <code class="docutils literal notranslate"><span class="pre">:</span></code> notation) will only include a coefficient for the interaction term <code class="docutils literal notranslate"><span class="pre">dep.cons:subst.cons</span></code>.</p>
<p>The choice between <code class="docutils literal notranslate"><span class="pre">*</span></code> and <code class="docutils literal notranslate"><span class="pre">:</span></code> depends on the specific research question and whether we want to include the <strong>main effects</strong> along with the interaction. In most cases, including the main effects (<code class="docutils literal notranslate"><span class="pre">*</span></code> notation) is recommended, as it provides a more complete picture of the relationships between the variables.</p>
</section>
<section id="quadratic-and-higher-order-polynomial-terms">
<h3>Quadratic and higher-order polynomial terms<a class="headerlink" href="#quadratic-and-higher-order-polynomial-terms" title="Link to this heading">#</a></h3>
<p>In many situations, the true underlying relationship between the outcome and the input variables might be non-linear. For example, the relationship might follow a U-shape (quadratic) or an S-shape (cubic). To capture these non-linear patterns, we can include <strong>polynomial</strong> terms in our multiple regression model.</p>
<p>A polynomial term is simply an independent variable raised to a power greater than 1. For instance, a quadratic term would be <span class="math notranslate nohighlight">\(x^2\)</span>, a cubic term would be <span class="math notranslate nohighlight">\(x^3\)</span>, and so on. For example, if we suspect a quadratic relationship between an input variable <span class="math notranslate nohighlight">\(x\)</span> and the outcome <span class="math notranslate nohighlight">\(y\)</span>, our model would take the form:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1x + \beta_2x^2\]</div>
<p>This equation represents a parabola, allowing us to model U-shaped relationships.</p>
<p>We can easily incorporate polynomial terms into our <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> formulas using the <code class="docutils literal notranslate"><span class="pre">I()</span></code> function (for identity) and the power operator (<code class="docutils literal notranslate"><span class="pre">**</span></code>). For example, let’s say we want to test if a quadratic model for the <code class="docutils literal notranslate"><span class="pre">age</span></code> variable improves our model fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with the interaction term and quadratic term for &#39;age&#39;</span>
<span class="n">formula_quadratic</span> <span class="o">=</span> <span class="s2">&quot;Q(&#39;dur.interv&#39;) ~ age + Q(&#39;scz.cons&#39;) + Q(&#39;dep.cons&#39;) * Q(&#39;subst.cons&#39;) + I(age ** 2)&quot;</span>

<span class="c1"># Fit the OLS model using the formula and data</span>
<span class="n">model_quadratic</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula_quadratic</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">analysis_data_interaction</span><span class="p">)</span>

<span class="c1"># Fit the model</span>
<span class="n">results_quadratic</span> <span class="o">=</span> <span class="n">model_quadratic</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_quadratic</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:        Q(&#39;dur.interv&#39;)   R-squared:                       0.061
Model:                            OLS   Adj. R-squared:                  0.053
Method:                 Least Squares   F-statistic:                     8.010
Date:                Tue, 14 Jan 2025   Prob (F-statistic):           2.16e-08
Time:                        09:03:37   Log-Likelihood:                -3259.6
No. Observations:                 747   AIC:                             6533.
Df Residuals:                     740   BIC:                             6566.
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
=================================================================================================
                                    coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------------
Intercept                        51.8248      6.187      8.377      0.000      39.679      63.971
age                               0.1005      0.288      0.349      0.727      -0.465       0.667
Q(&#39;scz.cons&#39;)                     1.9683      2.532      0.777      0.437      -3.003       6.940
Q(&#39;dep.cons&#39;)                     6.1648      1.699      3.629      0.000       2.830       9.500
Q(&#39;subst.cons&#39;)                   3.0786      2.311      1.332      0.183      -1.458       7.615
Q(&#39;dep.cons&#39;):Q(&#39;subst.cons&#39;)     4.5349      3.246      1.397      0.163      -1.838      10.908
I(age ** 2)                       0.0013      0.003      0.413      0.680      -0.005       0.008
==============================================================================
Omnibus:                       29.885   Durbin-Watson:                   1.081
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               33.235
Skew:                           0.462   Prob(JB):                     6.07e-08
Kurtosis:                       3.462   Cond. No.                     1.86e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.86e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>When we added a quadratic term for <code class="docutils literal notranslate"><span class="pre">age</span></code> to the model, the adjusted R-squared decreased slightly, suggesting that the quadratic term doesn’t meaningfully improve the model’s fit. This observation highlights that increasing model complexity doesn’t always lead to better results. We should carefully evaluate whether any added complexity is justified by the data and whether it truly enhances the model’s explanatory power and interpretability.</p>
</section>
<section id="special-variables">
<h3>Special variables<a class="headerlink" href="#special-variables" title="Link to this heading">#</a></h3>
<p>In some cases, we might encounter variables that require special treatment in our regression model. These could include variables with <em>logarithmic</em> relationships, variables that represent cyclical patterns, or variables with other unique characteristics.</p>
<p>Logarithmic transformations are often used in biology to deal with data that spans several orders of magnitude or exhibits exponential growth or decay. For example, if we have a variable that represents the population size of a species, it might be more appropriate to use the logarithm of the population size in our model.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>, we can easily incorporate logarithmic transformations using the <code class="docutils literal notranslate"><span class="pre">np.log()</span></code> function from the NumPy library. For example, if we want to include the logarithm of <code class="docutils literal notranslate"><span class="pre">age</span></code> in our model, we can modify our Patsy formula as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with the logarithm of &#39;age&#39;</span>
<span class="n">formula_log</span> <span class="o">=</span> <span class="s2">&quot;Q(&#39;dur.interv&#39;) ~ np.log(age) + Q(&#39;scz.cons&#39;) + Q(&#39;dep.cons&#39;) * Q(&#39;subst.cons&#39;)&quot;</span>

<span class="c1"># Fit the OLS model using the formula and data</span>
<span class="n">model_log</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula_log</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">analysis_data_interaction</span><span class="p">)</span>

<span class="c1"># Fit the model</span>
<span class="n">results_log</span> <span class="o">=</span> <span class="n">model_log</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_log</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=================================================================================================
                                    coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------------
Intercept                        28.8024      8.409      3.425      0.001      12.294      45.310
np.log(age)                       8.1036      2.251      3.600      0.000       3.684      12.523
Q(&#39;scz.cons&#39;)                     1.9757      2.534      0.780      0.436      -2.998       6.949
Q(&#39;dep.cons&#39;)                     6.1166      1.699      3.600      0.000       2.781       9.452
Q(&#39;subst.cons&#39;)                   3.1179      2.309      1.350      0.177      -1.416       7.651
Q(&#39;dep.cons&#39;):Q(&#39;subst.cons&#39;)     4.4546      3.247      1.372      0.171      -1.920      10.829
=================================================================================================
</pre></div>
</div>
</div>
</div>
<p>Other types of special variables might include:</p>
<ul class="simple">
<li><p><strong>Cyclical variables:</strong> these variables represent patterns that repeat over a specific period, such as time of day or day of the year. We might need to use trigonometric functions (sine, cosine) to model these variables effectively.</p></li>
<li><p><strong>Interaction terms with special variables:</strong> we can also create interaction terms that involve special variables, such as the interaction between the logarithm of a variable and another variable.</p></li>
</ul>
<p>By understanding how to handle special variables, we can build more accurate and nuanced regression models that capture the unique characteristics of our data.</p>
</section>
<section id="removal-of-intercept">
<h3>Removal of intercept<a class="headerlink" href="#removal-of-intercept" title="Link to this heading">#</a></h3>
<p>In linear regression models, the intercept term (<span class="math notranslate nohighlight">\(\beta_0\)</span>) represents the estimated value of the dependent variable when all independent variables are set to zero. However, there are cases where it might make sense to remove the intercept from the model.</p>
<p>Removing the intercept forces the regression line to pass through the origin (0, 0). This implies that when all independent variables are zero, the dependent variable is also expected to be zero. The interpretation of the remaining coefficients changes. Without the intercept, each coefficient represents the estimated change in the dependent variable for a one-unit change in the corresponding independent variable when all other variables are zero.</p>
<p>In some cases, removing the intercept can improve the model’s fit to the data, especially when the relationship between the variables suggests that the dependent variable should be zero when the independent variables are zero. But the decision to remove the intercept should be guided by domain knowledge and the specific research question. If there’s a strong theoretical reason to believe that the relationship between the variables should be constrained to pass through the origin, then removing the intercept might be appropriate. Be carefull, removing the intercept can sometimes lead to misleading results or poor model performance, especially if the true relationship doesn’t actually pass through the origin. It’s essential to carefully evaluate the model’s fit and the potential consequences of removing the intercept before making a decision.</p>
<p>To remove the intercept in <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>, we can add <code class="docutils literal notranslate"><span class="pre">-</span> <span class="pre">1</span></code> to the Patsy formula:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with the intercept removed</span>
<span class="n">formula_no_intercept</span> <span class="o">=</span> <span class="s2">&quot;Q(&#39;dur.interv&#39;) ~ age + Q(&#39;scz.cons&#39;) + Q(&#39;dep.cons&#39;) * Q(&#39;subst.cons&#39;) - 1&quot;</span>

<span class="c1"># Fit the OLS model using the formula and data</span>
<span class="n">model_no_intercept</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula_no_intercept</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">analysis_data_interaction</span><span class="p">)</span>

<span class="c1"># Fit the model</span>
<span class="n">results_no_intercept</span> <span class="o">=</span> <span class="n">model_no_intercept</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_no_intercept</span><span class="o">.</span><span class="n">summary2</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                        Results: Ordinary least squares
===============================================================================
Model:                  OLS              Adj. R-squared (uncentered): 0.874    
Dependent Variable:     Q(&#39;dur.interv&#39;)  AIC:                         6816.3656
Date:                   2025-01-14 09:03 BIC:                         6839.4460
No. Observations:       747              Log-Likelihood:              -3403.2  
Df Model:               5                F-statistic:                 1034.    
Df Residuals:           742              Prob (F-statistic):          0.00     
R-squared (uncentered): 0.874            Scale:                       534.02   
-------------------------------------------------------------------------------
                                Coef.  Std.Err.    t    P&gt;|t|   [0.025   0.975]
-------------------------------------------------------------------------------
age                             1.1987   0.0267 44.8706 0.0000   1.1462  1.2511
Q(&#39;scz.cons&#39;)                   5.6921   3.0553  1.8630 0.0629  -0.3060 11.6902
Q(&#39;dep.cons&#39;)                  15.2596   1.9689  7.7502 0.0000  11.3943 19.1250
Q(&#39;subst.cons&#39;)                23.0675   2.4647  9.3591 0.0000  18.2289 27.9062
Q(&#39;dep.cons&#39;):Q(&#39;subst.cons&#39;)  -5.5992   3.8719 -1.4461 0.1486 -13.2003  2.0019
-------------------------------------------------------------------------------
Omnibus:                    3.888            Durbin-Watson:               1.397
Prob(Omnibus):              0.143            Jarque-Bera (JB):            3.925
Skew:                       0.116            Prob(JB):                    0.141
Kurtosis:                   3.268            Condition No.:               214  
===============================================================================
Notes:
[1] R² is computed without centering (uncentered) since the
model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is
correctly specified.
</pre></div>
</div>
</div>
</div>
<p>Removing the intercept led to a dramatic increase in the adjusted R-squared, indicating a much stronger relationship between the predictors and <code class="docutils literal notranslate"><span class="pre">dur.interv</span></code> when the model is forced to pass through the origin. While this improvement is notable, we should be cautious about potential overfitting and ensure that removing the intercept aligns with our understanding of the underlying relationships between the variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_partregress_grid</span><span class="p">(</span><span class="n">results_no_intercept</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/025842e8a9a6c1d438ea49e0b5485c2cf34fb027929d29385b0b3763263911dc.png" src="_images/025842e8a9a6c1d438ea49e0b5485c2cf34fb027929d29385b0b3763263911dc.png" />
</div>
</div>
</section>
<section id="categorical-variables">
<h3>Categorical variables<a class="headerlink" href="#categorical-variables" title="Link to this heading">#</a></h3>
<p>Categorical variables, also known as <em>factor variables</em>, represent qualitative data that can be divided into groups or categories. In our dataset, the variable <code class="docutils literal notranslate"><span class="pre">prof</span></code> is a categorical variable that represents the profession of the inmates. It contains 8 non-NaN classes: <em>agriculteur</em> (farmer), <em>artisan</em> (craftsman), <em>cadre</em> (manager), <em>profession intermédiaire</em> (intermediate profession), <em>employé</em> (employee), <em>ouvrier</em> (worker), <em>autre</em> (other), and <em>sans emploi</em> (unemployed).</p>
<p>To include a categorical variable in a regression model, we need to create dummy variables. Dummy variables are binary variables that represent each category of the categorical variable. For example, to include the variable ‘prof’ in our model, we would create 7 dummy variables, one for each category except for a <em>baseline</em> or <em>reference</em> category.</p>
<p>The choice of reference category is arbitrary and does not affect the overall fit of the model. However, it does affect the interpretation of the coefficients for the dummy variables. The coefficient for a given dummy variable represents the difference in the mean of the outcome variable between the category represented by that dummy variable and the reference category, holding all other variables constant.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>, we can use the <code class="docutils literal notranslate"><span class="pre">C()</span></code> function in the formula to specify that a variable should be treated as categorical. For example, to include ‘prof’ as a categorical variable in our model, we would use the formula <code class="docutils literal notranslate"><span class="pre">Q('dur.interv')</span> <span class="pre">~</span> <span class="pre">age</span> <span class="pre">+</span> <span class="pre">C(prof)</span></code>. This would create 7 dummy variables for ‘prof’ and include them in the model.</p>
<p>Note that if ‘prof’ were an integer variable, we could explicitly treat it as categorical using the <code class="docutils literal notranslate"><span class="pre">C()</span></code> operator.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Examine the unique categories in the &#39;prof&#39; variable</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;prof&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;autre&#39; nan &#39;prof.interm?diaire&#39; &#39;ouvrier&#39; &#39;sans emploi&#39; &#39;artisan&#39;
 &#39;employ?&#39; &#39;agriculteur&#39; &#39;cadre&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a new DataFrame with only the relevant variables</span>
<span class="n">analysis_data_categorical</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;dur.interv&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;dep.cons&#39;</span><span class="p">,</span> <span class="s1">&#39;subst.cons&#39;</span><span class="p">,</span> <span class="s1">&#39;scz.cons&#39;</span><span class="p">,</span> <span class="s1">&#39;prof&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Drop rows with missing values in any of the selected columns</span>
<span class="n">analysis_data_categorical</span> <span class="o">=</span> <span class="n">analysis_data_categorical</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Define the model formula with &#39;prof&#39; as a categorical variable</span>
<span class="n">formula_categorical</span> <span class="o">=</span> <span class="s2">&quot;Q(&#39;dur.interv&#39;) ~ age + Q(&#39;dep.cons&#39;) + Q(&#39;subst.cons&#39;) + Q(&#39;scz.cons&#39;) + C(prof)&quot;</span>

<span class="c1"># Fit the OLS model using the formula and data</span>
<span class="n">model_categorical</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula_categorical</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">analysis_data_categorical</span><span class="p">)</span>

<span class="c1"># Fit the model</span>
<span class="n">results_categorical</span> <span class="o">=</span> <span class="n">model_categorical</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_categorical</span><span class="o">.</span><span class="n">summary2</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                        Results: Ordinary least squares
===============================================================================
Model:                   OLS                  Adj. R-squared:         0.052    
Dependent Variable:      Q(&#39;dur.interv&#39;)      AIC:                    6504.1550
Date:                    2025-01-14 09:03     BIC:                    6559.4834
No. Observations:        743                  Log-Likelihood:         -3240.1  
Df Model:                11                   F-statistic:            4.692    
Df Residuals:            731                  Prob (F-statistic):     5.82e-07 
R-squared:               0.066                Scale:                  365.04   
-------------------------------------------------------------------------------
                               Coef.   Std.Err.    t    P&gt;|t|   [0.025   0.975]
-------------------------------------------------------------------------------
Intercept                      62.7920  10.2078  6.1514 0.0000  42.7519 82.8321
C(prof)[T.artisan]            -11.4851   9.8294 -1.1685 0.2430 -30.7823  7.8120
C(prof)[T.autre]              -10.2875  10.3348 -0.9954 0.3199 -30.5770 10.0020
C(prof)[T.cadre]              -19.2964  10.3857 -1.8580 0.0636 -39.6857  1.0930
C(prof)[T.employ?]            -13.5581   9.7634 -1.3887 0.1654 -32.7257  5.6096
C(prof)[T.ouvrier]            -14.0127   9.7211 -1.4415 0.1499 -33.0973  5.0719
C(prof)[T.prof.interm?diaire] -13.0193   9.9691 -1.3060 0.1920 -32.5908  6.5522
C(prof)[T.sans emploi]        -14.2787   9.7178 -1.4693 0.1422 -33.3568  4.7995
age                             0.2129   0.0588  3.6180 0.0003   0.0974  0.3284
Q(&#39;dep.cons&#39;)                   7.3679   1.4584  5.0521 0.0000   4.5048 10.2311
Q(&#39;subst.cons&#39;)                 5.3459   1.7690  3.0220 0.0026   1.8729  8.8188
Q(&#39;scz.cons&#39;)                   2.5044   2.5473  0.9831 0.3259  -2.4966  7.5054
-------------------------------------------------------------------------------
Omnibus:                  29.873            Durbin-Watson:               1.083 
Prob(Omnibus):            0.000             Jarque-Bera (JB):            33.335
Skew:                     0.461             Prob(JB):                    0.000 
Kurtosis:                 3.475             Condition No.:               1615  
===============================================================================
Notes:
[1] Standard Errors assume that the covariance matrix of the errors is
correctly specified.
[2] The condition number is large, 1.61e+03. This might indicate
that there are strong multicollinearity or other numerical
problems.
</pre></div>
</div>
</div>
</div>
<p>By default, statsmodels chooses the first category <em>alphabetically</em> as the reference category when creating dummy variables for a categorical variable, and the base reference level is the value of the intercept. Alternatively, we can specify the reference category directly within the Patsy formula using the <a class="reference external" href="https://www.statsmodels.org/stable/examples/notebooks/generated/contrasts.html"><code class="docutils literal notranslate"><span class="pre">Treatment</span></code> <strong>contrast</strong></a>, providing a more concise way to control the reference category within the model specification itself. For example, if we want to set ‘ouvrier’ as the reference category for ‘prof’, we can use the following formula:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with &#39;prof&#39; as a categorical variable and &#39;ouvrier&#39; as the reference</span>
<span class="n">formula_categorical_releveled</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Q(&#39;dur.interv&#39;) ~ age + Q(&#39;dep.cons&#39;) + Q(&#39;subst.cons&#39;) + Q(&#39;scz.cons&#39;) + C(prof, Treatment(reference=&#39;ouvrier&#39;))</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Fit the OLS model using the formula and data</span>
<span class="n">model_categorical_releveled</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span>
    <span class="n">formula</span><span class="o">=</span><span class="n">formula_categorical_releveled</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">analysis_data_categorical</span><span class="p">)</span>

<span class="c1"># Fit the model</span>
<span class="n">results_categorical_releveld</span> <span class="o">=</span> <span class="n">model_categorical_releveled</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_categorical_releveld</span><span class="o">.</span><span class="n">summary2</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                       Results: Ordinary least squares
==============================================================================================================
Model:                              OLS                            Adj. R-squared:                   0.052    
Dependent Variable:                 Q(&#39;dur.interv&#39;)                AIC:                              6504.1550
Date:                               2025-01-14 09:03               BIC:                              6559.4834
No. Observations:                   743                            Log-Likelihood:                   -3240.1  
Df Model:                           11                             F-statistic:                      4.692    
Df Residuals:                       731                            Prob (F-statistic):               5.82e-07 
R-squared:                          0.066                          Scale:                            365.04   
--------------------------------------------------------------------------------------------------------------
                                                               Coef.  Std.Err.    t    P&gt;|t|   [0.025   0.975]
--------------------------------------------------------------------------------------------------------------
Intercept                                                     48.7793   2.8394 17.1796 0.0000  43.2050 54.3536
C(prof, Treatment(reference=&#39;ouvrier&#39;))[T.agriculteur]        14.0127   9.7211  1.4415 0.1499  -5.0719 33.0973
C(prof, Treatment(reference=&#39;ouvrier&#39;))[T.artisan]             2.5275   2.4899  1.0151 0.3104  -2.3606  7.4157
C(prof, Treatment(reference=&#39;ouvrier&#39;))[T.autre]               3.7252   3.9964  0.9321 0.3516  -4.1205 11.5710
C(prof, Treatment(reference=&#39;ouvrier&#39;))[T.cadre]              -5.2837   4.2557 -1.2416 0.2148 -13.6384  3.0711
C(prof, Treatment(reference=&#39;ouvrier&#39;))[T.employ?]             0.4546   2.1266  0.2138 0.8308  -3.7203  4.6296
C(prof, Treatment(reference=&#39;ouvrier&#39;))[T.prof.interm?diaire]  0.9934   2.9581  0.3358 0.7371  -4.8139  6.8008
C(prof, Treatment(reference=&#39;ouvrier&#39;))[T.sans emploi]        -0.2660   1.8773 -0.1417 0.8874  -3.9514  3.4195
age                                                            0.2129   0.0588  3.6180 0.0003   0.0974  0.3284
Q(&#39;dep.cons&#39;)                                                  7.3679   1.4584  5.0521 0.0000   4.5048 10.2311
Q(&#39;subst.cons&#39;)                                                5.3459   1.7690  3.0220 0.0026   1.8729  8.8188
Q(&#39;scz.cons&#39;)                                                  2.5044   2.5473  0.9831 0.3259  -2.4966  7.5054
--------------------------------------------------------------------------------------------------------------
Omnibus:                             29.873                      Durbin-Watson:                         1.083 
Prob(Omnibus):                       0.000                       Jarque-Bera (JB):                      33.335
Skew:                                0.461                       Prob(JB):                              0.000 
Kurtosis:                            3.475                       Condition No.:                         574   
==============================================================================================================
Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>The coefficient for ‘agriculteur’ is 14.0127, and its P value is 0.1499. This indicates that, on average, the interview duration for farmers is 14.0127 minutes longer than that for workers, keeping all other variables constant. However, this difference is not statistically significant at the 0.05 level.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In this chapter, we dove into the powerful world of multiple regression, exploring its ability to unravel complex relationships between multiple predictors and a continuous outcome variable. We journeyed through the key assumptions of this method, learned how to construct and interpret models, and explored techniques to refine and visualize our analyses.</p>
<p>We also encountered the intricacies of variable selection, the nuances of interactions between predictors, and the potential challenges posed by multicollinearity. By grounding our exploration in a real-world dataset on mental health in French prisons, we gained practical experience in applying these concepts.</p>
<p>As we move forward, we’ll encounter variations on the theme of regression. The next chapter introduces logistic regression, a technique designed to handle binary outcome variables—those with only two possible outcomes, such as presence or absence of a disease. We’ll discover how this method allows us to model and predict probabilities, opening up a new realm of research questions we can address.</p>
<p>Finally, we’ll delve into ANOVA (Analysis of Variance), a method closely related to multiple regression. While seemingly distinct, ANOVA can be seen as a special case of multiple regression where the predictors are categorical. This connection will deepen our understanding of both techniques and their versatility in analyzing complex biological data.</p>
</section>
<section id="cheat-sheet">
<h2>Cheat sheet<a class="headerlink" href="#cheat-sheet" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>Building the regression model<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.formula.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">smf</span>

<span class="c1"># Create a new DataFrame with only the relevant variables</span>
<span class="n">analysis_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="n">y</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">]]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Drop rows with missing values in any of the selected columns</span>
<span class="n">analysis_data</span> <span class="o">=</span> <span class="n">analysis_data</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Define the model formula using patsy syntax with the interaction term</span>
<span class="n">formula</span> <span class="o">=</span> <span class="s2">&quot;y ~ X1 + X2 + X3&quot;</span>

<span class="c1"># Quote variable names that contains special characters (e.g., periods `.` or hyphens `-`)</span>
<span class="c1"># formula = &quot;Q(&#39;dur.interv&#39;) ~ age + Q(&#39;dep.cons&#39;) + Q(&#39;subst.cons&#39;) + Q(&#39;scz.cons&#39;)&quot;</span>

<span class="c1"># Fit the OLS model using the formula and data</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">analysis_data</span><span class="p">)</span>

<span class="c1"># Fit the model</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the regression results summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="c1"># print(results.summary2())</span>

<span class="c1"># Using pingouin</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pingouin</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pg</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">y</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">]]</span>

<span class="c1"># Fit the linear regression model (for numerical variables)</span>
<span class="n">results_pingouin</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Print the regression results summary</span>
<span class="n">results_pingouin</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="diagnostics">
<h3>Diagnostics<a class="headerlink" href="#diagnostics" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="c1"># Get the residuals from the model</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">resid</span>

<span class="c1"># Plot the histogram of residuals</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot the Q-Q plot of residuals</span>
<span class="n">sm</span><span class="o">.</span><span class="n">qqplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>Visualizing regression relationships<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the partial regression plot grid</span>
<span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_partregress_grid</span><span class="p">(</span><span class="n">results</span><span class="p">);</span>

<span class="p">;</span><span class="c1"># Create the partial regression plot grid</span>
<span class="n">sm</span><span class="o">.</span><span class="n">graphics</span><span class="o">.</span><span class="n">plot_regress_exog</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">X1</span><span class="p">,);</span>
</pre></div>
</div>
</section>
<section id="predictions">
<h3>Predictions<a class="headerlink" href="#predictions" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate predicted values</span>
<span class="n">analysis_data</span><span class="p">[</span><span class="s1">&#39;pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3>Three-dimensional visualization<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>

<span class="c1"># Create a new DataFrame with only the relevant variables</span>
<span class="n">hyperplane_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="n">y</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">]]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Drop rows with missing values in any of the selected columns</span>
<span class="n">hyperplane_data</span> <span class="o">=</span> <span class="n">hyperplane_data</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Define the model formula using patsy syntax</span>
<span class="n">formula</span> <span class="o">=</span> <span class="s2">&quot;y ~ X1 + X2&quot;</span>

<span class="c1"># Fit the OLS model using the formula and data</span>
<span class="n">model_hyperplane</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">hyperplane_data</span><span class="p">)</span>
<span class="n">results_hyperplane</span> <span class="o">=</span> <span class="n">model_hyperplane</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Create the 3D plot</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># Scatter plot of the data points</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">hyperplane_data</span><span class="p">[</span><span class="n">y</span><span class="p">],</span>
    <span class="n">hyperplane_data</span><span class="p">[</span><span class="n">X1</span><span class="p">],</span>
    <span class="n">hyperplane_data</span><span class="p">[</span><span class="n">X2</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># Create the hyperplane</span>
<span class="n">x_surf</span><span class="p">,</span> <span class="n">y_surf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">hyperplane_data</span><span class="p">[</span><span class="n">X1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">hyperplane_data</span><span class="p">[</span><span class="n">X1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">hyperplane_data</span><span class="p">[</span><span class="n">X2</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">hyperplane_data</span><span class="p">[</span><span class="n">X2</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">exog</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">X1</span><span class="p">:</span> <span class="n">x_surf</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">X2</span><span class="p">:</span> <span class="n">y_surf</span><span class="o">.</span><span class="n">ravel</span><span class="p">()})</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">results_hyperplane</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">exog</span><span class="o">=</span><span class="n">exog</span><span class="p">)</span>
<span class="n">z_surf</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_surf</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x_surf</span><span class="p">,</span> <span class="n">y_surf</span><span class="p">,</span> <span class="n">z_surf</span><span class="p">)</span>

<span class="c1"># Set the view angle</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">325</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="id4">
<h3>Interactions<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with interaction using &#39;*&#39;, i.e.,</span>
<span class="c1"># include both the main effects of the variables and their interaction</span>
<span class="n">formula_star</span> <span class="o">=</span> <span class="s2">&quot;y ~ X1 + X2*X3&quot;</span>

<span class="c1"># Define the model formula with interaction using &#39;:&#39;</span>
<span class="n">formula_colon</span> <span class="o">=</span> <span class="s2">&quot;y ~ X1 + X2:X3&quot;</span>
<span class="c1"># only includes the interaction term itself</span>
</pre></div>
</div>
</section>
<section id="id5">
<h3>Quadratic and higher-order polynomial terms<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with the quadratic term for X1 using &#39;identity&#39; I()</span>
<span class="n">formula_quadratic</span> <span class="o">=</span> <span class="s2">&quot;y ~ X1 + X2 + X3 + I(X1**2)&quot;</span>
</pre></div>
</div>
</section>
<section id="id6">
<h3>Special variables<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with the logarithm of X1</span>
<span class="n">formula_log</span> <span class="o">=</span> <span class="s2">&quot;y ~ np.log(X1) + X1&quot;</span>
</pre></div>
</div>
</section>
<section id="id7">
<h3>Removal of intercept<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with the intercept removed</span>
<span class="n">formula_no_intercept</span> <span class="o">=</span> <span class="s2">&quot;y ~ X1 + X2 + X3 - 1&quot;</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>Categorical variables<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model formula with X3 as a categorical variable</span>
<span class="n">formula_categorical</span> <span class="o">=</span> <span class="s2">&quot;y ~ X1 +X2 + C(X3)&quot;</span>

<span class="c1"># Define the model formula with &#39;X3 as a categorical variable and &#39;reference&#39; as the reference</span>
<span class="n">formula_categorical_releveled</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">y ~ X1 + X2 + C(X3, Treatment(reference=&#39;reference&#39;))</span>
<span class="s2">&quot;&quot;&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="session-information">
<h2>Session information<a class="headerlink" href="#session-information" title="Link to this heading">#</a></h2>
<p>The output below details all packages and version necessary to reproduce the results in this report.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>python<span class="w"> </span>--version
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------&quot;</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">importlib.metadata</span><span class="w"> </span><span class="kn">import</span> <span class="n">version</span>

<span class="c1"># List of packages we want to check the version</span>
<span class="n">packages</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;numpy&#39;</span><span class="p">,</span> <span class="s1">&#39;pandas&#39;</span><span class="p">,</span> <span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;seaborn&#39;</span><span class="p">,</span> <span class="s1">&#39;pingouin&#39;</span><span class="p">,</span> <span class="s1">&#39;statsmodels&#39;</span><span class="p">]</span>

<span class="c1"># Initialize an empty list to store the versions</span>
<span class="n">versions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop over the packages</span>
<span class="k">for</span> <span class="n">package</span> <span class="ow">in</span> <span class="n">packages</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Get the version of the package</span>
        <span class="n">package_version</span> <span class="o">=</span> <span class="n">version</span><span class="p">(</span><span class="n">package</span><span class="p">)</span>
        <span class="c1"># Append the version to the list</span>
        <span class="n">versions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">package_version</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>  <span class="c1"># Use a more general exception for broader compatibility</span>
        <span class="n">versions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;Not installed&#39;</span><span class="p">)</span>

<span class="c1"># Print the versions</span>
<span class="k">for</span> <span class="n">package</span><span class="p">,</span> <span class="n">version</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">packages</span><span class="p">,</span> <span class="n">versions</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">package</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Python 3.12.8
-------------
numpy: 1.26.4
pandas: 2.2.2
matplotlib: 3.9.2
seaborn: 0.13.2
pingouin: 0.5.5
statsmodels: 0.14.2
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="36%20-%20Nonlinear%20regression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Nonlinear regression</p>
      </div>
    </a>
    <a class="right-next"
       href="38%20-%20Logistic%20regression.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Logistic regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">Definitions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-multiple-regression">What is multiple regression?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-terminology">Key terminology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mathematical-model">The mathematical model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation">Estimation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-coefficients">Interpreting the coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions">Assumptions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linearity">Linearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-of-errors">Independence of errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#homoscedasticity">Homoscedasticity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normality-of-errors">Normality of errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-perfect-multicollinearity">No perfect multicollinearity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-example">Real-world example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-the-data">Getting the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-regression-model">Building the regression model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-report">Interpretation of the report</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-model-fit">Overall model fit</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficients">Coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#other-diagnostics">Other diagnostics</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-regression-relationships">Visualizing regression relationships</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-and-adjusted-r-squared">R-squared and adjusted R-squared</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-techniques">Advanced techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improvement-of-the-fit">Improvement of the fit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection">Variable selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collinearity-and-multicollinearity">Collinearity and multicollinearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#three-dimensional-visualization">Three-dimensional visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactions">Interactions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-and-higher-order-polynomial-terms">Quadratic and higher-order polynomial terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-variables">Special variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#removal-of-intercept">Removal of intercept</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-variables">Categorical variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cheat-sheet">Cheat sheet</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Building the regression model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostics">Diagnostics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Visualizing regression relationships</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictions">Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Three-dimensional visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Interactions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Quadratic and higher-order polynomial terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Special variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Removal of intercept</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Categorical variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#session-information">Session information</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sébastien Wieckowski
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>