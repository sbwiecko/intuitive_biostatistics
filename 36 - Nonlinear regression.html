
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Nonlinear regression &#8212; The Python Companion of Intuitive Biostatistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-QQY9FLLPJ8"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-QQY9FLLPJ8');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-QQY9FLLPJ8');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '36 - Nonlinear regression';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Comparing models" href="35%20-%20Comparing%20models.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.jpg" class="logo__image only-light" alt="The Python Companion of Intuitive Biostatistics - Home"/>
    <script>document.write(`<img src="_static/logo.jpg" class="logo__image only-dark" alt="The Python Companion of Intuitive Biostatistics - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    Python Companion Guide to “Intuitive Biostatistics”
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Continuous variables</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09%20-%20Quantifying%20scatter%20of%20continuous%20data.html">Quantifying scatter of continuous data</a></li>
<li class="toctree-l1"><a class="reference internal" href="10%20-%20Gaussian%20distribution.html">The Gaussian distribution</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Confidence intervals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12%20-%20Confidence%20interval%20of%20a%20mean.html">Confidence interval of a mean</a></li>
<li class="toctree-l1"><a class="reference internal" href="04%20-%20Confidence%20interval%20of%20a%20proportion.html">Confidence interval of a proportion</a></li>
<li class="toctree-l1"><a class="reference internal" href="05%20-%20Confidence%20interval%20of%20survival%20data.html">Confidence interval of survival data</a></li>
<li class="toctree-l1"><a class="reference internal" href="06%20-%20Confidence%20interval%20of%20counted%20data.html">Confidence interval of counted data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Statistical significance and data assumptions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="15%20-%20Statistical%20significance.html">Statistical significance</a></li>
<li class="toctree-l1"><a class="reference internal" href="20%20-%20Statistical%20power%20and%20sample%20size.html">Statistical power and sample size</a></li>
<li class="toctree-l1"><a class="reference internal" href="24%20-%20Normality%20tests%20and%20outliers.html">Normality tests and outliers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Statistical tests</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="27%20-%20Comparing%20proportions.html">Comparing proportions</a></li>
<li class="toctree-l1"><a class="reference internal" href="29%20-%20Comparing%20survival%20curves.html">Comparing survival curves</a></li>
<li class="toctree-l1"><a class="reference internal" href="30%20-%20Comparing%20two%20unpaired%20means.html">Comparing two unpaired means</a></li>
<li class="toctree-l1"><a class="reference internal" href="31%20-%20Comparing%20paired%20data.html">Comparing paired data</a></li>
<li class="toctree-l1"><a class="reference internal" href="32%20-%20Correlation.html">Correlation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Fitting models to data</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="33%20-%20Simple%20linear%20regression.html">Simple linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="35%20-%20Comparing%20models.html">Comparing models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Nonlinear regression</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sbwiecko/intuitive_biostatistics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sbwiecko/intuitive_biostatistics/edit/master/36 - Nonlinear regression.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/36 - Nonlinear regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Nonlinear regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundation">Mathematical foundation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#various-nonlinear-models">Various nonlinear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-biological-example">A biological example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-estimation">Least squares estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinear-regression-in-python">Nonlinear regression in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-the-data">Exploring the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-guide-to-using-curve-fit">Step-by-Step guide to using <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-model-function">Defining the model function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#providing-initial-parameter-guesses">Providing initial parameter guesses</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-model-to-the-data">Fitting the model to the data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#extraction-standard-errors">Extraction standard errors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-nonlinear-regression-with-lmfit">Advanced nonlinear regression with <code class="docutils literal notranslate"><span class="pre">lmfit</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-parameter-bounds-and-constraints">Setting parameter bounds and constraints</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improved-handling-of-complex-models">Improved handling of complex models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-richer-fit-statistics-and-diagnostics">Accessing richer fit statistics and diagnostics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-values">Predicting values</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-the-goodness-of-fit">Assessing the goodness of fit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-analysis-in-nonlinear-regression">Residual analysis in nonlinear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-residuals-vs-predicted-values">Plotting residuals vs. predicted values</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-patterns-in-residuals-to-identify-potential-issues">Examining Patterns in Residuals to Identify Potential Issues</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-r-squared">Calculating R-squared</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-and-interpreting-non-linear-models">Visualizing and interpreting non-linear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-the-optimal-curve">Plotting the optimal curve</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-confidence-intervals">Generating confidence intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-the-parameter-space-visually">Exploring the parameter space visually</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-nonlinear-models">Comparing nonlinear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nested-models">Nested models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-extra-sum-of-squares-f-test">The extra sum-of-squares F test</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-models-to-compare">Fitting the models to compare</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-the-residuals">Comparing the residuals</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-variance">Analysis of variance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squares">Mean squares</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#f-ratio">F-ratio</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#p-value">P value</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-f-distribution-and-critical-values">Visualizing the F-distribution and critical values</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#anova-table">ANOVA table</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection-criteria-aic-and-bic">Model selection criteria: AIC and BIC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping-for-nonlinear-regression">Bootstrapping for nonlinear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping-in-non-linear-regression">Bootstrapping in non-linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-bootstrap-samples">Generating bootstrap samples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals">Confidence intervals</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-band">Confidence band</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cheat-sheet">Cheat sheet</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-a-model">Fitting a model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#goodness-of-fit">Goodness of fit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-non-linear-models">Visualizing non-linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Confidence intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-non-linear-models">Comparing non-linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping">Bootstrapping</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#session-information">Session information</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="nonlinear-regression">
<h1>Nonlinear regression<a class="headerlink" href="#nonlinear-regression" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In the previous chapter, we explored the power of linear regression to uncover relationships between variables. But what happens when the relationship isn’t a straight line? Many fascinating phenomena in biology, like how enzymes interact with substrates, how populations grow over time, or how organisms respond to drug doses, follow curves, not straight lines. This is where <strong>nonlinear regression</strong> comes into play.</p>
<p>Think of nonlinear regression as an extension of what we’ve already learned. It allows us to <em>fit any model where y is a function of X</em>, even when that <em>relationship is curved</em>. While the underlying math might be a bit more complex (involving some calculus and matrix algebra that we won’t delve into here), the core idea remains the same: we want to <em>find the parameter values</em> that bring our model’s predictions as close as possible to the actual data.</p>
<p>How does it achieve this? Nonlinear regression uses an <strong>iterative</strong> process, kind of like a series of educated guesses. We start with <em>initial estimates</em> for the <strong>parameters</strong>, which gives us a preliminary curve. Then, step-by-step, the method refines these parameters, adjusting the curve to better fit the data points. This continues until further adjustments no longer improve the fit.</p>
<p>In this chapter, we’ll learn how to use Python to perform nonlinear regression, visualize the results, and interpret what they tell us about the biological processes we’re studying. We’ll even explore a powerful package called <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> that makes working with these models even easier.</p>
</section>
<section id="mathematical-foundation">
<h2>Mathematical foundation<a class="headerlink" href="#mathematical-foundation" title="Link to this heading">#</a></h2>
<section id="various-nonlinear-models">
<h3>Various nonlinear models<a class="headerlink" href="#various-nonlinear-models" title="Link to this heading">#</a></h3>
<p>Before diving into the practicalities of fitting nonlinear models, let’s take a closer look at some of the functions commonly used to describe biological phenomena. These functions provide the building blocks for our models, and understanding their shapes and properties will help us interpret the results of our analyses.</p>
<ul>
<li><p><strong>Exponential growth and decay:</strong> we often encounter processes in biology where the rate of change is proportional to the current amount. This leads to exponential growth (e.g., bacterial population growth in ideal conditions) or decay (e.g., radioactive decay). The general form is:</p>
<ul class="simple">
<li><p>Growth: <span class="math notranslate nohighlight">\(y = a  \exp(b X)\)</span></p></li>
<li><p>Decay: <span class="math notranslate nohighlight">\(y = a \exp(-b X)\)</span></p></li>
</ul>
<p>Where <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are parameters controlling the <strong>initial</strong> value and the <strong>rate</strong> of growth/decay, respectively.</p>
</li>
<li><p><strong>Logarithmic functions:</strong> these functions describe relationships where the response variable changes slowly as the predictor variable increases. Examples include the relationship between species richness and habitat area or the response of sensory systems to stimuli. A common form is:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y = a + b  \log(X)\)</span></p></li>
</ul>
<p>Here, <span class="math notranslate nohighlight">\(a\)</span> represents a <strong>baseline</strong> value, and <span class="math notranslate nohighlight">\(b\)</span> determines the <strong>rate</strong> of change.</p>
</li>
<li><p><strong>Power functions:</strong> power functions capture relationships where one variable changes as a power of another. We see this in <em>allometric scaling</em>, where body size relates to metabolic rate, or in the relationship between the length and weight of an organism. The general form is:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y = a  X^b\)</span></p></li>
</ul>
<p>The parameter <span class="math notranslate nohighlight">\(a\)</span> is a <strong>scaling factor</strong>, and <span class="math notranslate nohighlight">\(b\)</span> determines the <strong>power</strong> relationship.</p>
</li>
<li><p><strong>Michaelis-Menten kinetics:</strong> this classic equation describes the rate of enzymatic reactions. It accounts for the saturation effect, where the reaction rate plateaus as the substrate concentration increases:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y = \frac{V_\text{max} X}{K_m + X}\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(V_\text{max}\)</span> represents the <strong>maximum</strong> reaction rate, and <span class="math notranslate nohighlight">\(K_m\)</span> is the Michaelis constant, indicating the substrate concentration at which <em>the reaction rate is half</em> of <span class="math notranslate nohighlight">\(V_\text{max}\)</span>.</p>
</li>
</ul>
<p>The parameters in these equations have biological meaning. Estimating them allows us to quantify key aspects of the system we’re studying. And once we have a well-fit model, we can use it to predict the response variable for new values of the predictor.</p>
</section>
<section id="a-biological-example">
<h3>A biological example<a class="headerlink" href="#a-biological-example" title="Link to this heading">#</a></h3>
<p>To illustrate how these functions apply to real-world research, let’s revisit the <a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/16736152/">study by Frazier and colleagues</a>, which we encountered in earlier chapters. They investigated how norepinephrine affects muscle relaxation in the bladders of young and old rats. Their particular interest lay in understanding whether aging affects the maximum relaxation achievable with high doses of norepinephrine.</p>
<p>In this study, the authors measured muscle relaxation in response to varying concentrations of norepinephrine. They hypothesized that the relationship would follow a <strong>sigmoidal</strong> (S-shaped) curve, which is often observed in <em>dose-response studies</em>. To model this relationship, they used a modified version of the <strong>Hill equation</strong>, also known as the <strong>4-parameter logistic (4PL)</strong> model:</p>
<div class="math notranslate nohighlight">
\[y = y_0 + \frac{y_\text{max} - y_0}{1 + 10^{(\log{\text{EC}_{50}} - X) \cdot n_H}}\]</div>
<p>In this equation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span> is the <strong>logarithm</strong> of the norepinephrine concentration. Using the logarithm often helps linearize the relationship and improve model fitting</p></li>
<li><p><span class="math notranslate nohighlight">\(y\)</span> is the muscle relaxation response (in %)</p></li>
</ul>
<p>In nonlinear regression, the <strong>parameters</strong> are the key to unlocking the secrets hidden within our data. They are the values that define the shape and position of the curve, and they often have direct biological meaning:</p>
<ul class="simple">
<li><p><strong>Bottom (<span class="math notranslate nohighlight">\(y_0\)</span>):</strong> this represents the <strong>baseline</strong> muscle relaxation when no norepinephrine is added. While we might <em>expect</em> this to be zero, it’s crucial to let the data guide us. In some experiments, there might be a small degree of relaxation even without the drug, perhaps due to other factors at play. Therefore, we’ll allow the model to estimate the <span class="math notranslate nohighlight">\(y_0\)</span> parameter from the data, unless we have a strong justification to fix it to a specific value.</p></li>
<li><p><strong>Top (<span class="math notranslate nohighlight">\(y_\text{max}\)</span>):</strong> this parameter represents the maximum muscle relaxation, measured as %E<sub>max</sub>, achieved at high norepinephrine concentrations. A higher <span class="math notranslate nohighlight">\(\text{Top}\)</span> value might indicate a greater potential for muscle relaxation in a particular group of individuals or under specific experimental conditions. In fact, this is precisely the kind of comparison we performed in the chapter on unpaired t-tests, where we assessed whether there was a significant difference in the mean %E<sub>max</sub> between the two age groups.</p></li>
<li><p><strong>logEC<sub>50</sub>:</strong> this parameter tells us about the sensitivity of the muscle to norepinephrine. A lower <span class="math notranslate nohighlight">\(\log{\text{EC}_{50}}\)</span> means that a lower concentration of norepinephrine is needed to achieve half-maximal relaxation, indicating higher sensitivity. We might find that muscles from younger individuals have a lower <span class="math notranslate nohighlight">\(\log{\text{EC}_{50}}\)</span> than those from older individuals, suggesting age-related changes in sensitivity to the neurotransmitter.</p></li>
<li><p><strong>Hill slope (<span class="math notranslate nohighlight">\(n_H\)</span>):</strong> this parameter, also called Hill coefficient, reflects how steeply the muscle relaxation increases with increasing norepinephrine concentration. A steeper slope suggests a more switch-like response, where a small change in concentration leads to a large change in relaxation. Differences in <span class="math notranslate nohighlight">\(n_H\)</span> between experimental groups could indicate variations in the underlying mechanisms of norepinephrine action.</p></li>
</ul>
</section>
<section id="least-squares-estimation">
<h3>Least squares estimation<a class="headerlink" href="#least-squares-estimation" title="Link to this heading">#</a></h3>
<p>Just like in linear regression, nonlinear regression relies on the principle of <strong>least squares estimation</strong>. But instead of fitting a straight line, we’re now dealing with curves. Imagine we have a set of data points and a nonlinear function that we think describes the relationship between them. Our goal is to find the <strong>parameter</strong> values that make this function fit the data as closely as possible.</p>
<p>Least squares estimation aims to* minimize the sum of the squares* of the residuals, in other words, we want to find the parameter values that make the sum of the squared residuals as small as possible.</p>
<p>Mathematically, we can express this as finding the values of the parameters (like <span class="math notranslate nohighlight">\(y_0\)</span>, <span class="math notranslate nohighlight">\(y_\text{max}\)</span>, <span class="math notranslate nohighlight">\(\log{\text{EC}_{50}}\)</span>, and <span class="math notranslate nohighlight">\(n_H\)</span> in our muscle relaxation example) that minimize the <strong>weighted sum of squares</strong>, often reprensented by the symbol <span class="math notranslate nohighlight">\(\chi^2\)</span> (chi-squared):</p>
<div class="math notranslate nohighlight">
\[\chi^2 = \sum_i^n{\frac{[y_i-y(x_i;a_k)]^2}{\epsilon_i}}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the observed value of the response variable for the <span class="math notranslate nohighlight">\(i\)</span>-th data point, <span class="math notranslate nohighlight">\(y(x_i; a_k)\)</span> is the predicted value of the response variable based on our model with parameters <span class="math notranslate nohighlight">\(a_k\)</span>, and <span class="math notranslate nohighlight">\(\epsilon_i\)</span> represents the estimated <em>uncertainty</em> associated with each data point. By including this in the denominator, we give less weight to data points with higher uncertainty, so they don’t unduly influence the fit. This is the core principle of <a class="reference external" href="https://en.wikipedia.org/wiki/Weighted_least_squares"><strong>weighted least squares (WLS)</strong></a>.</p>
<p>While the concept is similar to linear regression, finding the best-fit parameters in nonlinear models is a bit more involved. We can’t simply solve an equation directly like we did for linear regression. Instead, we use an iterative process, where the algorithm starts with initial guesses for the parameters and gradually refines them to minimize χ².</p>
<p>One common approach is the <strong><a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent algorithm</a></strong>. Imagine you’re trying to find the lowest point in a valley. Gradient descent is like taking small steps downhill, always moving in the direction of steepest descent, until you reach the bottom.</p>
<p><img alt="grandient descent illustration" src="https://www.researchgate.net/profile/Konstantinos-Patlatzoglou/publication/364197638/figure/fig9/AS:11431281088359013&#64;1665048829013/Gradient-descent-visualization-Amini-et-al-2018-At-each-training-step-epoch-the.png" /></p>
<p>In our case, the “valley” represents the  χ<sup>2</sup>  surface, and the “lowest point” corresponds to the parameter values that minimize the sum of squared residuals.  The gradient descent algorithm starts with initial guesses for the parameters and then repeatedly updates them using this equation:</p>
<div class="math notranslate nohighlight">
\[a_{\text{next}} = a_{\text{cur}} + \gamma \sum_i^n{\frac{(y_i - y(x_i;a_k))}{\epsilon_i} \frac{\partial y}{\partial a_k}}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(a_{\text{next}}\)</span> represents the updated value of the parameter in the <em>next</em> iteration</p></li>
<li><p><span class="math notranslate nohighlight">\(a_{\text{cur}}\)</span> is the <em>current</em> value of the parameter</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> is a <strong>step size</strong> parameter that controls how big of a step we take in the direction of descent</p></li>
<li><p>The remaining part of the equation essentially calculates the <strong>direction</strong> of steepest descent on the <span class="math notranslate nohighlight">\(\chi^2\)</span> surface.</p></li>
</ul>
</section>
</section>
<section id="nonlinear-regression-in-python">
<h2>Nonlinear regression in Python<a class="headerlink" href="#nonlinear-regression-in-python" title="Link to this heading">#</a></h2>
<section id="exploring-the-data">
<h3>Exploring the data<a class="headerlink" href="#exploring-the-data" title="Link to this heading">#</a></h3>
<p>Before we dive into the analysis, let’s take a quick look at the data we’ll be working with. This is always a good practice to get a sense of the relationships and potential issues.</p>
<p>One important assumption in nonlinear regression is that the <em>independent variable has minimal variability</em>. This means that we assume there’s very little error in measuring or setting the independent variable (in our case, the norepinephrine concentration). This is often the case in designed experiments where we carefully control the independent variable.</p>
<p>Here’s the data from Frazier et al., showing the muscle relaxation response to varying concentrations of norepinephrine.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Data from page 367</span>
<span class="n">relaxation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.6</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">,</span> <span class="mf">15.8</span><span class="p">,</span> <span class="mf">21.1</span><span class="p">,</span> <span class="mf">36.8</span><span class="p">,</span> <span class="mf">57.9</span><span class="p">,</span> <span class="mf">73.7</span><span class="p">,</span> <span class="mf">89.5</span><span class="p">,</span> <span class="mf">94.7</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
<span class="n">norepi_log</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">.5</span><span class="p">)</span>  <span class="c1"># Log scale concentrations</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that we’re working with the logarithm of the norepinephrine concentration (‘norepi_log’). This is often done in dose-response studies because it can help linearize the relationship and improve model fitting.</p>
<p>Now let’s visualize the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># For plotting, we&#39;ll need linear concentrations</span>
<span class="n">norepi_lin</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="n">norepi_log</span>

<span class="c1"># Create the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">norepi_lin</span><span class="p">,</span> <span class="n">relaxation</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Bladder muscle relaxation data for one young rat&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;[Norepinephrine, M] (log scale)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;</span><span class="si">% r</span><span class="s2">elaxation&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/abcc70e3a40d4c812d92a7d1bbed5843903df65d493bde09867f60128c1fa7b7.png" src="_images/abcc70e3a40d4c812d92a7d1bbed5843903df65d493bde09867f60128c1fa7b7.png" />
</div>
</div>
<p>This plot gives us a visual representation of the relationship between norepinephrine concentration and muscle relaxation. We can clearly see the sigmoidal shape, suggesting that the Hill equation might be a suitable model for this data.</p>
</section>
<section id="step-by-step-guide-to-using-curve-fit">
<h3>Step-by-Step guide to using <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code><a class="headerlink" href="#step-by-step-guide-to-using-curve-fit" title="Link to this heading">#</a></h3>
<p>Now that we have a visual understanding of our data, let’s use the <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"><code class="docutils literal notranslate"><span class="pre">curve_fit</span></code> function from the <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code> library</a> to fit the Hill equation.</p>
<section id="defining-the-model-function">
<h4>Defining the model function<a class="headerlink" href="#defining-the-model-function" title="Link to this heading">#</a></h4>
<p>In Python, we can define <strong>functions</strong> to represent the mathematical equations we want to use for our models. These functions will take the independent variable <span class="math notranslate nohighlight">\(X\)</span> (‘norepi_log’) and the necessary parameters as inputs and return the predicted values of the dependent variable <span class="math notranslate nohighlight">\(y\)</span> (‘relaxation’).</p>
<p>Here’s the general Hill equation (4PL model):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hill_equation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">logEC50</span><span class="p">,</span> <span class="n">hill_slope</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function defines the general Hill equation (4PL).</span>

<span class="sd">    Args:</span>
<span class="sd">      x: The logarithm of norepinephrine concentration.</span>
<span class="sd">      bottom: The baseline relaxation (Y0).</span>
<span class="sd">      top: The maximum relaxation (Ymax).</span>
<span class="sd">      logEC50: The logarithm of the EC50.</span>
<span class="sd">      hill_slope: The Hill slope (nH).</span>

<span class="sd">    Returns:</span>
<span class="sd">      The predicted relaxation values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bottom</span> <span class="o">+</span> <span class="p">(</span><span class="n">top</span> <span class="o">-</span> <span class="n">bottom</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">10</span><span class="o">**</span><span class="p">((</span><span class="n">logEC50</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">hill_slope</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Since we’ll be working with a case where the ‘bottom’ is fixed at 0, let’s define a separate function for that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hill_equation_bottom_zero</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">logEC50</span><span class="p">,</span> <span class="n">hill_slope</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function defines the Hill equation with bottom fixed at 0.</span>

<span class="sd">    Args:</span>
<span class="sd">      x: The logarithm of norepinephrine concentration.</span>
<span class="sd">      top: The maximum relaxation (Ymax).</span>
<span class="sd">      logEC50: The logarithm of the EC50.</span>
<span class="sd">      hill_slope: The Hill slope (nH).</span>

<span class="sd">    Returns:</span>
<span class="sd">      The predicted relaxation values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">top</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">10</span><span class="o">**</span><span class="p">((</span><span class="n">logEC50</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">hill_slope</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>And here’s a function for the <strong>3PL model</strong>, where the ‘hill_slope’ is fixed to 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hill_equation_3pl</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">logEC50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function defines the 3-parameter logistic (3PL) equation.</span>

<span class="sd">    Args:</span>
<span class="sd">      x: The logarithm of norepinephrine concentration.</span>
<span class="sd">      bottom: The baseline relaxation (Y0).</span>
<span class="sd">      top: The maximum relaxation (Ymax).</span>
<span class="sd">      logEC50: The logarithm of the EC50.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The predicted relaxation values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bottom</span> <span class="o">+</span> <span class="p">(</span><span class="n">top</span> <span class="o">-</span> <span class="n">bottom</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="n">logEC50</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>  <span class="c1"># hill_slope is fixed to 1</span>
</pre></div>
</div>
</div>
</div>
<p>By defining these separate functions, we make our code more modular and easier to understand. When we need to fit a specific model, we can simply call the corresponding function. This approach is good practice for organizing and maintaining scientific code which define particular mathematical equations.</p>
</section>
<section id="providing-initial-parameter-guesses">
<h4>Providing initial parameter guesses<a class="headerlink" href="#providing-initial-parameter-guesses" title="Link to this heading">#</a></h4>
<p>Before we can fit our model, we need to provide <strong>initial guesses</strong> for the parameters. This is important because nonlinear regression algorithms often use <strong>iterative</strong> methods that start from these <strong>initial values</strong> and refine them step by step. Good initial guesses can help the algorithm <strong>converge</strong> faster and find the true best-fit solution.</p>
<p>But how do we come up with reasonable initial guesses?</p>
<ul class="simple">
<li><p>Visual inspection of the data: looking at the plot of the data, can we estimate the ‘top’ (maximum value), and the ‘logEC50’ (x-value where the response is halfway between ‘top’ and ‘bottom’)? What about the ‘hill_slope’ (how steep the curve is)?</p></li>
<li><p>Prior knowledge or literature: do we have any prior information about the system we’re studying? Perhaps there are published studies that report similar parameter values for related experiments. This knowledge can be invaluable in choosing realistic initial guesses. For example, we expect no relaxation in the absence of norepinephrine, which guides our decision to use the <code class="docutils literal notranslate"><span class="pre">hill_equation_bottom_zero</span></code> function where the ‘bottom’ (minimum value) is fixed at zero.</p></li>
<li><p>Trial and error: if we’re unsure, we can start with some reasonable guesses and see how the fit looks. If it’s not good, we can adjust the initial guesses and try again.</p></li>
</ul>
<p>Based on a visual inspection of the data, we’re guessing that ‘top’ is around 100, ‘logEC50’ is around -6, and ‘hill_slope’ is around 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initial guess for the parameters (bottom is fixed to 0)</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># Initial guesses for top, logEC50, and hill_slope</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="fitting-the-model-to-the-data">
<h4>Fitting the model to the data<a class="headerlink" href="#fitting-the-model-to-the-data" title="Link to this heading">#</a></h4>
<p>Now that we have our initial guesses, we can use <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code> to find the best-fit parameters. <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code> takes the following main arguments:</p>
<ul class="simple">
<li><p>The model function (in our case, <code class="docutils literal notranslate"><span class="pre">hill_equation_bottom_zero</span></code>)</p></li>
<li><p>The independent variable data <span class="math notranslate nohighlight">\(X\)</span> (‘norepi_log’)</p></li>
<li><p>The dependent variable data <span class="math notranslate nohighlight">\(y\)</span> (‘relaxation’)</p></li>
<li><p>An optional initial guess for the parameters (‘p0’)</p></li>
</ul>
<p>Here’s how we can use <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code> for our muscle relaxation data:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">curve_fit</span></code>: it performs the nonlinear least squares optimization and returns two values:</p>
<ul>
<li><p>‘best_vals’: an <em>array</em> containing the best-fit values for the parameters (‘top’, ‘logEC50’, ‘hill_slope’ in this case)</p></li>
<li><p>‘covar’: the estimated covariance matrix of the parameters, which we’ll use later for calculating standard errors and confidence intervals</p></li>
</ul>
</li>
<li><p>Printing results: we print the best-fit values of the parameters, including the EC50 converted back to linear scale.</p></li>
</ul>
<p>This code snippet performs the core nonlinear regression analysis, giving us the parameter estimates that best describe the relationship between norepinephrine concentration and muscle relaxation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">curve_fit</span>

<span class="c1"># Fit the 4PL model to the data (bottom is fixed to 0 in the function definition)</span>
<span class="n">best_vals</span><span class="p">,</span> <span class="n">covar</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span>
    <span class="n">hill_equation_bottom_zero</span><span class="p">,</span>
    <span class="n">norepi_log</span><span class="p">,</span>
    <span class="n">relaxation</span><span class="p">,</span>
    <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>

<span class="c1"># Print the best-fit parameter values</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best-fit values of parameters (4PL model with fixed bottom):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-----------------------------------------------------------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;top = </span><span class="si">{</span><span class="n">best_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LogEC50 = </span><span class="si">{</span><span class="n">best_vals</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;EC50 = </span><span class="si">{</span><span class="mi">10</span><span class="o">**</span><span class="n">best_vals</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;hill_slope = </span><span class="si">{</span><span class="n">best_vals</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best-fit values of parameters (4PL model with fixed bottom):
-----------------------------------------------------------
top = 104.1
LogEC50 = -5.64
EC50 = 2.30e-06
hill_slope = 0.622
</pre></div>
</div>
</div>
</div>
</section>
<section id="extraction-standard-errors">
<h4>Extraction standard errors<a class="headerlink" href="#extraction-standard-errors" title="Link to this heading">#</a></h4>
<p>As we just saw, <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code> also returns ‘covar’, the <em>estimated covariance matrix</em> of the parameters from the last iteration of the fitting process. This matrix provides information about the variability of the estimated parameters and their relationships with each other. It can be represented in a general form like this:</p>
<p><span class="math notranslate nohighlight">\(\Sigma = \begin{bmatrix}
  \sigma_1^2 &amp; cov_{12} &amp; \dots &amp; cov_{1n} \\
  cov_{21} &amp; \sigma_2^2 &amp; \dots &amp; cov_{2n} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  cov_{n1} &amp; cov_{n2} &amp; \dots &amp; \sigma_n^2
\end{bmatrix}\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma_i^2\)</span> is the variance of the <span class="math notranslate nohighlight">\(i\)</span>-th parameter estimate</p></li>
<li><p><span class="math notranslate nohighlight">\(cov_{ij}\)</span> is the covariance between the <span class="math notranslate nohighlight">\(i\)</span>-th and <span class="math notranslate nohighlight">\(j\)</span>-th parameter estimates</p></li>
</ul>
<p>We encountered these terms in the chapters on correlation and simple linear regression. In the latter, the least squares method provided the standard error of the estimated coefficients as:</p>
<p><span class="math notranslate nohighlight">\(s_{\hat{\beta}_j} = \sqrt{s^2 [(\mathbf{X}^T \mathbf{X})^{-1}]_{j,j}}\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s^2\)</span> is the estimated variance of the error term, also known as the <strong>mean squared error (MSE)</strong>, and calculated as <span class="math notranslate nohighlight">\(s^2 = \frac{\text{RSS}}{\text{DF}}\)</span>, where <span class="math notranslate nohighlight">\(\text{RSS}\)</span> is the residual sum of squares and <span class="math notranslate nohighlight">\(\text{DF}\)</span> is the degrees of freedom.</p></li>
<li><p><span class="math notranslate nohighlight">\([(\mathbf{X}^T \mathbf{X})^{-1}]_{j,j}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th diagonal element of the inverted <span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span> matrix, which represents the variance of the <span class="math notranslate nohighlight">\(j\)</span>-th coefficient estimate.</p></li>
</ul>
<p>As we can see, the diagonal elements of this matrix provide the variances of the <em>parameter estimates</em>, which we’ll use to calculate the standard errors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the standard errors of the parameters</span>
<span class="n">standard_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">covar</span><span class="p">))</span>

<span class="c1"># Print the standard errors</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Standard errors of the parameters:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;----------------------------------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Top: </span><span class="si">{</span><span class="n">standard_errors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LogEC50: </span><span class="si">{</span><span class="n">standard_errors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;HillSlope: </span><span class="si">{</span><span class="n">standard_errors</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Standard errors of the parameters:
----------------------------------
Top: 2.06
LogEC50: 0.052
HillSlope: 0.0358
</pre></div>
</div>
</div>
</div>
<p>The standard errors give us a measure of the <strong>uncertainty</strong> in our parameter estimates. As we’ve learned in previous chapters, a larger standard error indicates greater uncertainty. To calculate the standard errors, we first extract the <em>diagonal elements of the covariance matrix</em>, which represent the <strong>variances</strong> of the parameters. Then, we take the square root of these variances.</p>
<p>Standard errors are crucial for assessing the reliability of our parameter estimates. They tell us how much the estimated parameters might vary due to random noise in the data. For instance, they play a role in <em>hypothesis tests</em> to determine whether a parameter is significantly different from a hypothesized value. We can also use the standard errors to construct <em>confidence intervals</em> around the parameter estimates, which we’ll explore in a later section of this chapter.</p>
</section>
</section>
</section>
<section id="advanced-nonlinear-regression-with-lmfit">
<h2>Advanced nonlinear regression with <code class="docutils literal notranslate"><span class="pre">lmfit</span></code><a class="headerlink" href="#advanced-nonlinear-regression-with-lmfit" title="Link to this heading">#</a></h2>
<p>In the previous section, we explored how to perform nonlinear regression using <code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit</span></code>. Now, let’s introduce a powerful package called <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> that offers several advantages and makes nonlinear regression even more flexible and intuitive.</p>
<p><a class="reference external" href="https://lmfit.github.io/lmfit-py/"><code class="docutils literal notranslate"><span class="pre">lmfit</span></code> is a free and open-source Python library</a> specifically designed for nonlinear least-squares minimization and curve fitting. It builds upon the foundation of <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code> but provides a higher-level interface with many enhancements.</p>
<p>Here are some key advantages of using <code class="docutils literal notranslate"><span class="pre">lmfit</span></code>:</p>
<ul class="simple">
<li><p><strong>Parameter objects:</strong> instead of working with plain numbers for parameters, <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> uses ‘Parameter’ objects. These objects allow us to:</p>
<ul>
<li><p>Give meaningful names to the parameters</p></li>
<li><p><a class="reference external" href="https://lmfit.github.io/lmfit-py/constraints.html">Set bounds and constraints on parameters</a> (e.g., force a parameter to be positive)</p></li>
<li><p>Easily fix parameters to constant values. For instance, we can use the general <code class="docutils literal notranslate"><span class="pre">hill_equation</span></code> function and fix the ‘bottom’ parameter to 0, effectively creating the <code class="docutils literal notranslate"><span class="pre">hill_equation_bottom_zero</span></code> functionality within <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> itself</p></li>
<li><p>Define relationships between parameters (e.g., one parameter is always twice the value of another)</p></li>
</ul>
</li>
<li><p><strong>Improved model fitting:</strong> provides a <code class="docutils literal notranslate"><span class="pre">Model</span></code> class that makes it easier to define and work with complex models. It also offers more robust fitting algorithms and better handling of difficult optimization problems.</p></li>
<li><p><strong>Enhanced reporting:</strong> provides more comprehensive fit statistics and diagnostic information, making it easier to assess the quality of your fit and identify potential issues.</p></li>
<li><p><strong>Flexibility:</strong> allows to easily switch between different fitting algorithms without changing your model definition.</p></li>
</ul>
<p>We’ll use <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> in this chapter because it offers a more intuitive and powerful way to perform nonlinear regression, especially for complex models or when we need more control over the fitting process. It allows us to clearly define and manage our model parameters, set constraints and bounds on parameters, which can be important for ensuring biologically meaningful results, and access a wider range of fit statistics and diagnostic tools, simplifying the workflow and providing richer insights into our model.</p>
<section id="setting-parameter-bounds-and-constraints">
<h3>Setting parameter bounds and constraints<a class="headerlink" href="#setting-parameter-bounds-and-constraints" title="Link to this heading">#</a></h3>
<p>One of the powerful features of <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> is the ability to set bounds and constraints on our parameters. This can be crucial for ensuring that our model results are biologically meaningful.</p>
<p>For example, in our muscle relaxation study, it doesn’t make sense for the ‘top’ parameter (maximum relaxation) to be negative. Similarly, the ‘hill_slope’ parameter should ideally be positive, as a negative slope would imply that muscle relaxation decreases with increasing norepinephrine concentration, which is contrary to our expectations.</p>
<p>Moreover, by fixing ‘bottom’ to 0 within the general <code class="docutils literal notranslate"><span class="pre">hill_equation</span></code>, we provide a concrete illustration of how <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> allows for flexible parameter handling. It shows that we can achieve the same result as defining a separate function (<code class="docutils literal notranslate"><span class="pre">hill_equation_bottom_zero</span></code>) but with more direct control within the <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> framework.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lmfit</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="c1"># Create a model object</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">hill_equation</span><span class="p">)</span>

<span class="c1"># Create parameter objects, setting bounds and constraints</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">make_params</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">logEC50</span><span class="o">=-</span><span class="mi">6</span><span class="p">,</span> <span class="n">hill_slope</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># params[&#39;bottom&#39;].value = 0  # Set initial value for bottom to zero</span>
<span class="c1"># params[&#39;bottom&#39;].vary = False  # and fix it (behaves like in hill_equation_bottom_zero)</span>
<span class="n">params</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;bottom&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Same as the two lines above</span>

<span class="n">params</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># top must be non-negative</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;hill_slope&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># hill_slope must be non-negative</span>

<span class="c1"># Display the initial parameters set</span>
<span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="jp-toc-ignore"><caption>Parameters</caption><tr><th style='text-align:left'>name</th><th style='text-align:left'>value</th><th style='text-align:left'>initial value</th><th style='text-align:left'>min</th><th style='text-align:left'>max</th><th style='text-align:right'>vary</th></tr><tr><td style='text-align:left'>bottom</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'>0</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>False</td></tr><tr><td style='text-align:left'>top</td><td style='text-align:left'> 100.000000</td><td style='text-align:left'>100.0</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>logEC50</td><td style='text-align:left'>-6.00000000</td><td style='text-align:left'>-6.0</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>hill_slope</td><td style='text-align:left'> 1.00000000</td><td style='text-align:left'>1.0</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr></table></div></div>
</div>
<p>First, we create a <code class="docutils literal notranslate"><span class="pre">Model</span></code> object from our <code class="docutils literal notranslate"><span class="pre">hill_equation</span></code> function. Then, we create a ‘Parameters’ object, which allows us to set initial guesses and constraints for the model parameters. We use this object to:</p>
<ul class="simple">
<li><p>Fix the ‘bottom’ parameter to 0 by setting its value attribute to 0 and its vary attribute to <code class="docutils literal notranslate"><span class="pre">False</span></code>. This prevents it from being adjusted during the fitting process</p></li>
<li><p>Set the ‘min’ attribute of the ‘top’ and ‘hill_slope’ parameters to 0, ensuring they cannot take negative values during the fitting process.</p></li>
</ul>
<p>By setting the constraints we defined earlier, we ensure that the optimization algorithm explores only biologically plausible parameter values. This prevents the algorithm from getting stuck in unrealistic regions of the parameter space, leading to more meaningful and interpretable results. Ultimately, this increases our confidence that the fitted model accurately reflects the underlying biological process.</p>
</section>
<section id="improved-handling-of-complex-models">
<h3>Improved handling of complex models<a class="headerlink" href="#improved-handling-of-complex-models" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">lmfit</span></code> offers significant improvements for handling complex models, including:</p>
<ul class="simple">
<li><p>Flexibility in model definition: the <code class="docutils literal notranslate"><span class="pre">Model</span></code> class allows for combining functions, defining parameter relationships, and handling intricate structures. For example, we can easily define a model that involves multiple interacting components or where one parameter is a function of another</p></li>
<li><p>Choice of minimizers and robust algorithms: <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> allows us to <a class="reference external" href="https://lmfit.github.io/lmfit-py/fitting.html#choosing-different-fitting-methods">choose from a variety of optimization algorithms (minimizers)</a> to find the best-fit parameters, including more robust algorithms that can handle high-dimensional parameter spaces and challenging objective functions. This flexibility can be crucial for complex models where different algorithms might perform better depending on the shape of the objective function and the parameter space</p></li>
<li><p>Better handling of parameter constraints: the ‘Parameters’ object provides a structured way to define constraints, ensuring biologically meaningful results</p></li>
<li><p>Enhanced reporting and diagnostics: <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> offers comprehensive fit statistics and diagnostic information to assess the quality of the fit and identify potential issues.</p></li>
</ul>
<p>Now, let’s use the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method to fit the model to our data, using the ‘Parameters’ object and the independent variable data. We’ll then display the optimized parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the model to the data</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">relaxation</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">norepi_log</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,)</span>

<span class="c1"># Display the optimized parameters</span>
<span class="n">result</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="jp-toc-ignore"><caption>Parameters</caption><tr><th style='text-align:left'>name</th><th style='text-align:left'>value</th><th style='text-align:left'>standard error</th><th style='text-align:left'>relative error</th><th style='text-align:left'>initial value</th><th style='text-align:left'>min</th><th style='text-align:left'>max</th><th style='text-align:right'>vary</th></tr><tr><td style='text-align:left'>bottom</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'></td><td style='text-align:left'>0</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>False</td></tr><tr><td style='text-align:left'>top</td><td style='text-align:left'> 104.053054</td><td style='text-align:left'> 2.05947861</td><td style='text-align:left'>(1.98%)</td><td style='text-align:left'>100.0</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>logEC50</td><td style='text-align:left'>-5.63808943</td><td style='text-align:left'> 0.05151637</td><td style='text-align:left'>(0.91%)</td><td style='text-align:left'>-6.0</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>hill_slope</td><td style='text-align:left'> 0.62209892</td><td style='text-align:left'> 0.03579809</td><td style='text-align:left'>(5.75%)</td><td style='text-align:left'>1.0</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr></table></div></div>
</div>
</section>
<section id="accessing-richer-fit-statistics-and-diagnostics">
<h3>Accessing richer fit statistics and diagnostics<a class="headerlink" href="#accessing-richer-fit-statistics-and-diagnostics" title="Link to this heading">#</a></h3>
<p>One of the major advantages of <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> is that it provides a wealth of information about the fit beyond just the parameter estimates. This information can help us assess the quality of the fit and identify potential issues.</p>
<p>Here’s how we can access some of these statistics and diagnostics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the fit report</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">fit_report</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[Model]]
    Model(hill_equation)
[[Fit Statistics]]
    # fitting method   = leastsq
    # function evals   = 25
    # data points      = 11
    # variables        = 3
    chi-square         = 42.9570499
    reduced chi-square = 5.36963124
    Akaike info crit   = 20.9853605
    Bayesian info crit = 22.1790464
    R-squared          = 0.99712027
[[Variables]]
    bottom:      0 (fixed)
    top:         104.053054 +/- 2.05947861 (1.98%) (init = 100)
    logEC50:    -5.63808943 +/- 0.05151637 (0.91%) (init = -6)
    hill_slope:  0.62209892 +/- 0.03579809 (5.75%) (init = 1)
[[Correlations]] (unreported correlations are &lt; 0.100)
    C(top, logEC50)        = +0.7796
    C(top, hill_slope)     = -0.6955
    C(logEC50, hill_slope) = -0.5410
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">fit_report</span></code> method prints a comprehensive report of the fit results, including:</p>
<ul class="simple">
<li><p>Best-fit parameter values and their <em>standard errors</em> (not the margin of error)</p></li>
<li><p>Chi-squared value and reduced chi-squared</p></li>
<li><p>Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)</p></li>
<li><p>R-squared</p></li>
<li><p>Correlation matrix of the parameters</p></li>
</ul>
<p>We can directly access specific fit statistics using attributes like <code class="docutils literal notranslate"><span class="pre">best_values</span></code>, <code class="docutils literal notranslate"><span class="pre">nfree</span></code>, <code class="docutils literal notranslate"><span class="pre">chisqr</span></code>, <code class="docutils literal notranslate"><span class="pre">redchi</span></code>, <code class="docutils literal notranslate"><span class="pre">aic</span></code>, <code class="docutils literal notranslate"><span class="pre">bic</span></code>, and <code class="docutils literal notranslate"><span class="pre">rsquared</span></code>. Similar to <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code>, the <code class="docutils literal notranslate"><span class="pre">covar</span></code> attribute in the SciPy optimization result holds the covariance matrix of the parameters. From this, we can calculate the correlation matrix, which can help us identify potential issues with parameter identifiability.</p>
<p>By taking advantage of these richer fit statistics and diagnostics, we can gain a deeper understanding of our nonlinear regression models and make more informed conclusions about the biological processes we’re studying.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Access specific fit statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best-fit values: &quot;</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">best_values</span><span class="p">)</span>  <span class="c1"># Returns a dictionnary</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DF: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">nfree</span><span class="si">:</span><span class="s2">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Absolute sum of squares (χ²): </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">chisqr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reduced chi-squared: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">redchi</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Akaike Information Criterion (AIC): </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">aic</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># type: ignore</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bayesian Information Criterion (BIC): </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">bic</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># type: ignore</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R²: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">rsquared</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Access parameter correlation matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Parameter correlation matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">covar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best-fit values:  {&#39;bottom&#39;: 0, &#39;top&#39;: 104.05305410668639, &#39;logEC50&#39;: -5.638089431195984, &#39;hill_slope&#39;: 0.6220989155826002}
DF: 8
Absolute sum of squares (χ²): 42.96
Reduced chi-squared: 5.37
Akaike Information Criterion (AIC): 20.99
Bayesian Information Criterion (BIC): 22.18
R²: 0.997

Parameter correlation matrix:
[[ 4.24145214e+00  8.27134462e-02 -5.12779994e-02]
 [ 8.27134462e-02  2.65393682e-03 -9.97621974e-04]
 [-5.12779994e-02 -9.97621974e-04  1.28150323e-03]]
</pre></div>
</div>
</div>
</div>
<p>The ‘result’ attribute provides a more structured representation of the fit results, including the optimized parameters, their standard errors, and other statistics. This can be useful for programmatically accessing specific values or for a more compact presentation of the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Access and display the fit results using result.result</span>
<span class="n">result</span><span class="o">.</span><span class="n">result</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><h2>Fit Result</h2> <table class="jp-toc-ignore"><caption class="jp-toc-ignore">Fit Statistics</caption><tr><td style='text-align:left'>fitting method</td><td style='text-align:right'>leastsq</td></tr><tr><td style='text-align:left'># function evals</td><td style='text-align:right'>25</td></tr><tr><td style='text-align:left'># data points</td><td style='text-align:right'>11</td></tr><tr><td style='text-align:left'># variables</td><td style='text-align:right'>3</td></tr><tr><td style='text-align:left'>chi-square</td><td style='text-align:right'> 42.9570499</td></tr><tr><td style='text-align:left'>reduced chi-square</td><td style='text-align:right'> 5.36963124</td></tr><tr><td style='text-align:left'>Akaike info crit.</td><td style='text-align:right'> 20.9853605</td></tr><tr><td style='text-align:left'>Bayesian info crit.</td><td style='text-align:right'> 22.1790464</td></tr></table><table class="jp-toc-ignore"><caption>Parameters</caption><tr><th style='text-align:left'>name</th><th style='text-align:left'>value</th><th style='text-align:left'>standard error</th><th style='text-align:left'>relative error</th><th style='text-align:left'>initial value</th><th style='text-align:left'>min</th><th style='text-align:left'>max</th><th style='text-align:right'>vary</th></tr><tr><td style='text-align:left'>bottom</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'></td><td style='text-align:left'>0</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>False</td></tr><tr><td style='text-align:left'>top</td><td style='text-align:left'> 104.053054</td><td style='text-align:left'> 2.05947861</td><td style='text-align:left'>(1.98%)</td><td style='text-align:left'>100.0</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>logEC50</td><td style='text-align:left'>-5.63808943</td><td style='text-align:left'> 0.05151637</td><td style='text-align:left'>(0.91%)</td><td style='text-align:left'>-6.0</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>hill_slope</td><td style='text-align:left'> 0.62209892</td><td style='text-align:left'> 0.03579809</td><td style='text-align:left'>(5.75%)</td><td style='text-align:left'>1.0</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr></table><table class="jp-toc-ignore"><caption>Correlations (unreported values are < 0.100)</caption><tr><th style='text-align:left'>Parameter1</th><th style='text-align:left'>Parameter 2</th><th style='text-align:right'>Correlation</th></tr><tr><td style='text-align:left'>top</td><td style='text-align:left'>logEC50</td><td style='text-align:right'>+0.7796</td></tr><tr><td style='text-align:left'>top</td><td style='text-align:left'>hill_slope</td><td style='text-align:right'>-0.6955</td></tr><tr><td style='text-align:left'>logEC50</td><td style='text-align:left'>hill_slope</td><td style='text-align:right'>-0.5410</td></tr></table></div></div>
</div>
</section>
<section id="predicting-values">
<h3>Predicting values<a class="headerlink" href="#predicting-values" title="Link to this heading">#</a></h3>
<p>When using <code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit</span></code>, we can use the best-fit parameters obtained from the fit, along with the original equation (defined as a Python function), to predict values.</p>
<p>For example, let’s say we want to predict the muscle relaxation response for a ‘log[Norepinephrine]’ value of -6.0. We can use the <code class="docutils literal notranslate"><span class="pre">hill_equation_bottom_zero</span></code> function and the ‘best_vals’ obtained from <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code> to calculate this prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict the y-value for a new x-value using curve_fit results</span>
<span class="n">new_x</span> <span class="o">=</span> <span class="o">-</span><span class="mf">6.0</span>  <span class="c1"># Example: log[Norepinephrine] = -6.0</span>
<span class="n">predicted_y</span> <span class="o">=</span> <span class="n">hill_equation_bottom_zero</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="o">*</span><span class="n">best_vals</span><span class="p">)</span>  <span class="c1"># Use * to unpack parameter values</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted y-value for x = </span><span class="si">{</span><span class="n">new_x</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">predicted_y</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted y-value for x = -6.0: 38.84%
</pre></div>
</div>
</div>
</div>
<p>In this code, the asterisk (<code class="docutils literal notranslate"><span class="pre">*</span></code>) before ‘best_vals’ is used to unpack the elements of the array and pass them as individual arguments to the <code class="docutils literal notranslate"><span class="pre">hill_equation_bottom_zero</span></code> function. This is necessary because the function expects the parameter values as separate arguments, not as a single array.</p>
<p>Similarly, we can use two asterisks (<code class="docutils literal notranslate"><span class="pre">**</span></code>) to unpack the ‘result.params’ dictionary with <code class="docutils literal notranslate"><span class="pre">lmfit</span></code>. This passes the key-value pairs of the dictionary as keyword arguments to the <code class="docutils literal notranslate"><span class="pre">hill_equation</span></code> function. This is necessary because the function expects the parameter values as separate keyword arguments with their corresponding names, not as a single dictionary. Note that we use the original <code class="docutils literal notranslate"><span class="pre">hill_equation</span></code> function here because the ‘result.params’ dictionary includes the ‘bottom’ parameter, which is not present in the <code class="docutils literal notranslate"><span class="pre">hill_equation_bottom_zero</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict the y-value for a new x-value using lmfit best-fit paramters</span>
<span class="n">new_x</span> <span class="o">=</span> <span class="o">-</span><span class="mf">6.0</span>  <span class="c1"># Example: log[Norepinephrine] = -6.0</span>
<span class="n">predicted_y</span> <span class="o">=</span> <span class="n">hill_equation</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="o">**</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>  <span class="c1"># Use ** to unpack parameter values</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted y-value for x = </span><span class="si">{</span><span class="n">new_x</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">predicted_y</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted y-value for x = -6.0: 38.84%
</pre></div>
</div>
</div>
</div>
<p>With <code class="docutils literal notranslate"><span class="pre">lmfit</span></code>, we can use the <a class="reference external" href="https://lmfit.github.io/lmfit-py/model.html#lmfit.model.Model.eval"><code class="docutils literal notranslate"><span class="pre">eval()</span></code> method</a> of the ‘result’ object to predict values. This method takes the independent variable (<code class="docutils literal notranslate"><span class="pre">x</span></code>) as an argument, along with any parameter values we want to specify.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict the y-value for a new x-value using lmfit result</span>
<span class="n">new_x</span> <span class="o">=</span> <span class="o">-</span><span class="mf">6.0</span>  <span class="c1"># Example: log[Norepinephrine] = -6.0</span>
<span class="n">predicted_y</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">new_x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted y-value for x = </span><span class="si">{</span><span class="n">new_x</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">predicted_y</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted y-value for x = -6.0: 38.84%
</pre></div>
</div>
</div>
</div>
<p>If we don’t specify a parameter value in <code class="docutils literal notranslate"><span class="pre">result.eval()</span></code>, it uses the optimized value from the fit. Interestingly, it also appears to use the independent variable from the fitting data, even if we don’t explicitly provide it as an argument.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict the y-values</span>
<span class="n">result</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([  3.41522451,   6.75770945,  12.95052424,  23.45062827,
        38.83501989,  57.15534876,  74.275411  ,  87.00942425,
        94.96425495,  99.40465299, 101.7287595 ])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="assessing-the-goodness-of-fit">
<h2>Assessing the goodness of fit<a class="headerlink" href="#assessing-the-goodness-of-fit" title="Link to this heading">#</a></h2>
<p>We’ve now explored how to use both <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code> and <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> to fit nonlinear models to our data. But how do we know if the model we’ve chosen is actually a <em>good</em> fit? This is where assessing the <strong>goodness of fit</strong> comes in.</p>
<p>Goodness of fit refers to how well our model describes the observed data. A good fit means that the model’s predictions closely match the actual data points, while a poor fit indicates that the model doesn’t accurately capture the relationship between the variables.</p>
<p>Why is assessing goodness of fit important?</p>
<ul class="simple">
<li><p>Confidence in conclusions: a good fit gives us more confidence that our model accurately reflects the underlying biological process and that our conclusions based on the model are reliable</p></li>
<li><p>Model selection: if we’re comparing different models, goodness of fit can help us choose the model that best describes the data.</p></li>
<li><p>Identifying potential issues: assessing goodness of fit can help us identify potential problems with our model, such as:</p>
<ul>
<li><p>Model misspecification: the chosen model might not be the right one for the data</p></li>
<li><p>Outliers: there might be unusual data points that are influencing the fit</p></li>
<li><p>Non-constant variance: the variability of the data might not be constant across the range of the independent variable</p></li>
</ul>
</li>
</ul>
<section id="residual-analysis-in-nonlinear-regression">
<h3>Residual analysis in nonlinear regression<a class="headerlink" href="#residual-analysis-in-nonlinear-regression" title="Link to this heading">#</a></h3>
<p>Just like in linear regression, one of the most effective ways to assess the goodness of fit of a nonlinear regression model is to examine the <strong>residuals</strong>. Remember that residuals are the differences between the observed data points and the values predicted by the model. They represent the “unexplained” variation in the data that the model doesn’t account for.</p>
<p>By analyzing the residuals, we can gain valuable insights into whether our model is a good fit for the data and identify potential areas for improvement.</p>
<section id="plotting-residuals-vs-predicted-values">
<h4>Plotting residuals vs. predicted values<a class="headerlink" href="#plotting-residuals-vs-predicted-values" title="Link to this heading">#</a></h4>
<p>A common and informative way to visualize <strong>residuals</strong> is to create a scatter plot of residuals on the y-axis against the corresponding <strong>predicted</strong> values on the x-axis. This type of plot can reveal patterns in the residuals that might indicate problems with the model, just as we saw in the context of linear regression.</p>
<p>To obtain the predicted values, we can use the <code class="docutils literal notranslate"><span class="pre">result.best_fit</span></code> attribute of the ‘result’ object. This attribute holds the values of the relaxation response that our model predicts for each norepinephrine concentration used in the experiment. Alternatively, the ‘result’ object also provides the residuals directly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the residuals</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">best_fit</span> <span class="o">-</span> <span class="n">relaxation</span>  <span class="c1"># result.best_fit holds the predicted values from the fitted model</span>
<span class="c1"># residuals = result.residual  # Same as above</span>
</pre></div>
</div>
</div>
</div>
<p>Note that the sign of the residuals might be reversed depending on whether we calculate them as <em>(data - model)</em> or <em>(model - data)</em>. However, the absolute distances of the residuals from the horizontal line, and therefore the sum of squares, will be identical.</p>
<p>To create a residuals vs. predicted values plot, we can either extract the predicted values from the model (<code class="docutils literal notranslate"><span class="pre">result.best_fit</span></code>) and plot them against the residuals we calculate manually, or we can use the convenient <code class="docutils literal notranslate"><span class="pre">result.plot_residuals()</span></code> method provided by <code class="docutils literal notranslate"><span class="pre">lmfit</span></code>. By default, this method uses the independent variable on the x-axis, but we can modify this behavior by providing the <code class="docutils literal notranslate"><span class="pre">x=</span></code> parameter with the predicted values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the residuals vs. predicted values plot manually</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># Create figure and axes objects</span>

<span class="c1"># Manual plot on the left subplot</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">best_fit</span><span class="p">,</span> <span class="n">residuals</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted values&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Residuals&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Residuals (Manual)&quot;</span><span class="p">)</span>

<span class="c1"># lmfit plot on the right subplot</span>
<span class="n">result</span><span class="o">.</span><span class="n">plot_residuals</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Provide axes object</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Independent variable&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Residuals (lmfit)&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7e7f031aab7c667ebbcf67c97e89e4914f24becde878102442202b22b5b63a35.png" src="_images/7e7f031aab7c667ebbcf67c97e89e4914f24becde878102442202b22b5b63a35.png" />
</div>
</div>
</section>
<section id="examining-patterns-in-residuals-to-identify-potential-issues">
<h4>Examining Patterns in Residuals to Identify Potential Issues<a class="headerlink" href="#examining-patterns-in-residuals-to-identify-potential-issues" title="Link to this heading">#</a></h4>
<p>Ideally, in a well-fit model, the residuals should be randomly scattered around the horizontal line at y=0, with no discernible patterns. If we observe any systematic patterns in the residuals, it can indicate problems with our model. Here are some common patterns to watch out for and what they might suggest:</p>
<ul class="simple">
<li><p><em>Curvature</em>: if the residuals show a curved pattern, it might suggest that the model is misspecified. This means that the functional form we’ve chosen (in our case, the Hill equation) doesn’t accurately capture the true relationship between the variables. We might need to consider a different type of nonlinear function or add more terms to our existing model to better capture the curvature.</p></li>
<li><p>Non-constant variance (<em>heteroscedasticity</em>): if the spread of the residuals increases or decreases systematically across the range of predicted values, it indicates non-constant variance. This violates one of the assumptions of nonlinear regression, which is that the variability of the data should be constant across the range of the independent variable. Non-constant variance can affect the reliability of our parameter estimates and confidence intervals. We might need to transform the data or use weighted regression techniques to address this issue.</p></li>
<li><p><em>Outliers</em>: individual points that lie far from the general pattern of residuals might be outliers. These points can have a disproportionate influence on the fit and can distort the parameter estimates. It’s important to investigate outliers to determine if they are due to data entry errors, measurement problems, or truly unusual observations. If we identify valid outliers, we might need to consider robust regression techniques that are less sensitive to outliers.</p></li>
<li><p><em>Autocorrelation</em>: in some cases, especially with time-series data, the residuals might show autocorrelation, meaning that adjacent residuals are correlated with each other. This violates the assumption of independence of errors and can affect the validity of our statistical inferences. We might need to use specialized time-series models or account for the autocorrelation in our analysis.</p></li>
</ul>
<p>By carefully examining the residuals vs. predicted values plot and looking for these patterns, we can gain valuable insights into the adequacy of our nonlinear regression model and identify potential areas for improvement. It’s important to remember that residual analysis is an iterative process. If we identify issues, we might need to adjust our model, transform the data, or use different analysis techniques to achieve a better fit.</p>
</section>
</section>
<section id="calculating-r-squared">
<h3>Calculating R-squared<a class="headerlink" href="#calculating-r-squared" title="Link to this heading">#</a></h3>
<p>In linear regression, we often use the <strong>R² (R-squared)</strong> value as a measure of goodness of fit. It represents the proportion of variance in the dependent variable that’s explained by the model. However, in nonlinear regression, R-squared has some limitations and should be interpreted with caution.</p>
<p>While there isn’t a single universally accepted definition of R² for nonlinear models, one common approach is to calculate it as:</p>
<div class="math notranslate nohighlight">
\[R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{RSS}\)</span> is the sum of squared errors (residuals), and <span class="math notranslate nohighlight">\(\text{TSS}\)</span> is the total sum of squares, calculated as the sum of squared differences between the observed data points and the mean of the dependent variable. Notice that the RSS calculated here should be the same as the ‘result.chisqr’ value provided by <code class="docutils literal notranslate"><span class="pre">lmfit</span></code>. While both refer to the sum of squared residuals, it’s important to note that this differs from the general chi-squared statistic we saw in the previous section, which incorporates weights based on the uncertainties of the data points.</p>
<p>This formula is analogous to the one used in linear regression. In fact, we can use the <code class="docutils literal notranslate"><span class="pre">compute_rss()</span></code> function we defined in the previous chapter on linear regression to calculate the RSS for our nonlinear model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define functions to compute RSS and estimate y</span>
<span class="k">def</span> <span class="nf">compute_rss</span><span class="p">(</span><span class="n">y_estimate</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_estimate</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Calculate RSS using the compute_rss() function from the previous chapter</span>
<span class="n">rss</span> <span class="o">=</span> <span class="n">compute_rss</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">best_fit</span><span class="p">,</span> <span class="n">relaxation</span><span class="p">)</span>

<span class="c1"># Calculate TSS</span>
<span class="n">tss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">relaxation</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">relaxation</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Calculate and print R-squared</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TSS (manual):</span><span class="se">\t</span><span class="si">{</span><span class="n">tss</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RSS (manual):</span><span class="se">\t</span><span class="si">{</span><span class="n">rss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;χ² (lmfit):</span><span class="se">\t</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">chisqr</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">24</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R² (manuel):</span><span class="se">\t</span><span class="si">{</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">rss</span><span class="o">/</span><span class="n">tss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R² (lmfit):</span><span class="se">\t</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">rsquared</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TSS (manual):	14917.02
RSS (manual):	42.95705
χ² (lmfit):	42.95705
------------------------
R² (manuel):	0.9971
R² (lmfit):	0.9971
</pre></div>
</div>
</div>
</div>
<p>It’s important to be aware of the limitations of R² in the nonlinear context:</p>
<ul class="simple">
<li><p>Not always interpretable as proportion of variance explained: in nonlinear models, R² doesn’t always strictly represent the proportion of variance explained. This is because the relationship between the independent and dependent variables might not be linear, and the concept of “explained variance” becomes less clear.</p></li>
<li><p>Can be artificially high: in some cases, R² can be artificially high for nonlinear models, especially if the model is overfitting the data. This means that the model is fitting the noise in the data rather than the true underlying relationship.</p></li>
<li><p>Not always comparable across models: R² values might not be directly comparable across different nonlinear models, especially if the models have different numbers of parameters or different functional forms.</p></li>
</ul>
<p>While R² can provide a general indication of how well a nonlinear model fits the data, it’s important to interpret it cautiously and consider its limitations. It’s generally more informative to focus on other aspects of goodness of fit, such as residual analysis, visual inspection of the fitted curve, and comparison to alternative models.</p>
</section>
</section>
<section id="visualizing-and-interpreting-non-linear-models">
<h2>Visualizing and interpreting non-linear models<a class="headerlink" href="#visualizing-and-interpreting-non-linear-models" title="Link to this heading">#</a></h2>
<p>Now that we’ve explored how to fit nonlinear models and assess their goodness of fit, let’s turn our attention to visualizing and interpreting the results. Visualizations are crucial for understanding the relationship between the variables, assessing the model’s fit, and communicating our findings effectively.</p>
<section id="plotting-the-optimal-curve">
<h3>Plotting the optimal curve<a class="headerlink" href="#plotting-the-optimal-curve" title="Link to this heading">#</a></h3>
<p>One of the first steps in visualizing a nonlinear model is to plot the optimal curve alongside the original data points. This allows us to see how well the model captures the overall trend in the data and identify any areas of discrepancy.</p>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">hill_equation_bottom_zero</span></code> function and the best-fit parameters from SciPy optimized ‘best_vals’ to calculate the predicted values for each point in a defined range of x-values. We can create a similar plot using the best-fit values from <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> and the <code class="docutils literal notranslate"><span class="pre">hill_equation</span></code> function, as the ‘result.params’ dictionary contains the fixed ‘bottom’ value as well. Finally, <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> provides a convenient method called result.plot_fit()` that allows us to directly visualize the fitted curve alongside the data. This simplifies the plotting process and provides a quick way to assess the model’s fit visually.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a figure with three subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Generate x-values for the curve in the range of norepi_log (log scale)</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">norepi_log</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">norepi_log</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Plot 1: Fitted `hill_equation_bottom_zero` (SciPy parameters)</span>
<span class="c1"># Calculate predicted values using the unpack (*) best-fit parameters array</span>
<span class="n">predicted_values</span> <span class="o">=</span> <span class="n">hill_equation_bottom_zero</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="o">*</span><span class="n">best_vals</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">norepi_lin</span><span class="p">,</span> <span class="n">relaxation</span><span class="p">,</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data&quot;</span><span class="p">)</span>  <span class="c1"># Plot the data</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="n">x_range</span><span class="p">,</span> <span class="n">predicted_values</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Fitted curve&quot;</span><span class="p">)</span>  <span class="c1"># Plot the fitted curve</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;[Norepinephrine, M]&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;</span><span class="si">% r</span><span class="s2">elaxation&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Fitted `hill_equation_bottom_zero`</span><span class="se">\n</span><span class="s2">(SciPy parameters)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot 2: Fitted `hill_equation` (lmfit parameters)</span>
<span class="c1"># Calculate predicted values using the unpack (**) best-fit parameters dictionnary</span>
<span class="n">predicted_values</span> <span class="o">=</span> <span class="n">hill_equation</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="o">**</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

<span class="c1"># Plot the fitted curve and the data</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">norepi_lin</span><span class="p">,</span> <span class="n">relaxation</span><span class="p">,</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data&quot;</span><span class="p">)</span>  <span class="c1"># Plot the data</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="n">x_range</span><span class="p">,</span> <span class="n">predicted_values</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Fitted curve&quot;</span><span class="p">)</span>  <span class="c1"># Plot the fitted curve</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;[Norepinephrine, M]&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;</span><span class="si">% r</span><span class="s2">elaxation&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Fitted `hill_equation`</span><span class="se">\n</span><span class="s2">(lmfit parameters)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot 3: Fitted `hill_equation`\n(`result.plot_fit`)</span>
<span class="n">result</span><span class="o">.</span><span class="n">plot_fit</span><span class="p">(</span>
    <span class="c1"># datafmt=&#39;ko&#39;,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Log[Norepinephrine, M]&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;</span><span class="si">% r</span><span class="s2">elaxation&quot;</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Fitted `hill_equation`</span><span class="se">\n</span><span class="s2">(`result.plot_fit`)&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="c1"># axes[2].set_xlabel(&quot;Log[Norepinephrine, M]&quot;)</span>
<span class="c1"># axes[2].set_ylabel(&quot;% relaxation&quot;)</span>
<span class="c1"># axes[2].set_title(&quot;Fitted `hill_equation`\n(`result.plot_fit`)&quot;)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1598ca5b2ca439267f633edc08a7670f56b8296d102baf7ef94ce7babb98c136.png" src="_images/1598ca5b2ca439267f633edc08a7670f56b8296d102baf7ef94ce7babb98c136.png" />
</div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> provides a convenient way to get a quick overview of the fit, including both the fitted curve and the residuals, using the <code class="docutils literal notranslate"><span class="pre">result.plot()</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">datafmt</span><span class="o">=</span><span class="s1">&#39;ko&#39;</span><span class="p">,</span>
    <span class="n">fitfmt</span><span class="o">=</span><span class="s1">&#39;r-&#39;</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Log[Norepinephrine, M]&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;</span><span class="si">% r</span><span class="s2">elaxation&quot;</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Fitted hill_equation (result.plot)&quot;</span><span class="p">,</span>
    <span class="n">data_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;ms&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span>
    <span class="n">fit_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">},</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0a37472f73b0ab4ede30dfb97521a8a85a2072db8cb258f87e493a43fd18b702.png" src="_images/0a37472f73b0ab4ede30dfb97521a8a85a2072db8cb258f87e493a43fd18b702.png" />
</div>
</div>
</section>
<section id="generating-confidence-intervals">
<h3>Generating confidence intervals<a class="headerlink" href="#generating-confidence-intervals" title="Link to this heading">#</a></h3>
<p>In addition to visualizing the best-fit curve, it’s often helpful to generate <strong>confidence intervals</strong> for the curve. These intervals provide a visual representation of the uncertainty in our model’s predictions. They show us the range of plausible values for the response variable at different values of the independent variable.</p>
<p>As we discussed earlier, <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> provides the standard errors of the estimated parameters. We can use these standard errors, along with the best-fit values, to construct <strong>asymptotic confidence intervals</strong> for the fitted curve. The general formula for calculating confidence intervals is:</p>
<div class="math notranslate nohighlight">
\[\hat \theta \pm t \times s_{\hat \theta}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat \theta\)</span> is the best-fit value for the parameter</p></li>
<li><p><span class="math notranslate nohighlight">\(s_{\hat \theta}\)</span> is the standard error of the parameter</p></li>
<li><p><span class="math notranslate nohighlight">\(t\)</span> is the critical value from the t-distribution for the desired level of confidence (e.g., 95%) and the number of degrees of freedom (which equals the number of data points minus the number of parameters fit by the regression).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">t</span> <span class="k">as</span> <span class="n">t_dist</span>

<span class="c1"># Calculate degrees of freedom</span>
<span class="n">df</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">norepi_log</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">best_vals</span><span class="p">)</span>

<span class="c1"># Calculate the critical t-value for a 95% confidence interval</span>
<span class="n">t_critical</span> <span class="o">=</span> <span class="n">t_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.95</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>

<span class="c1"># Function to calculate confidence intervals</span>
<span class="k">def</span> <span class="nf">get_ci</span><span class="p">(</span><span class="n">estimate</span><span class="p">,</span> <span class="n">se</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">estimate</span> <span class="o">-</span> <span class="n">t_critical</span> <span class="o">*</span> <span class="n">se</span><span class="p">,</span> <span class="n">estimate</span> <span class="o">+</span> <span class="n">t_critical</span> <span class="o">*</span> <span class="n">se</span><span class="p">)</span>

<span class="c1"># Print the 95% confidence intervals of the parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;95% CI of parameters:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---------------------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;df = </span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s1">&#39;Top&#39;</span><span class="p">,</span> <span class="s1">&#39;LogEC50&#39;</span><span class="p">,</span> <span class="s1">&#39;HillSlope&#39;</span><span class="p">]):</span>
    <span class="n">ci_low</span><span class="p">,</span> <span class="n">ci_high</span> <span class="o">=</span> <span class="n">get_ci</span><span class="p">(</span><span class="n">best_vals</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">standard_errors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">ci_low</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">ci_high</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Special case for EC50 (convert to linear scale)</span>
    <span class="k">if</span> <span class="n">param_name</span> <span class="o">==</span> <span class="s1">&#39;LogEC50&#39;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;EC50 = </span><span class="si">{</span><span class="mi">10</span><span class="o">**</span><span class="n">ci_low</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="mi">10</span><span class="o">**</span><span class="n">ci_high</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>95% CI of parameters:
---------------------
df = 8
Top = 99.30 to 108.80
LogEC50 = -5.76 to -5.52
EC50 = 1.75e-06 to 3.02e-06
HillSlope = 0.54 to 0.70
</pre></div>
</div>
</div>
</div>
<p>Additionally, the <code class="docutils literal notranslate"><span class="pre">result.ci_report()</span></code> method can be used to generate a report that explicitly shows the confidence intervals for each parameter. We can also mention the optional arguments <code class="docutils literal notranslate"><span class="pre">with_offset,</span> </code>ndigits<code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">and</span> </code>sigmas` that allow to customize the desired confidence levelt and format of the report.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the confidence interval report</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">ci_report</span><span class="p">(</span><span class="n">with_offset</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ndigits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sigmas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.95</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>             95.00%  _BEST_  95.00%
 top       :  99.660 104.053 109.115
 logEC50   :  -5.749  -5.638  -5.516
 hill_slope:   0.544   0.622   0.713
</pre></div>
</div>
</div>
</div>
<p>Notice that the confidence intervals reported by <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> are slightly different from the ones we calculated manually using the t-distribution, i.e., the asymptotic confidence intervals. This is because <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> uses <a class="reference external" href="https://lmfit.github.io/lmfit-py/confidence.html"><strong>profile likelihood confidence intervals</strong></a>, a more sophisticated method that takes into account the nonlinearity of the model and the correlations between the parameters. This generally provides more accurate and realistic confidence intervals, especially for nonlinear models.</p>
<p>However, directly applying this formula to the parameters and then generating the curve with the lower and upper confidence values of each parameter might not accurately reflect the uncertainty in the fitted curve, especially for nonlinear models. This is because:</p>
<ul class="simple">
<li><p><em>Nonlinear relationships</em>: in nonlinear models, the relationship between the parameters and the predicted values can be complex and non-intuitive.  Changing one parameter might affect the curve in a way that’s not simply captured by adding or subtracting a constant value.</p></li>
<li><p>Parameter interdependence: the parameters in a nonlinear model are often interdependent. This means that changing one parameter can affect the optimal values of other parameters. Simply applying the formula to each parameter independently doesn’t account for these interdependencies.</p></li>
<li><p>Asymmetric confidence intervals: in some cases, the confidence intervals around a parameter might be asymmetric, especially in nonlinear models. This means that the uncertainty might be greater in one direction than the other. Directly applying the formula assumes symmetric intervals, which might not be accurate.</p></li>
</ul>
<p>A simplified approach to drawing a confidence band is to generate predictions using the <code class="docutils literal notranslate"><span class="pre">hill_equation_bottom_zero</span></code> function, varying only one parameter at a time between its lower and upper confidence limits while holding the other parameters at their best-fit values. We can repeat this process for each of the ‘top’, ‘logEC50’, and ‘hill_slope’ parameters (recall that the ‘bottom’ parameter is fixed at zero). However, this simplification doesn’t account for the uncertainties and interdependencies of the parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a figure with three subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Define a function to plot the confidence band for a given parameter</span>
<span class="k">def</span> <span class="nf">plot_confidence_band</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param_index</span><span class="p">):</span>
    <span class="c1"># Calculate the lower and upper confidence bounds for the curve</span>
    <span class="n">params_low</span> <span class="o">=</span> <span class="n">best_vals</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">params_low</span><span class="p">[</span><span class="n">param_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_vals</span><span class="p">[</span><span class="n">param_index</span><span class="p">]</span> <span class="o">-</span> <span class="n">t_critical</span> <span class="o">*</span> <span class="n">standard_errors</span><span class="p">[</span><span class="n">param_index</span><span class="p">]</span>
    <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">hill_equation_bottom_zero</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="o">*</span><span class="n">params_low</span><span class="p">)</span>

    <span class="n">params_high</span> <span class="o">=</span> <span class="n">best_vals</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">params_high</span><span class="p">[</span><span class="n">param_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_vals</span><span class="p">[</span><span class="n">param_index</span><span class="p">]</span> <span class="o">+</span> <span class="n">t_critical</span> <span class="o">*</span> <span class="n">standard_errors</span><span class="p">[</span><span class="n">param_index</span><span class="p">]</span>
    <span class="n">upper_bound</span> <span class="o">=</span> <span class="n">hill_equation_bottom_zero</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="o">*</span><span class="n">params_high</span><span class="p">)</span>

    <span class="c1"># Plot the fitted curve, confidence band, and the data</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">norepi_lin</span><span class="p">,</span> <span class="n">relaxation</span><span class="p">,</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="n">x_range</span><span class="p">,</span> <span class="n">predicted_values</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Fitted curve&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="mi">10</span><span class="o">**</span><span class="n">x_range</span><span class="p">,</span>
        <span class="n">lower_bound</span><span class="p">,</span>
        <span class="n">upper_bound</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">.25</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Confidence band&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;[Norepinephrine, M] (log scale)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;</span><span class="si">% r</span><span class="s2">elaxation&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Varying </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot the confidence bands for each parameter</span>
<span class="n">plot_confidence_band</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">plot_confidence_band</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;logEC50&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_confidence_band</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;hill_slope&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/153d7aee91f4f2316eb7cdc07a5d20b3d76a2901fe7af97af28443a458017cfb.png" src="_images/153d7aee91f4f2316eb7cdc07a5d20b3d76a2901fe7af97af28443a458017cfb.png" />
</div>
</div>
<p>Instead of directly applying the confidence interval formula to individual parameters, <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> offers a more <em>robust approach</em> for generating confidence intervals that accurately reflect the uncertainty in the fitted curve. This is achieved using the <code class="docutils literal notranslate"><span class="pre">eval_uncertainty()</span></code> method.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">eval_uncertainty()</span></code> method calculates the uncertainty in the predicted values at each point along the curve, taking into account the <em>full covariance matrix</em> and the model’s nonlinearity. It applies error propagation techniques to estimate how the uncertainties in the parameters propagate through the nonlinear model function to affect the predicted values.</p>
<p>We can easily change the <code class="docutils literal notranslate"><span class="pre">sigma</span></code> value to visualize confidence bands corresponding to different confidence levels. For example, <code class="docutils literal notranslate"><span class="pre">sigma=1</span></code> corresponds to a 68% confidence interval (1 standard deviationn, approximately), while <code class="docutils literal notranslate"><span class="pre">sigma=1.96</span></code> corresponds to a 95% confidence interval.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the uncertainty in the predicted values</span>
<span class="n">dely</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">eval_uncertainty</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mf">1.96</span><span class="p">)</span> <span class="c1"># type: ignore</span>

<span class="c1"># Plot the fitted curve, confidence band, and the data</span>
<span class="n">result</span><span class="o">.</span><span class="n">plot_fit</span><span class="p">(</span>
    <span class="c1"># datafmt=&#39;ko&#39;,</span>
    <span class="n">numpoints</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">fit_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">norepi_log</span><span class="p">,</span>
    <span class="n">result</span><span class="o">.</span><span class="n">best_fit</span> <span class="o">-</span> <span class="n">dely</span><span class="p">,</span>
    <span class="n">result</span><span class="o">.</span><span class="n">best_fit</span> <span class="o">+</span> <span class="n">dely</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;greenyellow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">.75</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Confidence band (1.96 σ)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Log[Norepinephrine, M]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Relaxation (%)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Fitted hill_equation with confidence band (result.plot_fit)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6fda1a65ff24bcad782d4649c08cb9801f390071c135962192984835880ec460.png" src="_images/6fda1a65ff24bcad782d4649c08cb9801f390071c135962192984835880ec460.png" />
</div>
</div>
</section>
<section id="exploring-the-parameter-space-visually">
<h3>Exploring the parameter space visually<a class="headerlink" href="#exploring-the-parameter-space-visually" title="Link to this heading">#</a></h3>
<p>While the previous plots have shown us the overall fit of the model and its uncertainty, we can gain a deeper understanding by visually exploring the <strong>parameter space</strong>. This means visualizing how changes in the parameter values affect the shape and position of the fitted curve.</p>
<p>To explore the parameter space, we create plots where we vary one parameter at a time while holding the others constant. This will allow us to see how each parameter influences the curve and gain a more intuitive understanding of their roles in the model.</p>
<p>For example, we can plot the following curves on the same figure:</p>
<ul class="simple">
<li><p><em>Initial guess</em>: the curve generated using the initial parameter guesses we provided to <code class="docutils literal notranslate"><span class="pre">lmfit</span></code></p></li>
<li><p><em>Optimized parameters</em>: the curve generated using the optimized parameter values obtained from the fit</p></li>
<li><p><em>Modified parameters</em>: a curve generated with a specific parameter modified from its optimal value, e.g., a lower ‘logEC50’ or <em>an one unit Hill slope</em> (3PL equation), while keeping the other parameters at their optimal values. This allows us to isolate the effect of changing that specific parameter</p></li>
</ul>
<p>This visualization will help us see how the initial guess compares to the optimal fit and how individual parameters influence the shape and position of the curve.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a figure with three subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Plot 1: Original data and initial guess</span>
<span class="n">result</span><span class="o">.</span><span class="n">plot_fit</span><span class="p">(</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>  <span class="c1"># Plot on the first subplot</span>
    <span class="n">datafmt</span><span class="o">=</span><span class="s1">&#39;ko&#39;</span><span class="p">,</span>
    <span class="n">fitfmt</span><span class="o">=</span><span class="s1">&#39;r-&#39;</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Log[Norepinephrine, M]&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;</span><span class="si">% r</span><span class="s2">elaxation&quot;</span><span class="p">,</span>
    <span class="n">show_init</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">fit_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
    <span class="n">init_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="s1">&#39;grey&#39;</span><span class="p">})</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Original data and initial fit&quot;</span><span class="p">)</span>

<span class="c1"># Plot 2: Lower EC50</span>
<span class="n">result</span><span class="o">.</span><span class="n">plot_fit</span><span class="p">(</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Plot on the second subplot</span>
    <span class="n">datafmt</span><span class="o">=</span><span class="s1">&#39;ko&#39;</span><span class="p">,</span>
    <span class="n">fitfmt</span><span class="o">=</span><span class="s1">&#39;r-&#39;</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Log[Norepinephrine, M]&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;</span><span class="si">% r</span><span class="s2">elaxation&quot;</span><span class="p">,</span>
    <span class="n">show_init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Don&#39;t show the initial fit</span>
    <span class="n">fit_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">norepi_log</span><span class="p">,</span>
    <span class="n">result</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span>
        <span class="n">top</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
        <span class="n">logEC50</span><span class="o">=-</span><span class="mf">6.8</span><span class="p">,</span>
        <span class="n">hill_slope</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;hill_slope&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;lower $\rm EC_</span><span class="si">{50}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Lower EC50&quot;</span><span class="p">)</span>

<span class="c1"># Plot 3: Hill slope = 1</span>
<span class="n">result</span><span class="o">.</span><span class="n">plot_fit</span><span class="p">(</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>  <span class="c1"># Plot on the third subplot</span>
    <span class="n">datafmt</span><span class="o">=</span><span class="s1">&#39;ko&#39;</span><span class="p">,</span>
    <span class="n">fitfmt</span><span class="o">=</span><span class="s1">&#39;r-&#39;</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Log[Norepinephrine, M]&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;</span><span class="si">% r</span><span class="s2">elaxation&quot;</span><span class="p">,</span>
    <span class="n">show_init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Don&#39;t show the initial fit</span>
    <span class="n">fit_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;lw&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">norepi_log</span><span class="p">,</span>
    <span class="n">result</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span>
        <span class="n">top</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
        <span class="n">logEC50</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;logEC50&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
        <span class="n">hill_slope</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># It&#39;s finally a 3PL equation</span>
    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;hill_slope=1&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Hill slope of 1 (3PL)&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/de4f780d3609fc6f6583b19c964066cd245fc72661ea4a70d3ace82954dddb54.png" src="_images/de4f780d3609fc6f6583b19c964066cd245fc72661ea4a70d3ace82954dddb54.png" />
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">results.eval</span></code> method is used to evaluate the model function with a specific set of parameter values. It takes the parameter names as keyword arguments and returns the predicted values of the dependent variable.</p>
<p>In essence, this code snippet generates predicted values for the Hill equation with a modified ‘logEC50’ or ‘hill_slope’ values while keeping the other parameters at their optimal values. This allows to see how changing the parameters affects the shape and position of the curve, providing insights into its role in the model.</p>
<p>This technique is valuable for exploring the parameter space and understanding the sensitivity of the model to changes in individual parameters. It helps visualize the relationship between the parameters and the predicted values, aiding in the interpretation of the model’s behavior.</p>
</section>
</section>
<section id="comparing-nonlinear-models">
<h2>Comparing nonlinear models<a class="headerlink" href="#comparing-nonlinear-models" title="Link to this heading">#</a></h2>
<p>In the previous section, we explored how to visualize and interpret individual nonlinear models. We saw how to plot the fitted curve, generate confidence intervals, and explore the parameter space to understand the model’s behavior. But what if we have multiple candidate models that might describe our data? How do we choose the best one? This is where <strong>comparing nonlinear models</strong> becomes essential.</p>
<p>Just as we did in previous chapters with linear models and comparing models in general, we can compare nonlinear models using various statistical criteria. One common approach is to compare the <strong>sum of squares error (SSE)</strong> or the <strong>chi-squared (χ<sup>2</sup>)</strong> values for different models. A model with a lower SSE or χ<sup>2</sup> generally indicates a better fit, as it means the model’s predictions are closer to the observed data.</p>
<p>However, simply comparing the SSE or χ<sup>2</sup> values isn’t always sufficient, especially when comparing models with different numbers of parameters. A model with more parameters might have a lower SSE simply because it has more flexibility to fit the data, even if it doesn’t truly represent the underlying relationship better.</p>
<p>Therefore, we need more sophisticated methods to compare nonlinear models that take into account both the goodness of fit and the model’s complexity. In this section, we’ll explore some of these methods, including the <strong>extra sum-of-squares F test</strong> and <strong>information criteria</strong> like AIC and BIC.</p>
<section id="nested-models">
<h3>Nested models<a class="headerlink" href="#nested-models" title="Link to this heading">#</a></h3>
<p>Before we delve into specific statistical tests for comparing models, let’s revisit the concept of <strong>nested models</strong>. As we discussed in the previous chapter, two models are considered nested if one model (the <strong>reduced model</strong>) can be obtained from the other model (the <strong>full model</strong>) by imposing constraints on the parameters of the full model.</p>
<p>For example, in our muscle relaxation study, we could consider the following nested models:</p>
<ul class="simple">
<li><p><em>Full model</em>: the 4-parameter logistic (4PL) model, represented by the <code class="docutils literal notranslate"><span class="pre">hill_equation</span></code> function.</p></li>
<li><p><em>Reduced model 1</em>: the 4PL model with the ‘bottom’ parameter fixed to 0, represented by the <code class="docutils literal notranslate"><span class="pre">hill_equation_bottom_zero</span></code> function.</p></li>
<li><p><em>Reduced model 2</em>: the 3-parameter logistic (3PL) model, where the Hill slope is fixed to 1, represented by the <code class="docutils literal notranslate"><span class="pre">hill_equation_3pl</span></code> function.</p></li>
<li><p><em>Reduced model 3</em>: the 3PL model with the ‘bottom’ parameter fixed to 0. This is a 2-parameter model.</p></li>
</ul>
<p>The relationships between these models can be visualized as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Full Model (4PL)
   ├── Reduced Model 1 (4PL, bottom = 0)
   └── Reduced Model 2 (3PL, Hill slope = 1)
       └── Reduced Model 3 (3PL, bottom = 0, Hill slope = 1)
</pre></div>
</div>
</section>
<section id="the-extra-sum-of-squares-f-test">
<h3>The extra sum-of-squares F test<a class="headerlink" href="#the-extra-sum-of-squares-f-test" title="Link to this heading">#</a></h3>
<p>The <strong>extra sum-of-squares F test</strong> is a statistical test specifically designed to compare nested models. It assesses whether the additional parameters in the full model significantly improve the fit compared to the reduced model.</p>
<p>It compares the residual sum of squares (RSS) / sum of squared error (SSE) of the two models. It calculates an F-statistic based on the difference in SSE and the degrees of freedom of the two models. This F statistic is then compared to the F-distribution to determine the P value.</p>
<section id="fitting-the-models-to-compare">
<h4>Fitting the models to compare<a class="headerlink" href="#fitting-the-models-to-compare" title="Link to this heading">#</a></h4>
<p>Let’s now compare two versions of our Hill equation model we plotted just before, both with the ‘bottom’ parameter fixed to 0:</p>
<ul class="simple">
<li><p>Variable Hill slope: this is the model we fitted earlier using <code class="docutils literal notranslate"><span class="pre">lmfit</span></code>, where all three parameters (‘top’, ‘logEC50’, and ‘hill_slope’) were allowed to vary.</p></li>
<li><p>Fixed Hill slope (3PL model): this model has the Hill slope fixed to 1, effectively reducing it to a 3-parameter logistic (3PL) model. We will create another function called <code class="docutils literal notranslate"><span class="pre">hill_equation_3pl_bottom_zero</span></code>.</p></li>
</ul>
<p>To compare the models with variable and fixed Hill slopes, we first define separate <code class="docutils literal notranslate"><span class="pre">Model</span></code> objects for each case. This allows us to clearly distinguish between the models and avoid the need for manually fixing parameters using <code class="docutils literal notranslate"><span class="pre">result.eval()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hill_equation_3pl_bottom_zero</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">logEC50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function defines the 3-parameter logistic (3PL) equation with bottom fixed at 0.</span>

<span class="sd">    Args:</span>
<span class="sd">      x: The logarithm of norepinephrine concentration.</span>
<span class="sd">      top: The maximum relaxation (Ymax).</span>
<span class="sd">      logEC50: The logarithm of the EC50.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The predicted relaxation values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">top</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="n">logEC50</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>  <span class="c1"># hill_slope is fixed to 1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a model object for the variable slope model</span>
<span class="n">model_variable_slope</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">hill_equation_bottom_zero</span><span class="p">)</span>

<span class="c1"># Create a model object for the fixed slope (3PL) model</span>
<span class="n">model_fixed_slope</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">hill_equation_3pl_bottom_zero</span><span class="p">)</span>

<span class="c1"># Initial guesses for the parameters</span>
<span class="n">params_variable_slope</span> <span class="o">=</span> <span class="n">model_variable_slope</span><span class="o">.</span><span class="n">make_params</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">logEC50</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">hill_slope</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">params_fixed_slope</span> <span class="o">=</span> <span class="n">model_fixed_slope</span><span class="o">.</span><span class="n">make_params</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">logEC50</span><span class="o">=-</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Fit the models to the data</span>
<span class="n">result_variable_slope</span> <span class="o">=</span> <span class="n">model_variable_slope</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">relaxation</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">norepi_log</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params_variable_slope</span><span class="p">)</span>
<span class="n">result_fixed_slope</span> <span class="o">=</span> <span class="n">model_fixed_slope</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">relaxation</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">norepi_log</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params_fixed_slope</span><span class="p">)</span>

<span class="c1"># Print the reports</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_variable_slope</span><span class="o">.</span><span class="n">fit_report</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">fit_report</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[Model]]
    Model(hill_equation_bottom_zero)
[[Fit Statistics]]
    # fitting method   = leastsq
    # function evals   = 25
    # data points      = 11
    # variables        = 3
    chi-square         = 42.9570499
    reduced chi-square = 5.36963124
    Akaike info crit   = 20.9853605
    Bayesian info crit = 22.1790464
    R-squared          = 0.99712027
[[Variables]]
    top:         104.053046 +/- 2.05949490 (1.98%) (init = 100)
    logEC50:    -5.63808968 +/- 0.05151660 (0.91%) (init = -5)
    hill_slope:  0.62209903 +/- 0.03579791 (5.75%) (init = 0.6)
[[Correlations]] (unreported correlations are &lt; 0.100)
    C(top, logEC50)        = +0.7796
    C(top, hill_slope)     = -0.6955
    C(logEC50, hill_slope) = -0.5410
----------------------------------------
[[Model]]
    Model(hill_equation_3pl_bottom_zero)
[[Fit Statistics]]
    # fitting method   = leastsq
    # function evals   = 37
    # data points      = 11
    # variables        = 2
    chi-square         = 358.098998
    reduced chi-square = 39.7887776
    Akaike info crit   = 42.3120563
    Bayesian info crit = 43.1078468
    R-squared          = 0.97599393
[[Variables]]
    top:      96.1655811 +/- 3.21653274 (3.34%) (init = 100)
    logEC50: -5.75546524 +/- 0.08659489 (1.50%) (init = -5)
[[Correlations]] (unreported correlations are &lt; 0.100)
    C(top, logEC50) = +0.5024
</pre></div>
</div>
</div>
</div>
</section>
<section id="comparing-the-residuals">
<h4>Comparing the residuals<a class="headerlink" href="#comparing-the-residuals" title="Link to this heading">#</a></h4>
<p>We can compare these models by examining their residual plots. This will help us visually assess whether fixing the Hill slope to 1 introduces any systematic patterns in the residuals, which might indicate a poorer fit compared to the more flexible variable slope model. We can already see from the <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> reports that fixing the Hill slope to 1 indeed significantly increases the χ², suggesting that the variable slope model provides a better fit to the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">gridspec</span>

<span class="c1"># Create a GridSpec object to define the layout</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

<span class="c1"># Create the axes using the GridSpec</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax_main</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># Main plot spans both rows in the first column</span>
<span class="n">ax_resid_var</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># Variable slope residuals in the top-right</span>
<span class="n">ax_resid_fix</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># Fixed slope residuals in the bottom-right</span>

<span class="c1"># Plot the curves and data on the main axes</span>
<span class="n">ax_main</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">norepi_log</span><span class="p">,</span> <span class="n">result_variable_slope</span><span class="o">.</span><span class="n">best_fit</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Variable slope&#39;</span><span class="p">)</span>
<span class="n">ax_main</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">norepi_log</span><span class="p">,</span> <span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">best_fit</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fixed slope&#39;</span><span class="p">)</span>
<span class="n">ax_main</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">norepi_log</span><span class="p">,</span> <span class="n">relaxation</span><span class="p">,</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">ax_main</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Comparing the fit of two models&quot;</span><span class="p">)</span>
<span class="n">ax_main</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Log[Norepinephrine, M]&quot;</span><span class="p">)</span>
<span class="n">ax_main</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Relaxation (%)&quot;</span><span class="p">)</span>
<span class="n">ax_main</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax_main</span><span class="p">)</span>

<span class="c1"># Plot the residuals on their respective axes</span>
<span class="n">result_variable_slope</span><span class="o">.</span><span class="n">plot_residuals</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax_resid_var</span><span class="p">)</span>
<span class="n">ax_resid_var</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Residuals&quot;</span><span class="p">)</span>
<span class="n">ax_resid_var</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax_resid_var</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Residuals - Variable Slope&quot;</span><span class="p">)</span>

<span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">plot_residuals</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax_resid_fix</span><span class="p">)</span>
<span class="n">ax_resid_fix</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Residuals&quot;</span><span class="p">)</span>
<span class="n">ax_resid_fix</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax_resid_fix</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Residuals - Fixed Slope&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/36cc3611371fa4f7279007b5ddf78d42df176c495dc546cfc014afc28d56a6e1.png" src="_images/36cc3611371fa4f7279007b5ddf78d42df176c495dc546cfc014afc28d56a6e1.png" />
</div>
</div>
<p>The residual plot for the fixed slope model shows a clear wave pattern, indicating that the model’s predictions systematically deviate from the observed data. This suggests that fixing the Hill slope to 1 introduces a bias, and the model is not flexible enough to capture the true relationship between norepinephrine concentration and muscle relaxation. In contrast, the residual plot for the variable slope model shows no obvious patterns, indicating a better fit to the data.</p>
<p>This observation, combined with the increase in χ² we saw earlier, provides strong evidence that the variable slope model is more appropriate for describing this dataset. It highlights the importance of allowing the Hill slope to vary to capture the nuances of the relationship between the variables.</p>
</section>
<section id="analysis-of-variance">
<h4>Analysis of variance<a class="headerlink" href="#analysis-of-variance" title="Link to this heading">#</a></h4>
<p>Recall from the previous chapter that <strong>analysis of variance (ANOVA)</strong> provides a statistical framework for partitioning the total variation in a dataset into different sources. We can apply this framework to compare nested models by examining how much variation is explained by the additional parameters in the full model compared to the reduced model. To construct the ANOVA table, we extract the sum of squares (χ²) and degrees of freedom from the results of each model fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model Comparison:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;----------------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Model&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Sum of squares&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;DF&#39;</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Fixed slope&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">chisqr</span><span class="si">:</span><span class="s2">15.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">nfree</span><span class="si">:</span><span class="s2">5d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Variable slope&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result_variable_slope</span><span class="o">.</span><span class="n">chisqr</span><span class="si">:</span><span class="s2">15.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result_variable_slope</span><span class="o">.</span><span class="n">nfree</span><span class="si">:</span><span class="s2">5d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model Comparison:
----------------
Model            Sum of squares    DF
Fixed slope               358.1     9
Variable slope             43.0     8
</pre></div>
</div>
</div>
</div>
<p>The <em>null hypothesis</em> in the extra sum-of-squares F test is indeed that the simpler model (with fewer parameters) is adequate. In our case, the simpler model is the one with the fixed Hill slope (3PL model), which has one fewer parameter than the variable slope model (4PL model), since in both models ‘bottom’ was fixed to zero.</p>
<p>Remember that the <strong>degrees of freedom (DF)</strong> represent the number of independent pieces of information available to estimate a particular source of variation. In our case, the degrees of freedom are calculated as follows:</p>
<ul class="simple">
<li><p>Null hypothesis (fixed slope model): this is equal to the number of data points (11 in our example) minus the number of parameters in the fixed slope model (2: ‘top’ and ‘logEC50’). So, <span class="math notranslate nohighlight">\(\rm DF = 11 - 2 = 9\)</span>.</p></li>
<li><p>Alternative hypothesis (variable slope model): this is equal to the number of data points minus the number of parameters in the variable slope model (3: ‘top’, ‘logEC50’, and ‘hill_slope’). So, <span class="math notranslate nohighlight">\(\rm DF = 11 - 3 = 8\)</span>.</p></li>
<li><p>Difference (improvement): this is the difference in degrees of freedom between the two models, which represents the number of additional parameters in the variable slope model compared to the fixed slope model. So, <span class="math notranslate nohighlight">\(\rm DF = 9 - 8 = 1\)</span>.</p></li>
</ul>
<p>We can summarize the sources of variation from the models and the resulting improvement as follows:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Hypothesis</p></th>
<th class="head text-left"><p>Model</p></th>
<th class="head text-left"><p>χ²</p></th>
<th class="head text-left"><p>DF</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Null</p></td>
<td class="text-left"><p>Fixed slope (3PL)</p></td>
<td class="text-left"><p>358.1</p></td>
<td class="text-left"><p>9</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Alternative</p></td>
<td class="text-left"><p>Variable slope (4PL)</p></td>
<td class="text-left"><p>43.0</p></td>
<td class="text-left"><p>8</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Difference</p></td>
<td class="text-left"><p>Improvement</p></td>
<td class="text-left"><p>315.1</p></td>
<td class="text-left"><p>1</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="mean-squares">
<h4>Mean squares<a class="headerlink" href="#mean-squares" title="Link to this heading">#</a></h4>
<p>In the ANOVA table, we also calculate <strong>mean squares (MS)</strong>, which represent the average variation attributed to each source, adjusted for their degrees of freedom. They allow us to compare different sources of variation on a more even playing field.</p>
<p>To calculate the mean square for the <em>difference between the models</em> (MSR), we divide the sum of squares for the difference (SSR) by its degrees of freedom:</p>
<div class="math notranslate nohighlight">
\[\rm MSR = SSR / DF\]</div>
<p>We can also calculate the mean squared error (MSE) for each model individually. This represents the average variability of the residuals for that specific model. The general formula for MSE is:</p>
<div class="math notranslate nohighlight">
\[\text{MSE} = \text{SSE} / (n - p - 1)\]</div>
<p>where <span class="math notranslate nohighlight">\(\rm SSE\)</span> is the sum of squared errors (residuals) for the model, <span class="math notranslate nohighlight">\(n\)</span> is the number of data points, and <span class="math notranslate nohighlight">\(p\)</span> is the number of parameters in the model.</p>
<p>The MSE of the variable slope model represents the variance that remains unexplained after accounting for the additional flexibility of the variable slope. In other words, it’s the variance that the simpler model (fixed slope) would also have to deal with.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate and print the mean squares</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Mean squares:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------&quot;</span><span class="p">)</span>

<span class="c1"># Mean square for the fixed slope model (MSE_fixed)</span>
<span class="n">mse_fixed</span> <span class="o">=</span> <span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">chisqr</span> <span class="o">/</span> <span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">nfree</span> <span class="c1"># type: ignore</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE (fixed slope): </span><span class="si">{</span><span class="n">mse_fixed</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Mean square for the variable slope model (MSE_variable)</span>
<span class="n">mse_variable</span> <span class="o">=</span> <span class="n">result_variable_slope</span><span class="o">.</span><span class="n">chisqr</span> <span class="o">/</span> <span class="n">result_variable_slope</span><span class="o">.</span><span class="n">nfree</span> <span class="c1"># type: ignore</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE (variable slope): </span><span class="si">{</span><span class="n">mse_variable</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Mean square for the difference between the models (MSR)</span>
<span class="n">ms_diff</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">(</span><span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">chisqr</span> <span class="o">-</span> <span class="n">result_variable_slope</span><span class="o">.</span><span class="n">chisqr</span><span class="p">)</span> <span class="c1"># type: ignore</span>
    <span class="o">/</span>
    <span class="p">(</span><span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">nfree</span> <span class="o">-</span> <span class="n">result_variable_slope</span><span class="o">.</span><span class="n">nfree</span><span class="p">)</span> <span class="c1"># type: ignore</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSR (difference): </span><span class="si">{</span><span class="n">ms_diff</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean squares:
------------
MSE (fixed slope): 39.79
MSE (variable slope): 5.37
MSR (difference): 315.14
</pre></div>
</div>
</div>
</div>
</section>
<section id="f-ratio">
<h4>F-ratio<a class="headerlink" href="#f-ratio" title="Link to this heading">#</a></h4>
<p>However, it’s important to note that the MSE calculated in this way is not directly used in the F-test for comparing the models.</p>
<p>The F-test is designed to assess whether the additional complexity of the full model (variable slope) leads to a significant improvement in the fit compared to the reduced model (fixed slope). The term “extra sum of squares” comes from the idea that we’re examining the* extra amount of variation* in the dependent variable that is explained by adding <em>extra terms or parameters</em> to a model.</p>
<p>The F-ratio is essentially a ratio of the variance explained by the extra parameter(s) in the full model (MSR) to the variance that remains unexplained even by the full model (MSE of the variable slope model).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the F-ratio</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">relaxation</span><span class="p">)</span>  <span class="c1"># Number of observations</span>
<span class="n">df_numerator</span> <span class="o">=</span> <span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">nfree</span> <span class="o">-</span> <span class="n">result_variable_slope</span><span class="o">.</span><span class="n">nfree</span>  <span class="c1"># DF for the difference between models (1 in this case)</span>
<span class="n">df_denominator</span> <span class="o">=</span> <span class="n">result_variable_slope</span><span class="o">.</span><span class="n">nfree</span>  <span class="c1"># DF for the variable slope model (the more complex model)</span>
<span class="n">f_value</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">((</span><span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">chisqr</span> <span class="o">-</span> <span class="n">result_variable_slope</span><span class="o">.</span><span class="n">chisqr</span><span class="p">)</span> <span class="o">/</span> <span class="n">df_numerator</span><span class="p">)</span> <span class="c1"># type: ignore</span>
    <span class="o">/</span>
    <span class="p">(</span><span class="n">result_variable_slope</span><span class="o">.</span><span class="n">chisqr</span> <span class="o">/</span> <span class="n">df_denominator</span><span class="p">)</span> <span class="c1"># type: ignore</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F ratio = </span><span class="si">{</span><span class="n">f_value</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>F ratio = 58.690
</pre></div>
</div>
</div>
</div>
</section>
<section id="p-value">
<h4>P value<a class="headerlink" href="#p-value" title="Link to this heading">#</a></h4>
<p>To calculate the P value, we use the <code class="docutils literal notranslate"><span class="pre">scipy.stats.f</span></code> module in Python, which provides functions for working with the F-distribution. Specifically, we use the <code class="docutils literal notranslate"><span class="pre">sf()</span></code> function (survival function) which tells us the probability of observing an F-ratio as extreme as, or more extreme than, the one we calculated, if the null hypothesis were true. This probability is our P value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">f</span>

<span class="c1"># Calculate the P value using the survival function (sf) of the F-distribution</span>
<span class="n">p_value</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">f_value</span><span class="p">,</span> <span class="n">dfn</span><span class="o">=</span><span class="n">df_numerator</span><span class="p">,</span> <span class="n">dfd</span><span class="o">=</span><span class="n">df_denominator</span><span class="p">)</span>  <span class="c1"># Use df_numerator and df_denominator</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P-value = </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P-value = 0.00006
</pre></div>
</div>
</div>
</div>
<p>In our example comparing the fixed slope and variable slope models, with an F-ratio of 58.690 and degrees of freedom of 1 and 8, the P-value is &lt; 0.0001. This means that if the simpler fixed slope model were truly adequate, there would be less than 0.01% chance of observing an F-ratio this large or larger due to random sampling. This extremely small P value provides overwhelming evidence against the null hypothesis. We can therefore conclude that the variable slope model provides a significantly better fit to the data than the fixed slope model.</p>
</section>
<section id="visualizing-the-f-distribution-and-critical-values">
<h4>Visualizing the F-distribution and critical values<a class="headerlink" href="#visualizing-the-f-distribution-and-critical-values" title="Link to this heading">#</a></h4>
<p>To visualize how the P value and critical value are determined, we can consider the F-distribution. This distribution represents the probability of observing different F-ratios under the null hypothesis. The critical value is the F-value that corresponds to our chosen significance level (α). If our calculated F-ratio exceeds this critical value, it falls within the rejection region, leading us to reject the null hypothesis.</p>
<p>The P value represents the probability of observing an F-ratio as extreme as, or more extreme than, the one we calculated, assuming the null hypothesis is true. It’s represented by the area under the F-distribution curve to the right of our calculated F-ratio.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Significance level (alpha)</span>
<span class="n">α</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="c1"># Calculate critical F-value</span>
<span class="n">f_crit</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">dfn</span><span class="o">=</span><span class="n">df_numerator</span><span class="p">,</span> <span class="n">dfd</span><span class="o">=</span><span class="n">df_denominator</span><span class="p">)</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">α</span><span class="p">)</span>

<span class="c1"># Generate x values for plotting</span>
<span class="n">x_f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">hx_f</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_f</span><span class="p">,</span> <span class="n">df_numerator</span><span class="p">,</span> <span class="n">df_denominator</span><span class="p">)</span>

<span class="c1"># Create the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_f</span><span class="p">,</span> <span class="n">hx_f</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="c1"># Critical value</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">f_crit</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orangered&#39;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;F* (</span><span class="si">{</span><span class="n">f_crit</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Alpha area</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x_f</span><span class="p">[</span><span class="n">x_f</span> <span class="o">&gt;=</span> <span class="n">f_crit</span><span class="p">],</span>
    <span class="n">hx_f</span><span class="p">[</span><span class="n">x_f</span> <span class="o">&gt;=</span> <span class="n">f_crit</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tomato&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;α (</span><span class="si">{</span><span class="n">α</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># F-statistic</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">f_value</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;limegreen&#39;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;F (</span><span class="si">{</span><span class="n">f_value</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># P-value area</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">x_f</span><span class="p">[</span><span class="n">x_f</span> <span class="o">&gt;=</span> <span class="n">f_value</span><span class="p">],</span>
    <span class="n">hx_f</span><span class="p">[</span><span class="n">x_f</span> <span class="o">&gt;=</span> <span class="n">f_value</span><span class="p">],</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;greenyellow&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;P (</span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F-distribution (DFn=</span><span class="si">{</span><span class="n">df_numerator</span><span class="si">}</span><span class="s2">, DFd=</span><span class="si">{</span><span class="n">df_denominator</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">margins</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/140695ef16ae291fae155a498fdde12c85d1cc95aac5ed27e93ebee0154296df.png" src="_images/140695ef16ae291fae155a498fdde12c85d1cc95aac5ed27e93ebee0154296df.png" />
</div>
</div>
<p>In our example comparing the fixed slope and variable slope models, the calculated F-ratio of 58.690 falls far to the right of the critical value, and the corresponding area under the curve (P value) is extremely small (&lt;0.0001). This visually confirms the strong evidence against the null hypothesis, supporting our conclusion that the variable slope model provides a significantly better fit to the data than the fixed slope model.</p>
</section>
<section id="anova-table">
<h4>ANOVA table<a class="headerlink" href="#anova-table" title="Link to this heading">#</a></h4>
<p>As we’ve seen, the key components of ANOVA, i.e., sums of squares, degrees of freedom, and mean squares, provide a framework for comparing models and assessing their fit. We can construct an ANOVA table to summarize these components and formally test the significance of the difference between our fixed slope and variable slope models.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Source of variation</p></th>
<th class="head"><p>Sum of squares</p></th>
<th class="head"><p>DF</p></th>
<th class="head"><p>MS</p></th>
<th class="head"><p>F ratio</p></th>
<th class="head"><p>P value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Difference</p></td>
<td><p>315.1</p></td>
<td><p>1</p></td>
<td><p>315.14</p></td>
<td><p>58.69</p></td>
<td><p>&lt;0.0001</p></td>
</tr>
<tr class="row-odd"><td><p>Variable slope</p></td>
<td><p>43.0</p></td>
<td><p>8</p></td>
<td><p>5.37</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>Fixed slope</p></td>
<td><p>358.1</p></td>
<td><p>9</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<p>The null hypothesis in this comparison is that the <strong>fixed slope model is adequate</strong>, meaning that fixing the Hill slope to 1 does not significantly reduce the model’s ability to fit the data. In other words, the null hypothesis states that the additional flexibility of the variable slope in the 4PL model does not provide a significant improvement in the fit.</p>
<p>If the null hypothesis were true, we would expect the extra sum of squares due to the variable slope to be small, and the corresponding MSR would be similar to the MSE of the variable slope model. This would result in an F-ratio close to 1.0.</p>
<p>The variable slope model does provide a significantly better fit, resulting in a larger extra sum of squares, a larger MSR, and ultimately a larger F-ratio. This large F-ratio indicates that the improvement in fit due to the variable slope is greater than the variability that remains unexplained even by the more complex model.</p>
</section>
</section>
<section id="model-selection-criteria-aic-and-bic">
<h3>Model selection criteria: AIC and BIC<a class="headerlink" href="#model-selection-criteria-aic-and-bic" title="Link to this heading">#</a></h3>
<p>While the extra sum-of-squares F test is useful for comparing nested models, we often encounter situations where we need to compare models that are not nested. In such cases, we can use <strong>information criteria</strong> like the <strong>Akaike Information Criterion (AIC)</strong> and the <strong>Bayesian Information Criterion (BIC)</strong>.</p>
<p>AIC and BIC are statistical measures that estimate the prediction error for a statistical model relative to other statistical models trained on the same data. They are calculated based on the model’s likelihood (a measure of how well the model fits the data) and the number of parameters in the model:</p>
<ul class="simple">
<li><p>AIC is calculated as: <span class="math notranslate nohighlight">\(\text{AIC} = 2 k - 2 \log(L)\)</span>, where <span class="math notranslate nohighlight">\(k\)</span> is the number of parameters and <span class="math notranslate nohighlight">\(L\)</span> is the maximum likelihood estimator (MLE), discussed briefly at the end of the chapter on simple linear regression.</p></li>
<li><p>BIC is calculated as: <span class="math notranslate nohighlight">\(\text{BIC} = k \log(n) - 2 \log(L)\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p></li>
</ul>
<p>BIC penalizes models with more parameters more heavily than AIC, especially for larger sample sizes. This means that BIC tends to favor simpler models compared to AIC. On the other side, AIC is asymptotically equivalent to leave-one-out cross-validation, while BIC is consistent for model selection under certain assumptions.</p>
<p>The choice between AIC and BIC depends on the specific goals and preferences. If the primary goal is prediction accuracy, AIC might be preferred. If the goal is to identify the true underlying model, BIC might be more suitable.</p>
<p><code class="docutils literal notranslate"><span class="pre">lmfit</span></code> conveniently provides the AIC and BIC values in the fit report (<code class="docutils literal notranslate"><span class="pre">result.fit_report()</span></code>). We can also access them directly using the <code class="docutils literal notranslate"><span class="pre">result.aic</span></code> and <code class="docutils literal notranslate"><span class="pre">result.bic</span></code> attributes. We can use these values to compare different nonlinear models fit to the same data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the AIC and BIC values for the variable slope model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variable slope model:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;AIC: </span><span class="si">{</span><span class="n">result_variable_slope</span><span class="o">.</span><span class="n">aic</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># type: ignore</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;BIC: </span><span class="si">{</span><span class="n">result_variable_slope</span><span class="o">.</span><span class="n">bic</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># type: ignore</span>

<span class="c1"># Print the AIC and BIC values for the fixed slope model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Fixed slope model:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;AIC: </span><span class="si">{</span><span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">aic</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># type: ignore</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;BIC: </span><span class="si">{</span><span class="n">result_fixed_slope</span><span class="o">.</span><span class="n">bic</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># type: ignore</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Variable slope model:
AIC: 20.99
BIC: 22.18

Fixed slope model:
AIC: 42.31
BIC: 43.11
</pre></div>
</div>
</div>
</div>
<p><em>Lower values</em> of AIC and BIC indicate a <em>better model</em>. When comparing models, the model with the lowest AIC or BIC is generally preferred.</p>
<p>In our example, both AIC (20.99) and BIC (22.18) are lower for the variable slope model compared to the fixed slope model (AIC: 42.31, BIC: 43.11). This indicates that the variable slope model is preferred according to both criteria. It suggests that the improved fit achieved by allowing the Hill slope to vary outweighs the penalty for the additional parameter.</p>
<p>This conclusion reinforces the results of the extra sum-of-squares F test and the visual analysis of the residuals, providing further support for choosing the variable slope model as the better representation of the data.</p>
<p>However, it’s important to be aware of the potential for <strong>overfitting</strong>, especially when dealing with complex models. This is a particular concern in the field of <strong>machine learning</strong>, where models can have a large number of parameters and complex architectures.</p>
<p>Overfitting occurs when a model is too complex and learns the training data too well, including its noise and random fluctuations. This can lead to a model that performs well on the training data but poorly on new, unseen data.</p>
<p><strong>Model complexity</strong> refers to the number of parameters and the flexibility of a model. More complex models have more parameters and can fit a wider range of data patterns. However, this increased flexibility can also make them more prone to overfitting, leading to:</p>
<ul class="simple">
<li><p>Poor generalization: the model might not generalize well to new data, leading to inaccurate predictions.</p></li>
<li><p>Misleading interpretations: the model might capture spurious relationships in the data, leading to incorrect conclusions about the underlying biological processes.</p></li>
<li><p>Unstable parameter estimates: the estimated parameters might be highly sensitive to small changes in the data, making them unreliable.</p></li>
</ul>
<p>There are several techniques to address overfitting, including:</p>
<ul class="simple">
<li><p>Using simpler models: choosing a simpler model with fewer parameters can reduce the risk of overfitting.</p></li>
<li><p>Regularization: adding penalty terms to the objective function can discourage the model from fitting the noise in the data.</p></li>
<li><p>Cross-validation: evaluating the model on different subsets of the data can help assess its generalization performance.</p></li>
<li><p>Bootstrapping: as we’ll see in the next section, bootstrapping can help us assess the stability and variability of our parameter estimates and predictions, providing insights into the potential for overfitting.</p></li>
</ul>
</section>
</section>
<section id="bootstrapping-for-nonlinear-regression">
<h2>Bootstrapping for nonlinear regression<a class="headerlink" href="#bootstrapping-for-nonlinear-regression" title="Link to this heading">#</a></h2>
<p>Bootstrapping is a resampling method that allows us to estimate the uncertainty in our parameter estimates and predictions without making strong assumptions about the underlying distribution of the data. It’s particularly useful in situations where the data might not meet the assumptions of traditional statistical methods (e.g., normality of errors), we want to estimate the variability of our estimates or predictions, or we want to assess the stability and robustness of our model.</p>
<p>In this section, we’ll explore how to apply bootstrapping to nonlinear regression. We’ll see how it can help us:</p>
<ul class="simple">
<li><p>Estimate the uncertainty in our parameter estimates.</p></li>
<li><p>Generate confidence intervals for parameters and predictions.</p></li>
<li><p>Assess the stability of our model and its sensitivity to the data.</p></li>
</ul>
<section id="bootstrapping-in-non-linear-regression">
<h3>Bootstrapping in non-linear regression<a class="headerlink" href="#bootstrapping-in-non-linear-regression" title="Link to this heading">#</a></h3>
<p>As we’ve seen in previous chapters, <strong>bootstrapping</strong> is a powerful resampling technique that allows us to estimate the uncertainty in our statistical estimates. We’ve used it to construct confidence intervals for the mean, compare groups using t-tests, assess the strength of correlations, and even analyze the uncertainty in linear regression coefficients.</p>
<p>The good news is that this versatile technique can also be applied to nonlinear regression models!</p>
<p>Recall that the basic idea of bootstrapping is to create many new datasets by <em>randomly sampling with replacement</em> from our original dataset. We then fit our model to each of these bootstrap samples and obtain a distribution of parameter estimates. This distribution reflects the variability in our estimates due to the random sampling inherent in our original data.</p>
<p>By analyzing this distribution we can calculate the standard deviation of the bootstrap parameter estimates, which provides an estimate of the standard error of each parameter. We can also use the percentiles of the bootstrap distribution to construct confidence intervals for the parameters, and examine the shape of the bootstrap distribution to assess the <strong>stability</strong> of our parameter estimates and identify any potential biases.</p>
<p>While the general principles of bootstrapping remain the same, there are a few key differences and considerations when applying it to nonlinear regression:</p>
<ul class="simple">
<li><p>Model complexity: non-linear models can be more complex than linear models, with more parameters and potentially nonlinear relationships between the variables. This can affect the <em>convergence</em> and stability of the model fitting process during bootstrapping.</p></li>
<li><p>Computational cost: bootstrapping involves fitting the model many times (e.g., 1000 or more), which can be computationally expensive for complex nonlinear models.</p></li>
<li><p>Initial guesses: providing good initial guesses for the parameters becomes even more important in bootstrapping, as it can help ensure that the model converges to a reasonable solution for each bootstrap sample.</p></li>
</ul>
<section id="generating-bootstrap-samples">
<h4>Generating bootstrap samples<a class="headerlink" href="#generating-bootstrap-samples" title="Link to this heading">#</a></h4>
<p>Just as we did for correlation analysis and linear regression, the core idea of bootstrapping in nonlinear regression is to treat our observed sample of data points as a representation of the population. We then resample these data points with replacement to create many new bootstrap samples.</p>
<p>For each bootstrap sample, we fit our nonlinear regression model and obtain the estimated parameters. This process generates a collection of bootstrap estimates that we can use for inference:</p>
<ol class="arabic simple">
<li><p>Resample the data: randomly sample the data points (pairs of independent and dependent variable values) with replacement to create a bootstrap sample. The bootstrap sample should have the same size as the original dataset.</p></li>
<li><p>Fit the model: fit the nonlinear regression model to the bootstrap sample and obtain the estimated parameters.</p></li>
<li><p>Repeat: repeat steps 1 and 2 many times (e.g., 1000 or more) to create a collection of bootstrap estimates for the parameters.</p></li>
<li><p>Analyze the bootstrap distribution: use the distribution of the bootstrap estimates to calculate confidence intervals, standard errors, or other inferential statistics.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">draw_bs_pairs_nonlinreg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model_function</span><span class="p">,</span> <span class="n">initial_params</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform pairs bootstrap for nonlinear regression and return parameters.&quot;&quot;&quot;</span>

    <span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">bs_params_reps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">initial_params</span><span class="p">)))</span>  <span class="c1"># Array to store bootstrap parameter estimates</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
        <span class="n">bs_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inds</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">bs_x</span><span class="p">,</span> <span class="n">bs_y</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">bs_inds</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bs_inds</span><span class="p">]</span>

        <span class="c1"># Fit the nonlinear regression model with `curve_fit`</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">best_vals</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">model_function</span><span class="p">,</span> <span class="n">bs_x</span><span class="p">,</span> <span class="n">bs_y</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">initial_params</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># Handle cases where the fit might not converge</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Fit did not converge for bootstrap sample </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">bs_params_reps</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">best_vals</span>

    <span class="k">return</span> <span class="n">bs_params_reps</span>
</pre></div>
</div>
</div>
</div>
<p>We’ve added a <code class="docutils literal notranslate"><span class="pre">try-except</span></code> block to handle cases where the fit might not converge for some bootstrap samples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Set the number of bootstrap replicates</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="c1"># Generate bootstrap replicates of the parameters</span>
<span class="n">bs_params_reps</span> <span class="o">=</span> <span class="n">draw_bs_pairs_nonlinreg</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">norepi_log</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="n">relaxation</span><span class="p">,</span>
   <span class="n">model_function</span><span class="o">=</span><span class="n">hill_equation_bottom_zero</span><span class="p">,</span>
   <span class="n">initial_params</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span>  <span class="c1"># p0=[100, -6, 1], as in the previous sections</span>
   <span class="n">size</span><span class="o">=</span><span class="n">B</span><span class="p">)</span>

<span class="c1"># Print the first 5 replicates for each parameter</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;top replicates:&quot;</span><span class="p">,</span> <span class="n">bs_params_reps</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;logEC50 replicates:&quot;</span><span class="p">,</span> <span class="n">bs_params_reps</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;hill_slope replicates:&quot;</span><span class="p">,</span> <span class="n">bs_params_reps</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Warning: Fit did not converge for bootstrap sample 4678: Optimal parameters not found: Number of calls to function has reached maxfev = 800.
Warning: Fit did not converge for bootstrap sample 4949: Optimal parameters not found: Number of calls to function has reached maxfev = 800.
Warning: Fit did not converge for bootstrap sample 9372: Optimal parameters not found: Number of calls to function has reached maxfev = 800.
top replicates: [106.74351124 104.54797682 103.62662831 103.72617883 104.50075986]
logEC50 replicates: [-5.56677302 -5.68082929 -5.63270256 -5.64913139 -5.61797421]
hill_slope replicates: [0.60082667 0.58383754 0.63653616 0.60413869 0.63394325]
</pre></div>
</div>
</div>
</div>
<p>We might encounter “Warning: Fit did not converge for bootstrap sample xxxx” during the bootstrapping process. It indicates that the <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code> function failed to converge to a solution for some of the bootstrap samples. This is not uncommon in nonlinear regression, and it often suggests that the model is complex or the data is noisy or limited.</p>
<p>We could try to improve the initial guesses for the parameters, increase the number of iterations within <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code> using the ‘maxfev’ argument (which is 100*(N+1) by default), or try different fitting methods such as ‘lm’, ‘trf’, or ‘dogbox’. However, a small number of non-converged samples, as in our case, is usually not a major concern, especially when we have a large number of bootstrap replicates.</p>
</section>
<section id="confidence-intervals">
<h4>Confidence intervals<a class="headerlink" href="#confidence-intervals" title="Link to this heading">#</a></h4>
<p>Now that we have a collection of bootstrap replicates for our nonlinear model’s parameters, we can use them to construct confidence intervals. These intervals provide a range of plausible values for the true population values of the parameters.</p>
<p>To estimate the confidence intervals, we’ll use the percentiles of the bootstrap distributions. For example, to construct a 95% confidence interval for a parameter, we’ll use the 2.5th and 97.5th percentiles of its bootstrap replicates. This means that 95% of the bootstrapped values for that parameter fall within this interval, providing a range of plausible values for the true population parameter.</p>
<p>In nonlinear models, the interpretation of the parameters might be more complex than in linear models. The confidence intervals provide a range of plausible values, but the biological interpretation of those values might require careful consideration of the nonlinear relationships involved.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the maximal value of the &#39;top&#39; parameter in the bootstrap replications</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Maximal value of the top parameter in the bootstrap replicates: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">bs_params_reps</span><span class="p">[:,</span><span class="w"> </span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Use np.nanmax to ignore NaN values</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Maximal value of the top parameter in the bootstrap replicates: 924.2
</pre></div>
</div>
</div>
</div>
<p>In our example, we encountered a bootstrap replicate where the ‘top’ parameter reached a value of 924.2%, which is clearly unrealistic for a maximum relaxation response that should be around 100%. This illustrates a potential limitation of bootstrapping in nonlinear regression: the iterative optimization process can sometimes lead to <em>extreme parameter values</em> for certain bootstrap samples, especially if the model is complex or the data is noisy.</p>
<p>However, we can implement safeguards within the bootstrapping process itself to mitigate this issue. For instance, we can add checks to identify and exclude replicates with unreasonable parameter values, and assign NaN values This helps ensure that the bootstrap distribution is not unduly influenced by these extreme cases.</p>
<p>It’s important to be aware of these potential limitations and to carefully examine the bootstrap replicates to identify any unusual or unrealistic values. By implementing appropriate safeguards and refining the bootstrapping process, we can improve the reliability and accuracy of our uncertainty estimates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Set the number of bootstrap replicates</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="c1"># Generate bootstrap replicates of the parameters</span>
<span class="n">bs_params_reps</span> <span class="o">=</span> <span class="n">draw_bs_pairs_nonlinreg</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">norepi_log</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="n">relaxation</span><span class="p">,</span>
   <span class="n">model_function</span><span class="o">=</span><span class="n">hill_equation_bottom_zero</span><span class="p">,</span>
   <span class="n">initial_params</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span>  <span class="c1"># p0=[100, -6, 1], as in the previous sections</span>
   <span class="n">size</span><span class="o">=</span><span class="n">B</span><span class="p">)</span>

<span class="c1"># Check for unrealistic values and replace with NaN</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bs_params_reps</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">250</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>  <span class="c1"># Check &#39;top&#39; and &#39;hill_slope&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Unrealistic parameter values for bootstrap sample </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">bs_params_reps</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>  <span class="c1"># Assign NaN values for the entire row</span>

<span class="c1"># Print the number of NaN bootstrap replicates</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of invalid bootstrap replicates: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">bs_params_reps</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Warning: Fit did not converge for bootstrap sample 4678: Optimal parameters not found: Number of calls to function has reached maxfev = 800.
Warning: Fit did not converge for bootstrap sample 4949: Optimal parameters not found: Number of calls to function has reached maxfev = 800.
Warning: Fit did not converge for bootstrap sample 9372: Optimal parameters not found: Number of calls to function has reached maxfev = 800.
Warning: Unrealistic parameter values for bootstrap sample 13
Warning: Unrealistic parameter values for bootstrap sample 942
Warning: Unrealistic parameter values for bootstrap sample 3365
Warning: Unrealistic parameter values for bootstrap sample 3525
Warning: Unrealistic parameter values for bootstrap sample 5277
Warning: Unrealistic parameter values for bootstrap sample 8203
Warning: Unrealistic parameter values for bootstrap sample 9724
Number of invalid bootstrap replicates: 21
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the means, SEs, and 95% confidence intervals for each parameter</span>
<span class="n">bs_params_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">bs_params_reps</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Use np.nanmean to ignore NaNs</span>
<span class="n">bs_params_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanstd</span><span class="p">(</span><span class="n">bs_params_reps</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Use np.nanstd to ignore NaNs</span>
<span class="n">bs_params_ci</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanpercentile</span><span class="p">(</span><span class="n">bs_params_reps</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Use np.nanpercentile to ignore NaNs</span>

<span class="c1"># Print the results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bootstrap Statistics:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---------------------&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="s1">&#39;logEC50&#39;</span><span class="p">,</span> <span class="s1">&#39;hill_slope&#39;</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean = </span><span class="si">{</span><span class="n">bs_params_means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE = </span><span class="si">{</span><span class="n">bs_params_se</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI = </span><span class="si">{</span><span class="n">bs_params_ci</span><span class="p">[:,</span><span class="w"> </span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Bootstrap Statistics:
---------------------
top:
  Mean = 104.8493
  SE = 3.9434
  95% CI = [101.75121552 112.26225763]
logEC50:
  Mean = -5.6261
  SE = 0.0713
  95% CI = [-5.71889986 -5.46448744]
hill_slope:
  Mean = 0.6172
  SE = 0.0429
  95% CI = [0.53992946 0.70512507]
</pre></div>
</div>
</div>
</div>
<p>Interestingly, the bootstrap confidence interval for the ‘top’ parameter ([101.75, 112.26]) is wider and slightly higher than the parametric intervals calculated earlier, either manually ([99.3, 108.80]) or by <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> ([99.660, 109.115]). This is likely related to the higher standard error estimated by the bootstrap method (3.9434) compared to the parametric approaches (2.06). This suggests greater uncertainty in estimating the maximum relaxation (‘top’) compared to the other parameters, possibly due to the ‘top’ parameter being more sensitive to variations in the data, especially at higher norepinephrine concentrations. The structure of the Hill equation itself might also contribute to this, as it could allow for more flexibility in the ‘top’ parameter compared to the others. Additionally, there might be interdependencies between the parameters, where the uncertainty in estimating ‘logEC50’ and ‘hill_slope’ indirectly influences the uncertainty in ‘top’.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a figure with three subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Plot the bootstrap distributions for each parameter</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="s1">&#39;logEC50&#39;</span><span class="p">,</span> <span class="s1">&#39;hill_slope&#39;</span><span class="p">]):</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
        <span class="n">bs_params_reps</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span>
        <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gold&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Bootstrap </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;cornflowerblue&#39;</span><span class="p">,</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Observed </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">bs_params_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;2.5th percentile&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">bs_params_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;97.5th percentile&#39;</span><span class="p">)</span>
    <span class="c1"># Adjust slightly the x-axis limits</span>
    <span class="k">if</span> <span class="n">param_name</span> <span class="o">==</span> <span class="s1">&#39;top&#39;</span><span class="p">:</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="mi">95</span><span class="p">,</span> <span class="mi">125</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">param_name</span> <span class="o">==</span> <span class="s1">&#39;logEC50&#39;</span><span class="p">:</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">))</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap distribution of </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> (B=</span><span class="si">{</span><span class="n">B</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3147af1c065515ba49be88536df20d38c016939343bf1b110627b36132dffe49.png" src="_images/3147af1c065515ba49be88536df20d38c016939343bf1b110627b36132dffe49.png" />
</div>
</div>
<p>The bootstrap distribution for the ‘top’ parameter shows a clear skew towards higher values, while the distributions for the ‘logEC50’ and ‘hill_slope’ parameters appear more symmetric.But overall, the results from the bootstrapping approach are comparable to those obtained from the parametric methods, suggesting that both provide reasonable estimates of the uncertainty in our nonlinear model.</p>
</section>
<section id="confidence-band">
<h4>Confidence band<a class="headerlink" href="#confidence-band" title="Link to this heading">#</a></h4>
<p>Earlier, we visualized the 95% confidence band around the fitted nonlinear curve. Now, let’s explore how the curve itself varies across different bootstrap samples. This will provide a more intuitive understanding of the uncertainty captured by the confidence band.</p>
<p>The following plot shows several bootstrapped curves, each fitted to a different resampled dataset. The variability of these curves reflects the uncertainty in the estimated relationship between the variables. Notice how the spread of these curves relates to the width of the confidence band we visualized earlier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the variability of the fitted curves</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="c1"># Generate predicted values using bootstrap parameter replicates</span>
    <span class="n">predicted_values</span> <span class="o">=</span> <span class="n">hill_equation_bottom_zero</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="o">*</span><span class="n">bs_params_reps</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="mi">10</span><span class="o">**</span><span class="n">x_range</span><span class="p">,</span>
        <span class="n">predicted_values</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;forestgreen&#39;</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">.4</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Bootstrap replicates&#39;</span> <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># Print label only once in legend</span>

<span class="c1"># Plot the original data and the best-fit curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">norepi_lin</span><span class="p">,</span> <span class="n">relaxation</span><span class="p">,</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="mi">10</span><span class="o">**</span><span class="n">x_range</span><span class="p">,</span>
    <span class="n">hill_equation_bottom_zero</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="o">*</span><span class="n">best_vals</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;crimson&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Best fit&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;[Norepinephrine, M] (log scale)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">% r</span><span class="s2">elaxation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Bootstrapped curves and the confidence band&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/227bb7b84efd102089e21d61239fb0b491ea6e8a7276bdffa1af228dfbdfde12.png" src="_images/227bb7b84efd102089e21d61239fb0b491ea6e8a7276bdffa1af228dfbdfde12.png" />
</div>
</div>
<p>Looking at the plot, we can see that the bootstrapped curves exhibit more variability in the upper-right region, corresponding to the highest norepinephrine concentrations. Some of the curves even detach from the confidence band at the two highest x-values. This suggests that our model is less certain about the predictions in that region, possibly due to the limited data or the nature of the response plateauing at higher concentrations.</p>
</section>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In this chapter, we explored nonlinear regression, learning how to model complex relationships in biological data that cannot be captured by linear models. We started with the mathematical foundations, examining common nonlinear functions and the concept of least squares estimation.</p>
<p>We then dove into the practical application of nonlinear regression using Python. We learned how to fit models using <code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit</span></code> and the more advanced <code class="docutils literal notranslate"><span class="pre">lmfit</span></code> package. This included defining model functions, providing initial parameter guesses, and extracting the best-fit parameter estimates.</p>
<p><code class="docutils literal notranslate"><span class="pre">lmfit</span></code> provided us with tools to impose constraints on parameters, ensuring biologically meaningful results. We also gained access to a wider range of fit statistics and diagnostics to assess the quality of our models.</p>
<p>Visualization played a crucial role in understanding and interpreting the results. We plotted fitted curves, generated confidence intervals, and explored the parameter space to see how changes in parameter values affect the model’s predictions.</p>
<p>We discussed the importance of considering model complexity and the risk of overfitting, where a model captures noise rather than the true underlying relationship.</p>
<p>Finally, we applied bootstrapping to estimate the uncertainty in our parameter estimates and predictions. This non-parametric technique allowed us to assess the variability and stability of our models.</p>
<p>With this knowledge of nonlinear regression, we are now equipped to analyze more complex biological datasets and uncover hidden patterns that linear models cannot capture. In the next chapter, we’ll expand our toolkit further by exploring multiple regression, where we’ll incorporate multiple independent variables into our models. This will allow us to analyze even more intricate scenarios and gain a deeper understanding of the factors that influence biological processes.</p>
</section>
<section id="cheat-sheet">
<h2>Cheat sheet<a class="headerlink" href="#cheat-sheet" title="Link to this heading">#</a></h2>
<section id="fitting-a-model">
<h3>Fitting a model<a class="headerlink" href="#fitting-a-model" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining the model function</span>
<span class="k">def</span> <span class="nf">hill_equation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">logEC50</span><span class="p">,</span> <span class="n">hill_slope</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">bottom</span> <span class="o">+</span> <span class="p">(</span><span class="n">top</span> <span class="o">-</span> <span class="n">bottom</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">10</span><span class="o">**</span><span class="p">((</span><span class="n">logEC50</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">hill_slope</span><span class="p">))</span>

<span class="n">p0</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span>  <span class="c1"># Initial guess for the parameters</span>

<span class="c1"># Fit the model to the data with curve_fit</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">curve_fit</span>

<span class="n">popt</span><span class="p">,</span> <span class="n">pcov</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span>
    <span class="n">hill_equation</span><span class="p">,</span>
    <span class="n">X</span><span class="p">,</span>  <span class="c1"># Array of the independent variable</span>
    <span class="n">y</span><span class="p">,</span>  <span class="c1"># Array of the dependent variable</span>
    <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>

<span class="c1"># Calculate the standard errors of the parameters</span>
<span class="n">standard_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">pcov</span><span class="p">))</span>

<span class="c1"># Fit the model using lmfit</span>
<span class="kn">from</span> <span class="nn">lmfit</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">hill_equation</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">make_params</span><span class="p">()</span>  <span class="c1"># Initialize parameters object</span>
<span class="n">params</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Add individual paramters and constraints</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>  <span class="c1"># Fit the model to the data</span>
<span class="n">result</span><span class="o">.</span><span class="n">best_values</span>  <span class="c1"># Access the best-fit values</span>
<span class="n">result</span><span class="o">.</span><span class="n">chisqr</span>  <span class="c1"># Access the sum of sqaures error (RSS)</span>

<span class="c1"># Print the full fit report</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">fit_report</span><span class="p">())</span>

<span class="c1"># Access and display the fit results using result.result</span>
<span class="n">result</span><span class="o">.</span><span class="n">result</span>

<span class="c1"># Predictions</span>
<span class="n">predicted_y</span> <span class="o">=</span> <span class="n">hill_equation</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="o">*</span><span class="n">popt</span><span class="p">)</span>  <span class="c1"># Best-fit values from SciPy</span>
<span class="n">predicted_y</span> <span class="o">=</span> <span class="n">hill_equation</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="o">**</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>  <span class="c1"># From lmfit</span>
<span class="n">predicted_y</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">new_x</span><span class="p">)</span>  <span class="c1"># Using directly lmfit result</span>
</pre></div>
</div>
</section>
<section id="goodness-of-fit">
<h3>Goodness of fit<a class="headerlink" href="#goodness-of-fit" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="c1"># Plot residuals</span>
<span class="n">result</span><span class="o">.</span><span class="n">plot_residuals</span><span class="p">()</span>

<span class="n">result</span><span class="o">.</span><span class="n">rsquared</span>  <span class="c1"># R²</span>
</pre></div>
</div>
</section>
<section id="visualizing-non-linear-models">
<h3>Visualizing non-linear models<a class="headerlink" href="#visualizing-non-linear-models" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the optimal curve</span>
<span class="n">result</span><span class="o">.</span><span class="n">plot_fit</span><span class="p">()</span>

<span class="c1"># Plot the optimal curve and residuals</span>
<span class="n">result</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

<span class="c1"># Explore the parameter space</span>
<span class="n">result</span><span class="o">.</span><span class="n">plot_fit</span><span class="p">()</span>  <span class="c1"># Data and best-fit</span>
<span class="n">predicted_y</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">a1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="n">predicted_y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id1">
<h3>Confidence intervals<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the confidence interval report</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">ci_report</span><span class="p">())</span>  <span class="c1"># See also with_offset and sigmas options</span>

<span class="c1"># Calculate the uncertainty in the predicted values</span>
<span class="n">result</span><span class="o">.</span><span class="n">eval_uncertainty</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="comparing-non-linear-models">
<h3>Comparing non-linear models<a class="headerlink" href="#comparing-non-linear-models" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the two models to compare </span>
<span class="c1"># model_1 = Model(equation_1)  # More complex of the two</span>
<span class="c1"># params_model_1 = model_1.make_params()</span>
<span class="c1"># result_model_1 = model_1.fit()</span>
<span class="c1"># model_2 = Model(equation_2)  # Nested model</span>

<span class="c1"># Calculate the F-ratio</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Number of observations</span>
<span class="n">df_numerator</span> <span class="o">=</span> <span class="n">result_model_2</span><span class="o">.</span><span class="n">nfree</span> <span class="o">-</span> <span class="n">result_model_1</span><span class="o">.</span><span class="n">nfree</span>  <span class="c1"># DF for the difference between models</span>
<span class="n">df_denominator</span> <span class="o">=</span> <span class="n">result_model_1</span><span class="o">.</span><span class="n">nfree</span>  <span class="c1"># DF the more complex model</span>
<span class="n">f_value</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">((</span><span class="n">result_model_2</span><span class="o">.</span><span class="n">chisqr</span> <span class="o">-</span> <span class="n">result_model_1</span><span class="o">.</span><span class="n">chisqr</span><span class="p">)</span> <span class="o">/</span> <span class="n">df_numerator</span><span class="p">)</span>
    <span class="o">/</span>
    <span class="p">(</span><span class="n">result_model_1</span><span class="o">.</span><span class="n">chisqr</span> <span class="o">/</span> <span class="n">df_denominator</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Calculate the P value using the survival function of the F-distribution</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">f</span>
<span class="n">p_value</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">f_value</span><span class="p">,</span> <span class="n">dfn</span><span class="o">=</span><span class="n">df_numerator</span><span class="p">,</span> <span class="n">dfd</span><span class="o">=</span><span class="n">df_denominator</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="bootstrapping">
<h3>Bootstrapping<a class="headerlink" href="#bootstrapping" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">draw_bs_pairs_nonlinreg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model_function</span><span class="p">,</span> <span class="n">initial_params</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform pairs bootstrap for nonlinear regression and return parameters.&quot;&quot;&quot;</span>

    <span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">bs_params_reps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">initial_params</span><span class="p">)))</span>  <span class="c1"># Array to store bootstrap parameter estimates</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
        <span class="n">bs_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inds</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">bs_x</span><span class="p">,</span> <span class="n">bs_y</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">bs_inds</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bs_inds</span><span class="p">]</span>

        <span class="c1"># Fit the nonlinear regression model with `curve_fit`</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">best_vals</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">curve_fit</span><span class="p">(</span><span class="n">model_function</span><span class="p">,</span> <span class="n">bs_x</span><span class="p">,</span> <span class="n">bs_y</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">initial_params</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># Handle cases where the fit might not converge</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Fit did not converge for bootstrap sample </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">bs_params_reps</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">best_vals</span>

    <span class="k">return</span> <span class="n">bs_params_reps</span>

<span class="c1"># Set the number of bootstrap replicates</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="c1"># Generate bootstrap replicates of the parameters</span>
<span class="n">bs_params_reps</span> <span class="o">=</span> <span class="n">draw_bs_pairs_nonlinreg</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
   <span class="n">model_function</span><span class="o">=</span><span class="n">hill_equation</span>
   <span class="n">initial_params</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span>
   <span class="n">size</span><span class="o">=</span><span class="n">B</span><span class="p">)</span>

<span class="c1"># Check for unrealistic values and replace with NaN</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bs_params_reps</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">250</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>  <span class="c1"># Check &#39;top&#39; and &#39;hill_slope&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Unrealistic parameter values for bootstrap sample </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">bs_params_reps</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>  <span class="c1"># Assign NaN values for the entire row</span>

<span class="c1"># Calculate the means, SEs, and 95% confidence intervals for each parameter</span>
<span class="n">bs_params_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">bs_params_reps</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Use np.nanmean to ignore NaNs</span>
<span class="n">bs_params_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanstd</span><span class="p">(</span><span class="n">bs_params_reps</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Use np.nanstd to ignore NaNs</span>
<span class="n">bs_params_ci</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanpercentile</span><span class="p">(</span><span class="n">bs_params_reps</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Use np.nanpercentile to ignore NaNs</span>
</pre></div>
</div>
</section>
</section>
<section id="session-information">
<h2>Session information<a class="headerlink" href="#session-information" title="Link to this heading">#</a></h2>
<p>The output below details all packages and version necessary to reproduce the results in this report.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>python<span class="w"> </span>--version
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">importlib.metadata</span> <span class="kn">import</span> <span class="n">version</span>

<span class="c1"># List of packages we want to check the version</span>
<span class="n">packages</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;numpy&#39;</span><span class="p">,</span> <span class="s1">&#39;pandas&#39;</span><span class="p">,</span> <span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;seaborn&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy&#39;</span><span class="p">,</span> <span class="s1">&#39;lmfit&#39;</span><span class="p">]</span>

<span class="c1"># Initialize an empty list to store the versions</span>
<span class="n">versions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop over the packages</span>
<span class="k">for</span> <span class="n">package</span> <span class="ow">in</span> <span class="n">packages</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Get the version of the package</span>
        <span class="n">package_version</span> <span class="o">=</span> <span class="n">version</span><span class="p">(</span><span class="n">package</span><span class="p">)</span>
        <span class="c1"># Append the version to the list</span>
        <span class="n">versions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">package_version</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>  <span class="c1"># Use a more general exception for broader compatibility</span>
        <span class="n">versions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;Not installed&#39;</span><span class="p">)</span>

<span class="c1"># Print the versions</span>
<span class="k">for</span> <span class="n">package</span><span class="p">,</span> <span class="n">version</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">packages</span><span class="p">,</span> <span class="n">versions</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">package</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Python 3.12.8
-------------
numpy: 1.26.4
pandas: 2.2.2
matplotlib: 3.9.2
seaborn: 0.13.2
scipy: 1.14.1
lmfit: 1.3.2
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="35%20-%20Comparing%20models.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Comparing models</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundation">Mathematical foundation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#various-nonlinear-models">Various nonlinear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-biological-example">A biological example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-estimation">Least squares estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinear-regression-in-python">Nonlinear regression in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-the-data">Exploring the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-guide-to-using-curve-fit">Step-by-Step guide to using <code class="docutils literal notranslate"><span class="pre">curve_fit</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-model-function">Defining the model function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#providing-initial-parameter-guesses">Providing initial parameter guesses</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-model-to-the-data">Fitting the model to the data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#extraction-standard-errors">Extraction standard errors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-nonlinear-regression-with-lmfit">Advanced nonlinear regression with <code class="docutils literal notranslate"><span class="pre">lmfit</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-parameter-bounds-and-constraints">Setting parameter bounds and constraints</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improved-handling-of-complex-models">Improved handling of complex models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-richer-fit-statistics-and-diagnostics">Accessing richer fit statistics and diagnostics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-values">Predicting values</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-the-goodness-of-fit">Assessing the goodness of fit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-analysis-in-nonlinear-regression">Residual analysis in nonlinear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-residuals-vs-predicted-values">Plotting residuals vs. predicted values</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-patterns-in-residuals-to-identify-potential-issues">Examining Patterns in Residuals to Identify Potential Issues</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-r-squared">Calculating R-squared</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-and-interpreting-non-linear-models">Visualizing and interpreting non-linear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-the-optimal-curve">Plotting the optimal curve</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-confidence-intervals">Generating confidence intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-the-parameter-space-visually">Exploring the parameter space visually</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-nonlinear-models">Comparing nonlinear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nested-models">Nested models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-extra-sum-of-squares-f-test">The extra sum-of-squares F test</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-models-to-compare">Fitting the models to compare</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-the-residuals">Comparing the residuals</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-variance">Analysis of variance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squares">Mean squares</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#f-ratio">F-ratio</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#p-value">P value</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-f-distribution-and-critical-values">Visualizing the F-distribution and critical values</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#anova-table">ANOVA table</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection-criteria-aic-and-bic">Model selection criteria: AIC and BIC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping-for-nonlinear-regression">Bootstrapping for nonlinear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping-in-non-linear-regression">Bootstrapping in non-linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-bootstrap-samples">Generating bootstrap samples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals">Confidence intervals</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-band">Confidence band</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cheat-sheet">Cheat sheet</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-a-model">Fitting a model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#goodness-of-fit">Goodness of fit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-non-linear-models">Visualizing non-linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Confidence intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-non-linear-models">Comparing non-linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrapping">Bootstrapping</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#session-information">Session information</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sébastien Wieckowski
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>