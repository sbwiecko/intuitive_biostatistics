
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Statistical Significance &#8212; The Python Companion of Intuitive Biostatistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '15 - Statistical Significance';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Confidence Interval of a Mean" href="12%20-%20Confidence%20Interval%20of%20a%20Mean.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.jpg" class="logo__image only-light" alt="The Python Companion of Intuitive Biostatistics - Home"/>
    <script>document.write(`<img src="_static/logo.jpg" class="logo__image only-dark" alt="The Python Companion of Intuitive Biostatistics - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    Python Companion Guide to “Intuitive Biostatistics, 4th edition”
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04%20-%20Confidence%20Interval%20of%20a%20Proportion.html">Confidence Interval of a Proportion</a></li>
<li class="toctree-l1"><a class="reference internal" href="05%20-%20Confidence%20Interval%20of%20Survival%20Data.html">Confidence Interval of Survival Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="06%20-%20Confidence%20Interval%20of%20Counted%20Data%20%28Poisson%20Distribution%29.html">Confidence Interval of Counted Data (Poisson Distribution)</a></li>
<li class="toctree-l1"><a class="reference internal" href="09%20-%20Quantifying%20Scatter.html">Quantifying Scatter</a></li>
<li class="toctree-l1"><a class="reference internal" href="10%20-%20Gaussian%20Distribution.html">The Gaussian Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="12%20-%20Confidence%20Interval%20of%20a%20Mean.html">Confidence Interval of a Mean</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Statistical Significance</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sbwiecko/intuitive_biostatistics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sbwiecko/intuitive_biostatistics/edit/master/15 - Statistical Significance.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/15 - Statistical Significance.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Statistical Significance</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-reproducibility-challenge-of-p-values">The reproducibility challenge of P-values</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-variability-of-p-values-under-a-true-effect">The variability of P-values under a true effect</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-risk-of-false-positives-under-the-null-hypothesis">The risk of false positives under the null hypothesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-influence-of-sample-size-on-p-values">The influence of sample size on P-values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-danger-of-ad-hoc-sample-size-decisions-and-cumulative-p-values">The danger of ad hoc sample size decisions and cumulative P-values</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-and-p-values">Confidence intervals and P-values</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-significance-and-hypothesis-testing">Statistical significance and hypothesis testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-i-and-type-ii-errors">Type I and type II errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-statistical-significance">Interpreting statistical significance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-significance-level-alpha">The significance level (<span class="math notranslate nohighlight">\(\alpha\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-false-positive-report-probability-fprp">The False Positive Report Probability (FPRP)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prior-probability-influences-the-fprp">The prior probability influences the FPRP</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-1">Prior probability = 1%</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-50">Prior probability = 50%</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-challenge-of-multiple-comparisons">The challenge of multiple comparisons</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bonferroni-correction">The Bonferroni correction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-false-discovery-rate-fdr">The False Discovery Rate (FDR)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-power">Statistical power</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interlude-the-effect-size">Interlude - The effect size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relationship-between-power-effect-size-sample-size-and-variability">Relationship between power, effect size, sample size, and variability</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-sample-t-test">One-sample t-test</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#unpaired-t-test">Unpaired t-test</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-choice-of-80-power">The choice of 80% power</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#paired-t-test">Paired t-test</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-a-sample-size">Choosing a sample size</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-for-power-calculations">Tools for power calculations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-sizes-for-comparing-proportions-a-b-test">Sample sizes for comparing proportions (A/B test)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#incorporating-fprp-into-sample-size-decisions">Incorporating FPRP into sample size decisions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#complexities-when-computing-sample-size">Complexities when computing sample size</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-simulation-based-approach">The simulation-based approach</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-for-equivalence-or-noninferiority">Testing for equivalence or noninferiority</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bioequivalence">Bioequivalence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-equivalence-and-noninferiority-tests">Interpreting equivalence and noninferiority tests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#two-one-sided-tests-tost">Two One-Sided Tests (TOST)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-diagnostic-accuracy">Assessing diagnostic accuracy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">Definitions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-curves">ROC curves</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-thresholds">Decision thresholds</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#area-under-the-curve-auc">Area Under the Curve (AUC)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-and-python">ROC and Python</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-studies">Case studies</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#porphyria-test-in-0-01-prevalence">Porphyria test in 0.01% prevalence</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#porphyria-test-in-50-prevalence">Porphyria test in 50% prevalence</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hiv-test">HIV test</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-with-statistical-hypothesis-testing">Analogy with statistical hypothesis testing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-revisited">Bayes revisited</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#session-information">Session Information</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="statistical-significance">
<h1>Statistical Significance<a class="headerlink" href="#statistical-significance" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In the world of biostatistics, we often face a crucial question: Does the data we’ve gathered reveal a <em>true effect</em>, or is it simply the result of <em>random chance</em>? The concept of <strong>statistical significance</strong> offers a framework to grapple with this uncertainty, providing a tool to help us discern <em>meaningful</em> patterns from the <em>noise</em> inherent in biological systems. This chapter delves into the core of statistical significance, exploring why it matters, when it’s most useful, and how it can be a valuable guide in drawing conclusions from the data. Importantly, we’ll also discuss the potential pitfalls of relying too heavily on this concept, as well as when alternative approaches might be more appropriate.</p>
</section>
<section id="the-reproducibility-challenge-of-p-values">
<h2>The reproducibility challenge of P-values<a class="headerlink" href="#the-reproducibility-challenge-of-p-values" title="Link to this heading">#</a></h2>
<section id="the-variability-of-p-values-under-a-true-effect">
<h3>The variability of P-values under a true effect<a class="headerlink" href="#the-variability-of-p-values-under-a-true-effect" title="Link to this heading">#</a></h3>
<p>P-values, a cornerstone of statistical significance testing, can often give the illusion of definitive answers. However, they are inherently sensitive to the specific data set at hand, leading to a reproducibility challenge. To illustrate this, let’s consider a Python simulation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">ttest_ind</span>

<span class="c1"># For reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Parameters</span>
<span class="n">sample_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">population_std_dev</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">mean_difference</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store p-values</span>
<span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Run simulations</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
    <span class="c1"># Sample from two populations with different means</span>
    <span class="n">data1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">population_std_dev</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">)</span>
    <span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean_difference</span><span class="p">,</span> <span class="n">population_std_dev</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">)</span>
    
    <span class="c1"># Perform t-test</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">)</span>
    <span class="n">p_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>

<span class="c1"># Analyze p-values</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Proportion of P-values &lt; 0.05:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Proportion of P-values &lt; 0.05: 0.592
</pre></div>
</div>
</div>
</div>
<p>This code repeatedly samples 10 data points from two Gaussian distributions with the same standard deviation (5.0) but a mean difference of 5.0, so under a <em>true effect</em>. Each time, it performs a <strong>t-test</strong> to compare the means and records the resulting p-value.</p>
<p>What we often find is a surprising inconsistency. Even though the underlying populations have a true difference in means, the proportion of P-values considered “statistically significant” (typically below 0.05) fluctuates significantly across simulations. This highlights the inherent variability of p-values and the potential for misleading conclusions when a single analysis is treated as definitive.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># For reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Simulation parameters</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1"># Sample size per group</span>
<span class="n">SD</span> <span class="o">=</span> <span class="mf">5.0</span>    <span class="c1"># Standard deviation</span>
<span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span>    <span class="c1"># Mean of the first population</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">2500</span>

<span class="c1"># Store p-values</span>
<span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Run simulations</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
    <span class="n">pop_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">SD</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
    <span class="n">pop_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span> <span class="o">+</span> <span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">SD</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>  
    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">pop_1</span><span class="p">,</span> <span class="n">pop_2</span><span class="p">)</span>
    <span class="n">p_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>

<span class="c1"># Calculate percentiles</span>
<span class="n">percentiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">])</span>  <span class="c1"># Get 2.5% and 97.5% percentiles</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The 2.5th and 97.5th percentiles are </span><span class="si">{</span><span class="n">percentiles</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span> <span class="c1"># Optional: set a clean seaborn style</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="c1"># Histogram with log scale</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;skyblue&quot;</span><span class="p">)</span>
<span class="c1"># Vertical line at p=0.05</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;P = 0.05&quot;</span><span class="p">)</span>

<span class="c1"># Add percentile lines and annotations</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">percentiles</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;P-value&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency (log scale)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="s2">&quot;Distribution of P-values from 2500 simulations</span><span class="se">\n</span><span class="s2">(True mean difference = 5.0)&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The 2.5th and 97.5th percentiles are [1.69152124e-04 7.02604441e-01]
</pre></div>
</div>
<img alt="_images/be03c553d4d32d704392510b7fe1f4cf892a7897625665b096427e02931eac4c.png" src="_images/be03c553d4d32d704392510b7fe1f4cf892a7897625665b096427e02931eac4c.png" />
</div>
</div>
<p>As our simulation demonstrates, the middle 95% of P-values can span a vast range, from as low as 0.000169 to as high as 0.7026 - a difference of over three orders of magnitude! This immense variability underscores a crucial point: P-values are far less reproducible than most researchers anticipate.</p>
<p>This lack of reproducibility has significant implications for the conclusions we draw about statistical significance. Consider a study that finds a P-value of exactly 0.05, barely crossing the threshold for significance. If this study were repeated, there’s only a 50% chance that the new P-value would also be below 0.05, as revealed by simulations:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Initial P value</p></th>
<th class="head text-left"><p>Probability of P-value &lt; 0.05 in replication</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>0.10</p></td>
<td class="text-left"><p>38 %</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>0.05</p></td>
<td class="text-left"><p>50 %</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>0.01</p></td>
<td class="text-left"><p>73 %</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>0.001</p></td>
<td class="text-left"><p>91 %</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>0.00031</p></td>
<td class="text-left"><p>95 %</p></td>
</tr>
</tbody>
</table>
<p>In other words, even a seemingly “significant” finding is surprisingly susceptible to being overturned by chance in subsequent experiments.</p>
<p>This phenomenon isn’t limited to borderline cases. Even with a highly “significant” initial P-value of 0.001, there’s no guarantee that a repeat study will yield a P-value below the traditional 0.05 threshold. This highlights a fundamental truth: statistical significance, as determined by P-values, is not a static property of a phenomenon but rather a probabilistic outcome dependent on the specific data at hand.</p>
</section>
<section id="the-risk-of-false-positives-under-the-null-hypothesis">
<h3>The risk of false positives under the null hypothesis<a class="headerlink" href="#the-risk-of-false-positives-under-the-null-hypothesis" title="Link to this heading">#</a></h3>
<p>While the variability of P-values under a true effect is a concern, an even more insidious issue arises when there is no real effect to be found. In this scenario, the <em>null hypothesis is true</em>, yet random sampling can still lead to statistically significant results. This simulation explores the frequency of such false positives and their implications for drawing reliable conclusions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulation parameters</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>      <span class="c1"># Sample size per group</span>
<span class="n">SD</span> <span class="o">=</span> <span class="mf">5.0</span>    <span class="c1"># Common standard deviation</span>
<span class="n">mean</span> <span class="o">=</span> <span class="mi">10</span>   <span class="c1"># Common mean for both populations</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">2500</span>

<span class="c1"># For reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Store p-values</span>
<span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Run simulations</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
    <span class="n">pop_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">SD</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
    <span class="c1"># Same mean and SD for both populations, same sample size</span>
    <span class="n">pop_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">SD</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">pop_1</span><span class="p">,</span> <span class="n">pop_2</span><span class="p">)</span>
    <span class="n">p_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>

<span class="c1"># Calculate significance proportion</span>
<span class="n">signif_proportion</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">0.05</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;skyblue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
    <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;P = 0.05&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;P-value&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency (Log Scale)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="s2">&quot;Distribution of P-values from 2500 simulations</span><span class="se">\n\</span>
<span class="s2">(null hypothesis true: no mean difference)&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="mf">0.0011</span><span class="p">,</span> <span class="mi">290</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">signif_proportion</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">num_simulations</span><span class="p">)</span><span class="si">}</span><span class="s2"> experiments out of </span><span class="se">\n\</span>
<span class="si">{</span><span class="n">num_simulations</span><span class="si">}</span><span class="s2"> yield p ≤ 0.05 (</span><span class="si">{</span><span class="n">signif_proportion</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span> 
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/dc10c72ee2046b4f107ecd42ade47979fdfdb2eae4882e311690730d1a2b4591.png" src="_images/dc10c72ee2046b4f107ecd42ade47979fdfdb2eae4882e311690730d1a2b4591.png" />
</div>
</div>
<p>In our simulation, where both populations had the same mean, approximately 5% of the experiments yielded P-values below the 0.05 significance threshold. This phenomenon is known as a <strong>Type I error</strong>, or a <strong>false positive</strong>. This aligns with the chosen significance level, highlighting that statistical significance does not guarantee a true effect.</p>
<p>This underscores the importance of interpreting P-values cautiously and considering them within the broader context of the research question, effect size, and potential for replication.</p>
</section>
<section id="the-influence-of-sample-size-on-p-values">
<h3>The influence of sample size on P-values<a class="headerlink" href="#the-influence-of-sample-size-on-p-values" title="Link to this heading">#</a></h3>
<p>The size of the sample (the number of observations in each group) plays a crucial role in determining the outcome of a statistical test. Even with the same underlying effect size and variability, a larger sample size can drastically alter the resulting P-value. Let’s illustrate this with a simulation using the t-distribution’s cumulative distribution function (CDF). Here the number of degrees of freedom equals <span class="math notranslate nohighlight">\(n_1 + n_2 - 2\)</span> which, with <span class="math notranslate nohighlight">\(n_1 = n_2 = n\)</span>, equals <span class="math notranslate nohighlight">\(2 \times n - 2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">t</span>

<span class="c1"># Simulation parameters</span>
<span class="n">mean1</span><span class="p">,</span> <span class="n">mean2</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span>       <span class="c1"># Group means (fixed difference)</span>
<span class="n">mean_diff</span> <span class="o">=</span> <span class="n">mean2</span> <span class="o">-</span> <span class="n">mean1</span>
<span class="n">SD</span> <span class="o">=</span> <span class="mi">5</span>                      <span class="c1"># Standard deviation (fixed)</span>

<span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">31</span><span class="p">):</span>      <span class="c1"># Varying sample sizes</span>
    <span class="c1"># Calculate t-statistic (see previous chapter)</span>
    <span class="n">t_stat</span> <span class="o">=</span> <span class="n">mean_diff</span> <span class="o">/</span> <span class="p">(</span><span class="n">SD</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">p_value</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_stat</span><span class="p">),</span> <span class="n">df</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># Two-tailed p-value</span>
    <span class="n">p_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">31</span><span class="p">)),</span> <span class="n">y</span><span class="o">=</span><span class="n">p_values</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sample Size (per group)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;P-value&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="s2">&quot;Impact of sample size on P-values</span><span class="se">\n\</span>
<span class="s2">(fixed mean difference and standard deviation)&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
    <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;P = 0.05&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/299a9c8a8e02732b573ae186db376c3619dd70e7aa783267d0e9c1038c782cc7.png" src="_images/299a9c8a8e02732b573ae186db376c3619dd70e7aa783267d0e9c1038c782cc7.png" />
</div>
</div>
<p>Of course, we can achieve the same results using both <code class="docutils literal notranslate"><span class="pre">scipy.stats.ttest_ind</span></code> (for the t-test) and directly working with the t-distribution, because the t-test relies on the t-distribution to model the behavior of the test statistic under the null hypothesis (will be discussed in later chapter).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sample data (two groups)</span>
<span class="n">data1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">14</span><span class="p">]</span>
<span class="n">data2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>

<span class="c1"># Method 1: Using scipy.stats.ttest_ind</span>
<span class="n">t_stat</span><span class="p">,</span> <span class="n">p_value_ttest</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">)</span>

<span class="c1"># Method 2: Calculating directly from t-distribution</span>
<span class="n">n1</span><span class="p">,</span> <span class="n">n2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)</span>                <span class="c1"># Sample sizes</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">n1</span> <span class="o">+</span> <span class="n">n2</span> <span class="o">-</span> <span class="mi">2</span>                               <span class="c1"># Degrees of freedom</span>
<span class="n">mean1</span><span class="p">,</span> <span class="n">mean2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data2</span><span class="p">)</span>  <span class="c1"># Sample means</span>
<span class="n">pooled_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
    <span class="p">(</span>   <span class="c1"># Pooled standard deviation</span>
        <span class="p">(</span><span class="n">n1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">n2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span> <span class="o">/</span> <span class="n">df</span>
<span class="p">)</span>
<span class="n">t_stat_manual</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean1</span> <span class="o">-</span> <span class="n">mean2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">pooled_std</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">n2</span><span class="p">))</span>

<span class="c1"># Calculate p-value using t-distribution CDF</span>
<span class="n">p_value_manual</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_stat_manual</span><span class="p">),</span> <span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">))</span> 

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Results using ttest_ind:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t-statistic: </span><span class="si">{</span><span class="n">t_stat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, p-value: </span><span class="si">{</span><span class="n">p_value_ttest</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Results using manual calculation:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t-statistic: </span><span class="si">{</span><span class="n">t_stat_manual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, p-value: </span><span class="si">{</span><span class="n">p_value_manual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Results using ttest_ind:
t-statistic: -4.6677, p-value: 0.0016

Results using manual calculation:
t-statistic: -4.6677, p-value: 0.0016
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-danger-of-ad-hoc-sample-size-decisions-and-cumulative-p-values">
<h3>The danger of ad hoc sample size decisions and cumulative P-values<a class="headerlink" href="#the-danger-of-ad-hoc-sample-size-decisions-and-cumulative-p-values" title="Link to this heading">#</a></h3>
<p>In research, it’s tempting to collect data <em>incrementally</em> and assess statistical significance after each addition. However, this <strong>ad hoc</strong> approach to sample size can lead to misleading conclusions. Let’s simulate how cumulative P-values behave under the null hypothesis, where there’s no true difference between groups.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulation parameters</span>
<span class="n">sample_size_max</span> <span class="o">=</span> <span class="mi">75</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">common_mean</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">common_std_dev</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="c1"># Seaborn color palette</span>
<span class="n">palette</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">n_colors</span><span class="o">=</span><span class="n">num_simulations</span><span class="p">)</span>

<span class="c1"># For reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="c1"># Plotting setup</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>

<span class="c1"># Run simulations and plot</span>
<span class="k">for</span> <span class="n">sim_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
    <span class="c1"># Generate data for two populations with the same mean and SD</span>
    <span class="n">pop_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">common_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">common_std_dev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">sample_size_max</span><span class="p">)</span>
    <span class="n">pop_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">common_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">common_std_dev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">sample_size_max</span><span class="p">)</span>

    <span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">sample_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Iterate over increasing sample sizes</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">pop_1</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">pop_2</span><span class="p">[:</span><span class="n">n</span><span class="p">])</span>
        <span class="n">p_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>

    <span class="c1"># Plot for this simulation using the Seaborn palette</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">p_values</span><span class="p">,</span>
        <span class="c1"># marker=&#39;o&#39;,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">palette</span><span class="p">[</span><span class="n">sim_num</span><span class="p">],</span>
        <span class="c1"># alpha=0.7, </span>
        <span class="c1"># label=f&quot;Simulation {sim_num + 1}&quot;,</span>
    <span class="p">)</span>

<span class="c1"># Add labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
    <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;p = 0.05&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sample size (per group)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;P-value (log scale)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Impact of increasing incrementally sample size on P-values</span><span class="se">\n</span><span class="s2"> </span><span class="se">\</span>
<span class="s2">(</span><span class="si">{</span><span class="n">num_simulations</span><span class="si">}</span><span class="s2"> simulations) - null hypothesis true: no mean difference&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">sample_size_max</span> <span class="o">+</span> <span class="mi">5</span><span class="p">))</span>
<span class="c1"># plt.grid(axis=&#39;y&#39;, linestyle=&#39;--&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a5bd1102475ad2237d3ffb0ee8da82b4914f7d82e9f4b2845ec86c5e1f36ebee.png" src="_images/a5bd1102475ad2237d3ffb0ee8da82b4914f7d82e9f4b2845ec86c5e1f36ebee.png" />
</div>
</div>
<p>Consider the potentially misleading practice of collecting data until a statistically significant result is achieved, then abruptly stopping the experiment. This approach capitalizes on the inherent randomness of sampling. If data collection were to continue beyond the initial “significant” finding, the P-value might fluctuate back into the non-significant range as new data points are added.</p>
<p>This is evident with the first line in our previous simulation (where the curve rises between 13 and 14). However, for other samples, the rise of the P-value back into non-significance could occur later or even never happen, leaving a false sense of confidence in the results. In real-world research, this fluctuation would often remain hidden due to the premature halt of data collection.</p>
<p>This dynamic adjustment of sample size, known as <strong>sequential data analysis</strong>, introduces <strong>bias</strong> and inflates the rate of <strong>false positives</strong> (<strong>Type I errors</strong>). It’s a statistical illusion, creating the misleading impression of a significant effect where none truly exists.</p>
</section>
</section>
<section id="confidence-intervals-and-p-values">
<h2>Confidence intervals and P-values<a class="headerlink" href="#confidence-intervals-and-p-values" title="Link to this heading">#</a></h2>
<p>Confidence intervals offer a valuable alternative to P-values for assessing statistical significance. They provide a <em>range of plausible values for the true population parameter</em> (e.g., mean difference, effect size) with a specified level of confidence (typically 95%). Unlike P-values, which focus solely on rejecting or failing to reject a null hypothesis, confidence intervals offer a more informative picture of the magnitude and uncertainty of an effect.</p>
<p>There’s a direct relationship between confidence intervals and P-values, particularly for two-sided tests:</p>
<ul class="simple">
<li><p><strong>95% CI excludes the null value</strong>: If the 95% confidence interval for a parameter (e.g., mean difference) does not include the null hypothesis value (often 0 for differences or 1 for ratios), then the corresponding P-value will be less than 0.05, indicating a <strong>statistically significant</strong> result.</p></li>
<li><p><strong>95% CI includes the null value</strong>: Conversely, if the 95% confidence interval does contain the null hypothesis value, the P-value will be greater than or equal to 0.05, suggesting that the results are <strong>not statistically significant</strong>.</p></li>
</ul>
<img src="https://ars.els-cdn.com/content/image/1-s2.0-S1063458412007789-gr1.jpg" alt="Statistically and clinically significant effects, measured in arbitrary units on an absolute scale, as evaluated by P-values and confidence intervals" style="width: 600px;"/>
<p>Examples:</p>
<ul class="simple">
<li><p>Comparing means: If the 95% CI for the difference between two means does not include zero, we can conclude a statistically significant difference exists between the groups (P &lt; 0.05).</p></li>
<li><p>Comparing proportions: If the 95% CI for the ratio of two proportions does not include 1.0, we can conclude a statistically significant difference exists (P &lt; 0.05).</p></li>
<li><p>Comparing percentages: If we’re testing whether a set of percentages differs from a hypothesized value of 100, and the 95% CI of the mean of the percentages excludes 100, then the discrepancy is statistically significant.</p></li>
</ul>
<p>Confidence intervals offer several advantages over P-values alone:</p>
<ul class="simple">
<li><p>Effect size estimation: CIs provide an estimate of the magnitude of the effect, not just whether it’s statistically significant.</p></li>
<li><p>Precision assessment: The width of the CI reflects the precision of the estimate. Narrower CIs indicate more precise estimates.</p></li>
<li><p>Decision relevance: CIs help assess the practical significance of the effect by showing the range of plausible values.</p></li>
</ul>
<p>In biostatistics and many other fields, it’s increasingly recommended to report confidence intervals alongside P-values. This provides a more comprehensive and informative view of our findings, allowing for better decision-making based on both statistical significance and the estimated magnitude of the effect.</p>
</section>
<section id="statistical-significance-and-hypothesis-testing">
<h2>Statistical significance and hypothesis testing<a class="headerlink" href="#statistical-significance-and-hypothesis-testing" title="Link to this heading">#</a></h2>
<p>Statistical hypothesis testing offers a standardized framework for drawing conclusions from data. At its core, this approach streamlines the complex process of interpreting results by distilling findings into a <strong>binary outcome</strong>: “statistically significant” or “not statistically significant.” This automation of decision-making can be both a strength and a weakness. In this section, we’ll delve deeper into the principles of hypothesis testing, exploring how it simplifies decision-making, the criteria it relies upon, and the potential pitfalls that arise from oversimplification. We’ll also discuss when and how hypothesis testing can be a valuable tool in the biostatistical arsenal.</p>
<section id="type-i-and-type-ii-errors">
<h3>Type I and type II errors<a class="headerlink" href="#type-i-and-type-ii-errors" title="Link to this heading">#</a></h3>
<p>The binary nature of statistical hypothesis testing (reject H0 or do not reject H0) creates a landscape of potential errors, as our decisions are made under uncertainty. These errors are classified into two distinct types, each with its own consequences:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Reject H0</p></th>
<th class="head"><p>Do not reject H0</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>H0 is true</p></td>
<td><p>Type I error</p></td>
<td><p>(no error)</p></td>
</tr>
<tr class="row-odd"><td><p>H0 is false</p></td>
<td><p>(no error)</p></td>
<td><p>Type II error</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p><strong>Type I error</strong> (<strong>False Positive</strong>): this occurs when we reject the null hypothesis (H0) even though it’s actually true. In other words, we conclude there’s a significant effect when, in reality, it’s due to chance. Think of this as a “false alarm”. In everyday life, we might mistakenly flag a legitimate email as spam or, more seriously, wrongly convict someone of a crime they didn’t commit.</p></li>
<li><p><strong>Type II error</strong> (<strong>False Negative</strong>): This happens when we fail to reject the null hypothesis (H0) when it’s actually false. This means we miss a real effect and incorrectly conclude there’s no significant difference. Consider this a “missed opportunity.” In everyday life, we might mistakenly let a spam email slip into our inbox or, more seriously, wrongly acquit someone who actually committed a crime.</p></li>
</ul>
<p>A third type of error, known as a <strong>Type S</strong> or <strong>Type III error</strong>, occurs when the direction of the effect is incorrectly concluded. In other words, we might find a statistically significant difference, but the direction of that difference is the <em>opposite</em> of what is actually true. Imagine a study that aims to determine if a new drug lowers blood pressure. A Type S error would occur if the study concludes that the drug raises blood pressure when it actually lowers it, even though the statistical test shows a significant difference.</p>
</section>
<section id="interpreting-statistical-significance">
<h3>Interpreting statistical significance<a class="headerlink" href="#interpreting-statistical-significance" title="Link to this heading">#</a></h3>
<section id="the-significance-level-alpha">
<h4>The significance level (<span class="math notranslate nohighlight">\(\alpha\)</span>)<a class="headerlink" href="#the-significance-level-alpha" title="Link to this heading">#</a></h4>
<p>When a statistical test yields a “significant” result, it means the observed effect is unlikely to have occurred by chance alone, given a pre-specified threshold of improbability. This threshold is known as the significance level, denoted by the Greek letter alpha (<span class="math notranslate nohighlight">\(\alpha\)</span>).</p>
<p>Typically, researchers set alpha at 0.05, meaning there’s a 5% chance of incorrectly rejecting the null hypothesis (H0) when it’s actually true (a Type I error). A P-value less than alpha leads to rejecting H0, while a P-value greater than or equal to alpha leads to failing to reject H0.</p>
<p>While a significant result (<span class="math notranslate nohighlight">\(P &lt; \alpha\)</span>) suggests an effect is unlikely due to chance, it doesn’t guarantee the effect is:</p>
<ul class="simple">
<li><p>Large: a small effect can be statistically significant with a large enough sample size.</p></li>
<li><p>Important: statistical significance doesn’t equate to practical or clinical significance.</p></li>
<li><p>Accurate: a significant result could still be a Type I error.</p></li>
</ul>
<p>Now, we can ask ourselves:</p>
<ul class="simple">
<li><p>Assuming the null hypothesis is true, what is the probability of obtaining a P-value that would lead us to incorrectly reject the null hypothesis?</p></li>
<li><p>If we repeatedly conduct experiments where there is no true effect, what proportion of these experiments will mistakenly lead us to conclude that there is a significant effect?</p></li>
</ul>
<p>In practical terms, <span class="math notranslate nohighlight">\(\alpha\)</span> can also be understood through repeated experimentation. Imagine conducting numerous experiments where the null hypothesis (H0) is true. Let <span class="math notranslate nohighlight">\(A\)</span> represent the number of times we incorrectly reject H0 (Type I errors), and let <span class="math notranslate nohighlight">\(B\)</span> represent the number of times we correctly fail to reject H0:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head text-center"><p>Reject H0</p></th>
<th class="head text-center"><p>Do not reject H0</p></th>
<th class="head text-center"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>H0 is true</p></td>
<td class="text-center"><p>A (Type I)</p></td>
<td class="text-center"><p>B</p></td>
<td class="text-center"><p>A + B</p></td>
</tr>
<tr class="row-odd"><td><p>H0 is false</p></td>
<td class="text-center"><p>C</p></td>
<td class="text-center"><p>D (Type II)</p></td>
<td class="text-center"><p>C + D</p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td class="text-center"><p>A + C</p></td>
<td class="text-center"><p>B + D</p></td>
<td class="text-center"><p>A+B+C+D</p></td>
</tr>
</tbody>
</table>
<p>The significance level only considers analyses where <em>H0 is true</em> (first row of the table). Of all experiments (<span class="math notranslate nohighlight">\(A+B\)</span>), the number of times H0 is rejected equals <span class="math notranslate nohighlight">\(A\)</span>, so that:</p>
<div class="math notranslate nohighlight">
\[\alpha = \frac{A}{A+B}\]</div>
<p>In other words, <span class="math notranslate nohighlight">\(\alpha\)</span> is the proportion of experiments where we mistakenly reject H0 when it’s actually true. This highlights a key point: even when there is no real effect, a certain percentage of studies will yield statistically significant results simply due to random chance.</p>
</section>
<section id="the-false-positive-report-probability-fprp">
<h4>The False Positive Report Probability (FPRP)<a class="headerlink" href="#the-false-positive-report-probability-fprp" title="Link to this heading">#</a></h4>
<p>While <span class="math notranslate nohighlight">\(\alpha\)</span> focuses on the risk of a Type I error within a single experiment, the False Positive Report Probability (FPRP) takes a broader perspective. It considers the probability that a statistically significant finding in a research report actually represents a false positive result.</p>
<p>The FPRP acknowledges that research doesn’t exist in isolation. In practice, multiple studies are often conducted to investigate a particular phenomenon. Some of these studies might yield significant results by chance (Type I errors). The FPRP asks: given a significant finding in a published report, what’s the probability that this result is actually a false positive?</p>
<p>The FPRP only considers analyses that <strong>reject H0</strong> (first column of the table). Of all these studies that report a statistically significant result (reject H0) <span class="math notranslate nohighlight">\(A+C\)</span>, and with <span class="math notranslate nohighlight">\(C\)</span> the number of studies that report a significant result and where H0 is truly false (true positive), the number in which H0 is true equals <span class="math notranslate nohighlight">\(A\)</span>, therefore:</p>
<div class="math notranslate nohighlight">
\[\text{FPRP} = \frac{A}{A+C}\]</div>
<p>In other words, the FPRP is the proportion of significant results that are actually false positives.</p>
</section>
</section>
<section id="the-prior-probability-influences-the-fprp">
<h3>The prior probability influences the FPRP<a class="headerlink" href="#the-prior-probability-influences-the-fprp" title="Link to this heading">#</a></h3>
<p>The False Positive Report Probability (FPRP) isn’t a fixed value; it varies depending on several factors, including the significance level (<span class="math notranslate nohighlight">\(\alpha\)</span>) and statistical power of the study. However, an often overlooked but critical factor is the <strong>prior probability</strong> of a true effect. This reflects our <em>existing knowledge</em> or <em>belief</em> about the likelihood of the phenomenon under investigation before we even conduct the experiment.</p>
<p>Intuitively, if we’re testing a hypothesis that seems highly implausible based on prior research or scientific understanding, even a statistically significant result should be met with greater skepticism. In contrast, a significant finding for a well-established phenomenon is more likely to be a true positive.</p>
<p>The table below illustrates how the FPRP changes depending on the prior probability of a true effect, assuming a fixed significance level and power.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Prior probability</p></th>
<th class="head"><p>FPRP as P&lt;0.05</p></th>
<th class="head"><p>FPRP as 0.045&lt;P&lt;0.05</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0%</p></td>
<td><p>100%</p></td>
<td><p>100%</p></td>
</tr>
<tr class="row-odd"><td><p>1%</p></td>
<td><p>86%</p></td>
<td><p>97%</p></td>
</tr>
<tr class="row-even"><td><p>10%</p></td>
<td><p>36%</p></td>
<td><p>78%</p></td>
</tr>
<tr class="row-odd"><td><p>50%</p></td>
<td><p>5.9%</p></td>
<td><p>27%</p></td>
</tr>
<tr class="row-even"><td><p>100%</p></td>
<td><p>0%</p></td>
<td><p>0%</p></td>
</tr>
</tbody>
</table>
<p>Let’s explore these scenarios to understand the nuanced relationship between FPRP and the broader scientific context.</p>
<section id="prior-probability-1">
<h4>Prior probability = 1%<a class="headerlink" href="#prior-probability-1" title="Link to this heading">#</a></h4>
<p>Consider a scenario where we’re testing 1,000 new drug compounds, but prior research indicates that the probability of any single drug being successful is a mere 1%. What can we expect in terms of both true and false positive findings?</p>
<p><em>We need to consider not just the risk of false positives, but also the chance of missing a truly effective drug (a false negative). This is where the concept of <strong>power</strong> comes in. We will discuss it in more details in another section, but briefly, power is the probability that our experiment will correctly identify a truly effective drug as significant. In this case, with a 1% prior probability, a high-powered study is crucial. If our study has low power, we might miss out on most of the effective drugs, even if they exist.</em></p>
<ul class="simple">
<li><p>Of 1000 drugs screened we expect 10 (1%) that really work</p></li>
<li><p>Of the 10 drugs that really work we expect to obtain a statistically significant result in 8 (80 % power)</p></li>
<li><p>Of the 990 drugs that are really ineffective we expect to obtain a statistically significant result in 5% (<span class="math notranslate nohighlight">\(\alpha\)</span> set to 0.05), i.e. <span class="math notranslate nohighlight">\(5\% \times 990 = 49\)</span> false positive</p></li>
<li><p>Of 1000 tests of different drugs we therefore expect to obtain statistically significant difference in <span class="math notranslate nohighlight">\(8+49=57\)</span></p></li>
<li><p>The FPRP equals <span class="math notranslate nohighlight">\(49/57=86%\)</span></p></li>
</ul>
<div class="highlight-markdown notranslate"><div class="highlight"><pre><span></span>            8 have P&lt;0.05 (80% power)
           /
       10 work (prior probability=50%)
      /    \
     /      2 have P&gt;0.05
1000 drugs
     \          49 have P&lt;0.05 (5% significance level)
      \        /
       990 don&#39;t work
               \
                941 have P&gt;0.05
</pre></div>
</div>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head text-center"><p>Statistically significant</p></th>
<th class="head text-center"><p>Not significant</p></th>
<th class="head text-center"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Drug is ineffective</p></td>
<td class="text-center"><p>49</p></td>
<td class="text-center"><p>941</p></td>
<td class="text-center"><p>990</p></td>
</tr>
<tr class="row-odd"><td><p>Drug is effective</p></td>
<td class="text-center"><p>8</p></td>
<td class="text-center"><p>2</p></td>
<td class="text-center"><p>10</p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td class="text-center"><p>57</p></td>
<td class="text-center"><p>943</p></td>
<td class="text-center"><p>1000</p></td>
</tr>
</tbody>
</table>
<p>With such a low prior probability of success (1%), conducting this experiment with a conventional significance level of 5% (<span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>) is likely to yield a high rate of false positives. This means that many of the drugs identified as ‘significant’ would actually be ineffective. To ensure the validity and usefulness of this drug screening, a much stricter significance level, such as 0.1% (<span class="math notranslate nohighlight">\(\alpha = 0.001\)</span>), may be necessary. This stricter threshold would reduce the risk of falsely identifying ineffective drugs as promising candidates, but it would also increase the risk of missing truly effective drugs (Type II errors). Balancing these risks requires careful consideration of the specific context, costs, and potential benefits of the drug screening program.</p>
</section>
<section id="prior-probability-50">
<h4>Prior probability = 50%<a class="headerlink" href="#prior-probability-50" title="Link to this heading">#</a></h4>
<p>Even perfectly performed experiments are less reproducible than most expect, and many statistically significant results are false positives in situations where false positive results are likely, e.g. with low prior probability in observational studies or when multiple comparisons are made. In contrast, when the prior probability of a true effect is higher, such as 50%, the likelihood of a significant result being a true positive increases substantially, with here the FPRP equals <span class="math notranslate nohighlight">\(25/425 = 5.9%\)</span>, even if the study is underpowered:</p>
<div class="highlight-markdown notranslate"><div class="highlight"><pre><span></span>             400 have P&lt;0.05 (80% power)
            /
       500 work (prior probability=50%)
      /     \
     /       100 have P&gt;0.05
1000 drugs
     \          25 have P&lt;0.05 (5% significance level)
      \        /
       500 don&#39;t work
               \
                475 have P&gt;0.05
</pre></div>
</div>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head text-center"><p>Statistically significant</p></th>
<th class="head text-center"><p>Not significant</p></th>
<th class="head text-center"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Drug is ineffective</p></td>
<td class="text-center"><p>25</p></td>
<td class="text-center"><p>475</p></td>
<td class="text-center"><p>500</p></td>
</tr>
<tr class="row-odd"><td><p>Drug is effective</p></td>
<td class="text-center"><p>400</p></td>
<td class="text-center"><p>100</p></td>
<td class="text-center"><p>500</p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td class="text-center"><p>425</p></td>
<td class="text-center"><p>943</p></td>
<td class="text-center"><p>1000</p></td>
</tr>
</tbody>
</table>
<p>Furthermore, if an experiment has low power to find the difference/effect we are looking for, we are likely to end up with a result that is not statistically significant even if the effect is real. And if the low power study does reach a conclusion that the effect is statistically significant, the results are hard to interpret because the FPRP will be high, and the effect size observed in that study is likely to be larger than the actual effect size because only large observed effects (even if due to chance) will yield a P-value less than 0.05. Experiments designed with low power cannot be very informative, and it should not be surprising when they cannot be reproduced.</p>
</section>
</section>
</section>
<section id="the-challenge-of-multiple-comparisons">
<h2>The challenge of multiple comparisons<a class="headerlink" href="#the-challenge-of-multiple-comparisons" title="Link to this heading">#</a></h2>
<p>When conducting multiple statistical tests within a study, the probability of encountering at least one false positive result increases significantly. This is known as the multiple comparisons problem.</p>
<p>Each statistical test carries an inherent risk of a Type I error (false positive), and conducting multiple tests compounds this risk. Think of it like rolling the dice multiple times - the more you roll, the higher the chance of getting a “lucky” (but misleading) significant result.</p>
<p>The multiple comparisons problem can lead to false discoveries and unreliable conclusions. In this section, we’ll explore the causes and consequences of this issue, and we’ll introduce statistical methods to mitigate the inflated risk of false positives.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>  <span class="c1"># Number of comparisons (1 to 59)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.95</span><span class="o">**</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Probability of at least one significant result (%)</span>

<span class="c1"># Plot</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
    <span class="mi">5</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;α = 0.05&quot;</span><span class="p">)</span>  <span class="c1"># Add alpha level line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="s2">&quot;Probability of obtaining at least one statistically significant result by chance&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of comparisons&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Probability (%)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/390887bfca56d31001c7ebf1554571acce7326cd5461d4a2e962867b4ff785af.png" src="_images/390887bfca56d31001c7ebf1554571acce7326cd5461d4a2e962867b4ff785af.png" />
</div>
</div>
<p>This plot illustrate the concept of the multiple comparisons problem. The x-axis represents the number of comparisons made in a study, while the y-axis shows the probability of obtaining at least one statistically significant result purely by chance.</p>
<p>As we can see, even with a single comparison (x=1) and an alpha level of 0.05, there’s a 5% chance of getting a false positive. As the number of comparisons increases, this probability rapidly rises. For instance, with just 14 comparisons, the probability of at least one false positive exceeds 50%!</p>
<section id="the-bonferroni-correction">
<h3>The Bonferroni correction<a class="headerlink" href="#the-bonferroni-correction" title="Link to this heading">#</a></h3>
<p>The simplest approach to controlling the familywise error rate (FWER) is the <strong>Bonferroni correction</strong>, which involves dividing the desired overall significance level (<span class="math notranslate nohighlight">\(\alpha\)</span>) by the number of comparisons (<span class="math notranslate nohighlight">\(K\)</span>): <span class="math notranslate nohighlight">\(\alpha_\text{adjusted} = \alpha / K\)</span>. This adjustment ensures that if all the null hypotheses are true, the probability of observing at least one false positive (Type I error) among all <span class="math notranslate nohighlight">\(K\)</span> comparisons is at most <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>The Bonferroni correction does not guarantee exactly a 95% chance of seeing no significant results if all null hypotheses are true. It provides a more conservative upper bound on this probability, making it less likely to make any false discoveries. However, this comes at the cost of reduced power to detect true effects (increased risk of Type II errors).</p>
</section>
<section id="the-false-discovery-rate-fdr">
<h3>The False Discovery Rate (FDR)<a class="headerlink" href="#the-false-discovery-rate-fdr" title="Link to this heading">#</a></h3>
<p>The False Discovery Rate (FDR) offers a more flexible alternative to the Bonferroni correction for addressing multiple comparisons. Instead of strictly controlling the probability of any false positives, the FDR aims to control the proportion of false positives among the discoveries we make. The FDR addresses these key questions:</p>
<ul class="simple">
<li><p>If a comparison is deemed significant (P-value below a threshold), what’s the chance it’s actually a false positive (H0 is true)?</p></li>
<li><p>Of all significant findings, what fraction are expected to be false positives?</p></li>
</ul>
<p>The desired FDR, often denoted as <span class="math notranslate nohighlight">\(Q\)</span>, is the acceptable proportion of false discoveries. For example, with Q = 10%, we aim for at least 90% of our significant results to be true positives.</p>
<p>A widely used method to control the FDR is the <strong>Benjamini-Hochberg</strong> procedure. It sets dynamic thresholds for each comparison, depending on its rank among the P-values. For example, if we want an FDR of 5% across 100 comparisons, we would:</p>
<ol class="arabic simple">
<li><p>Rank P-values: Order the P-values from smallest to largest (<span class="math notranslate nohighlight">\(P(1), P(2), ..., P(100)\)</span>).</p></li>
<li><p>Calculate Thresholds: For each P-value <span class="math notranslate nohighlight">\(P(i)\)</span>, calculate a threshold <span class="math notranslate nohighlight">\(T(i)\)</span> using this formula <span class="math notranslate nohighlight">\(T(i) = (i / m) * Q\)</span>, with <span class="math notranslate nohighlight">\(i\)</span> the rank of the P-value (<span class="math notranslate nohighlight">\(1, 2, ..., 100\)</span>), and <span class="math notranslate nohighlight">\(m\)</span> the total number of comparisons (<span class="math notranslate nohighlight">\(100\)</span>)</p></li>
<li><p>Compare and Decide: If <span class="math notranslate nohighlight">\(P(i) \leq T(i)\)</span>, declare the comparison as significant, if <span class="math notranslate nohighlight">\(P(i) \gt T(i)\)</span>, declare the comparison as not significant.</p></li>
</ol>
<p>For example, for the smallest P-value (<span class="math notranslate nohighlight">\(i = 1\)</span>), <span class="math notranslate nohighlight">\(T(1) = (1 / 100) \times 0.05 = 0.0005\)</span>, so, the smallest P-value would be considered significant if it’s less than or equal to 0.0005. The second smallest P-value’s threshold would be <span class="math notranslate nohighlight">\((2/100) \times 0.05 = 0.001\)</span>. For the largest P-value (<span class="math notranslate nohighlight">\(i = 100\)</span>), <span class="math notranslate nohighlight">\(T(100) = (100 / 100) \times 0.05 = 0.05\)</span>, so the largest P-value is compared directly to the overall alpha level (0.05).</p>
<p>This approach ensures that, on average, no more than 5% of the significant results we identify will be false positives.</p>
</section>
</section>
<section id="statistical-power">
<h2>Statistical power<a class="headerlink" href="#statistical-power" title="Link to this heading">#</a></h2>
<section id="definition">
<h3>Definition<a class="headerlink" href="#definition" title="Link to this heading">#</a></h3>
<p>The <strong>power</strong> of an experimental design answers this crucial question: If there truly is an effect of a specified size, and the experiment is repeated many times, what proportion of those experiments will yield a statistically significant result? This concept is essential for determining sample size requirements and for interpreting results that are not statistically significant.</p>
<p>While the definition of a P-value starts with the assumption that the null hypothesis (H0) is true, we often want to know what happens when H0 is false, and there is a real effect. Statistical power addresses this by asking: if a specific effect truly exists (e.g., a difference between groups, a relative risk, a correlation), what is the probability of obtaining a statistically significant result in a single experiment?</p>
<p>Power is the proportion of hypothetical experiments (if we were to repeat our study many times) in which we would expect to correctly reject the null hypothesis, given that the alternative hypothesis (the effect we’re looking for) is actually true</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-center"><p>Decision: Reject H0</p></th>
<th class="head text-center"><p>Decision: Fail to reject H0</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>No effect (H0 True)</p></td>
<td class="text-center"><p>Type I error (<span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
<td class="text-center"><p>Correct decision (<span class="math notranslate nohighlight">\(1 - \alpha\)</span>)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Effect exists</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(1 - \beta\)</span></p></td>
<td class="text-center"><p>Type II error (<span class="math notranslate nohighlight">\(\beta\)</span>)</p></td>
</tr>
</tbody>
</table>
<img src="https://static.wingify.com/gcp/uploads/sites/3/2020/12/graphical-representation-of-type-1-and-type-2-errors.png" alt="type I and type II errors seen in H0 and H1 distributions" style="width: 500px;"/>
<p>Since <span class="math notranslate nohighlight">\(\beta\)</span> represents the probability of one outcome (failing to reject a false H0), <span class="math notranslate nohighlight">\(1 - \beta\)</span> represents the probability of the complementary outcome (correctly rejecting a false H0). This complementary outcome is what we define as statistical power.</p>
<p>In general, experiments with high statistical power are characterized by:</p>
<ul class="simple">
<li><p>Large sample sizes: more data points provide greater precision and reduce the impact of random variation.</p></li>
<li><p>Large effect sizes: the greater the magnitude of the effect we’re looking for, the easier it is to detect.</p></li>
<li><p>Low variability (scatter): less variability within our data makes it easier to distinguish a true signal from noise.</p></li>
</ul>
<p>And when there is no true effect (the null hypothesis is true), power tends to be as low as the chosen significance level (often 5%). However, as the true effect size increases (while keeping sample size and variability constant), power increases in a sigmoidal (S-shaped) curve, theoretically reaching 100%. This means that with a large enough effect, we’re virtually guaranteed to detect it.</p>
</section>
<section id="interlude-the-effect-size">
<h3>Interlude - The effect size<a class="headerlink" href="#interlude-the-effect-size" title="Link to this heading">#</a></h3>
<p>Statistical significance (<span class="math notranslate nohighlight">\(P \lt \alpha\)</span>) tells us if an observed effect is likely not due to chance. However, it doesn’t reveal how large or meaningful the effect is. This is where <strong>effect size</strong> comes in. Effect size is a standardized measure of the magnitude of a phenomenon, allowing us to compare the strength of effects across different studies, even when they use different scales or measures.</p>
<p>One of the most common measures of effect size for comparing two groups is <strong>Cohen’s <span class="math notranslate nohighlight">\(d\)</span></strong>.  It quantifies the difference between two group means in terms of their <em>pooled standard deviation</em>:</p>
<div class="math notranslate nohighlight">
\[d = \frac{\overline{X} - \overline{Y}}{\text{pooled standard deviation}} = \frac{\overline{X} - \overline{Y}}{\sqrt{\frac{(n_{1} - 1)\sigma_{1}^{2} + (n_{2} - 1) \sigma_{2}^{2}}{n1 + n2 - 2}}}\]</div>
<p>For a comparison between 2 groups with the <em>same sample size</em> (<span class="math notranslate nohighlight">\(n_1 = n_2 = n\)</span>), and the <em>same variance</em> <span class="math notranslate nohighlight">\(\sigma^2\)</span> (this is often referred to as the homoscedasticity or equal variance assumption, e.g., in controlled experiments, matched pairs designs or random sampling from a homogeneous population), the equation can be simplified to</p>
<div class="math notranslate nohighlight">
\[d = \frac{\overline{X} - \overline{Y}}{\sqrt{\frac{2 (n - 1) \sigma^2}{2 (n - 1)}}} = \frac{\overline{X} - \overline{Y}}{\sigma}\]</div>
<p>Cohen’s d values are typically interpreted as follows:</p>
<ul class="simple">
<li><p>Small effect: <span class="math notranslate nohighlight">\(d = 0.2\)</span></p></li>
<li><p>Medium effect: <span class="math notranslate nohighlight">\(d = 0.5\)</span></p></li>
<li><p>Large effect: <span class="math notranslate nohighlight">\(d = 0.8\)</span></p></li>
</ul>
</section>
<section id="relationship-between-power-effect-size-sample-size-and-variability">
<h3>Relationship between power, effect size, sample size, and variability<a class="headerlink" href="#relationship-between-power-effect-size-sample-size-and-variability" title="Link to this heading">#</a></h3>
<p>Statistical power is not a single, fixed value. It’s a dynamic concept influenced by several key factors: the size of the effect we’re looking for, the amount of variability in the data, and the sample size of our study. In this section, we’ll use simulations to visualize how these factors interact and impact the power of a statistical test.</p>
<section id="one-sample-t-test">
<h4>One-sample t-test<a class="headerlink" href="#one-sample-t-test" title="Link to this heading">#</a></h4>
<p><strong>Statistical power calculations</strong> can be used to determine the sample size needed to detect if a sample mean is significantly different from a known population mean or a hypothesized value, for example how many patients need to be enrolled in a study to determine if a new drug significantly reduces blood pressure compared to a baseline value. It can also be used to determine the number of pairs needed to detect a significant difference between two measurements taken on the same subjects, e.g., how many participants should be included in a study to assess if a new therapy significantly improves cognitive function compared to their pre-treatment scores.</p>
<p>To put these power calculations into practice, we can leverage Python’s <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> library, which provides convenient tools for power analysis. The <a class="reference external" href="https://www.statsmodels.org/stable/generated/statsmodels.stats.power.TTestPower.html"><code class="docutils literal notranslate"><span class="pre">TTestPower</span></code></a> function within this library allows us to estimate the required sample size for <em>one-sample and paired t-tests</em>, given the desired power, significance level, and anticipated effect size. For example, a pharmaceutical company aims to determine if their new drug can significantly lower cholesterol levels. They plan to conduct a clinical trial with a one-sample t-test, comparing the mean cholesterol level of patients taking the new drug to the known population mean.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.stats.power</span> <span class="kn">import</span> <span class="n">TTestPower</span>

<span class="c1"># Simulation parameters</span>
<span class="n">diff</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10</span>            <span class="c1"># Mean level **decrease** compared to known population mean</span>
<span class="n">std_devs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">]</span>   <span class="c1"># Different levels of standard deviation</span>
<span class="n">power</span> <span class="o">=</span> <span class="mf">0.8</span>           <span class="c1"># Desired power (80%)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>          <span class="c1"># Significance level</span>
<span class="n">max_sample_size</span> <span class="o">=</span> <span class="mi">151</span> <span class="c1"># Sample sizes to consider</span>

<span class="c1"># Calculate power curves</span>
<span class="n">power_analysis</span> <span class="o">=</span> <span class="n">TTestPower</span><span class="p">()</span>

<span class="n">power_curves</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">sd</span> <span class="ow">in</span> <span class="n">std_devs</span><span class="p">:</span>
    <span class="n">power_curves</span><span class="p">[</span><span class="n">sd</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">power_analysis</span><span class="o">.</span><span class="n">power</span><span class="p">(</span>
            <span class="n">effect_size</span><span class="o">=</span><span class="n">diff</span><span class="o">/</span><span class="n">sd</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">nobs</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
            <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;smaller&#39;</span><span class="p">,</span>
        <span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_sample_size</span><span class="p">)]</span>  <span class="c1"># Start from 2 observations</span>
    
<span class="c1"># Calculate required sample sizes for each SD</span>
<span class="n">required_sample_sizes</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">sd</span> <span class="ow">in</span> <span class="n">std_devs</span><span class="p">:</span>
    <span class="n">required_sample_sizes</span><span class="p">[</span><span class="n">sd</span><span class="p">]</span> <span class="o">=</span> <span class="n">power_analysis</span><span class="o">.</span><span class="n">solve_power</span><span class="p">(</span>
        <span class="n">effect_size</span><span class="o">=</span><span class="n">diff</span><span class="o">/</span><span class="n">sd</span><span class="p">,</span>
        <span class="n">power</span><span class="o">=</span><span class="n">power</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
        <span class="n">nobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># this is the one we want to solve</span>
        <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;smaller&#39;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="k">for</span> <span class="n">sd</span><span class="p">,</span> <span class="n">powers</span> <span class="ow">in</span> <span class="n">power_curves</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_sample_size</span><span class="p">),</span>
        <span class="n">powers</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">fr</span><span class="s1">&#39;$\sigma$ = </span><span class="si">{</span><span class="n">sd</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="n">power</span><span class="p">,</span>
    <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Desired power (</span><span class="si">{</span><span class="n">power</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">sd</span><span class="p">,</span> <span class="n">powers</span> <span class="ow">in</span> <span class="n">required_sample_sizes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
        <span class="n">required_sample_sizes</span><span class="p">[</span><span class="n">sd</span><span class="p">],</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
        <span class="c1"># label=f&#39;N for power=0.8, SD={sd}&#39;,</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sample size&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Statistical power ($1 - \beta$)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Power curves for one-sample t-test</span><span class="se">\n</span><span class="s2">(mean decrease = </span><span class="si">{</span><span class="n">diff</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># plt.grid(axis=&#39;y&#39;, linestyle=&#39;--&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_sample_size</span><span class="o">+</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/043f92293bd7e07afb1a39a1e0cf56373fdd679e33e8dded3a0cd519a93012a6.png" src="_images/043f92293bd7e07afb1a39a1e0cf56373fdd679e33e8dded3a0cd519a93012a6.png" />
</div>
</div>
<p>This plot illustrates how increasing standard deviation (greater variability in the data) lowers the statistical power for a given sample size and effect size. To maintain a desired level of power when dealing with higher variability, we need to increase the sample size.</p>
<p>The plot is also a valuable tool for planning studies. It helps us determine the appropriate sample size needed to detect a specific effect with a desired level of confidence, given the expected variability in the data. If our study didn’t find a significant result, this plot can help us assess whether it was due to a lack of a true effect or simply a lack of power (insufficient sample size). Finally, by understanding the relationship between power, sample size, and variability, we can allocate the research resources more efficiently, avoiding underpowered studies that are unlikely to yield meaningful results.</p>
</section>
<section id="unpaired-t-test">
<h4>Unpaired t-test<a class="headerlink" href="#unpaired-t-test" title="Link to this heading">#</a></h4>
<p>Effect size and variance are inversely related. As seen just above, Cohen’s d is the standardized difference between two group means, divided by the pooled standard deviation (which is a measure of variability).</p>
<p>Power calculations for independent t-tests serve a crucial role in experimental design and interpretation of results. Their primary purpose is to determine the minimum sample size needed in each group to detect a statistically significant difference between the means of two independent groups, with a specified level of confidence. In the following example, we the <a class="reference external" href="https://www.statsmodels.org/stable/generated/statsmodels.stats.power.TTestIndPower.html"><code class="docutils literal notranslate"><span class="pre">TTestIndPower</span></code></a> function for analyzing statistical power in the context of <em>independent two-sample t-tests</em>. In particular, the <code class="docutils literal notranslate"><span class="pre">plot_power</span></code> function within provides a convenient way to visualize the relationship between power, the effect size and sample size in a <em>two-sample t-test</em> scenario.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.stats.power</span> <span class="kn">import</span> <span class="n">TTestIndPower</span>

<span class="c1"># Analysis setup</span>
<span class="n">power_analysis</span> <span class="o">=</span> <span class="n">TTestIndPower</span><span class="p">()</span>
<span class="n">effect_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">401</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">power</span> <span class="o">=</span> <span class="mf">0.8</span>  <span class="c1"># Desired power (80%)</span>

<span class="c1"># Calculate required sample sizes for each SD</span>
<span class="n">required_sample_sizes</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">es</span> <span class="ow">in</span> <span class="n">effect_sizes</span><span class="p">:</span>
    <span class="n">required_sample_sizes</span><span class="p">[</span><span class="n">es</span><span class="p">]</span> <span class="o">=</span> <span class="n">power_analysis</span><span class="o">.</span><span class="n">solve_power</span><span class="p">(</span>
        <span class="n">effect_size</span><span class="o">=</span><span class="n">es</span><span class="p">,</span>
        <span class="n">power</span><span class="o">=</span><span class="n">power</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
        <span class="c1"># parameters are a little different than in TTestPower</span>
        <span class="n">ratio</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># ratio nobs2 over nobs1</span>
        <span class="n">nobs1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># this is the one we want to solve</span>
        <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;two-sided&#39;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Plotting with plot_power function</span>
<span class="n">power_analysis</span><span class="o">.</span><span class="n">plot_power</span><span class="p">(</span>
    <span class="n">dep_var</span><span class="o">=</span><span class="s1">&#39;nobs&#39;</span><span class="p">,</span>
    <span class="n">nobs</span><span class="o">=</span><span class="n">sample_sizes</span><span class="p">,</span>
    <span class="n">effect_size</span><span class="o">=</span><span class="n">effect_sizes</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="n">power</span><span class="p">,</span>
    <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Desired power (</span><span class="si">{</span><span class="n">power</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">es</span><span class="p">,</span> <span class="n">powers</span> <span class="ow">in</span> <span class="n">required_sample_sizes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span>
        <span class="n">required_sample_sizes</span><span class="p">[</span><span class="n">es</span><span class="p">],</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
        <span class="c1"># label=f&#39;N for power=0.8, SD={sd}&#39;,</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size (per group)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Power ($1 - \beta$)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">)</span><span class="o">+</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Power analysis for independent t-test&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/df16f1dca9feba53931e23bd51c5ab4804c9febb03b500b800690833ee272e94.png" src="_images/df16f1dca9feba53931e23bd51c5ab4804c9febb03b500b800690833ee272e94.png" />
</div>
</div>
<p>Each curve corresponds to a different effect size, showing how power increases with sample size for that particular effect. The curves have a characteristic sigmoidal (S-shaped) pattern, reflecting how power initially increases rapidly with sample size and then levels off. Finally, larger effect sizes lead to steeper curves, meaning they require smaller sample sizes to achieve the same level of power.</p>
<p>In conclusion, as the sample size per group increases, so does the power to detect a statistically significant difference between the means. This means <em>larger samples are more likely to identify a true effect</em>, as demonstrated in the figure below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Parameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>                          <span class="c1"># Significance level</span>
<span class="n">power</span> <span class="o">=</span> <span class="mf">0.8</span>                           <span class="c1"># Desired power</span>
<span class="n">sd</span> <span class="o">=</span> <span class="mi">10</span>                               <span class="c1"># Standard deviation</span>
<span class="n">mean_difference</span> <span class="o">=</span> <span class="mi">5</span>                   <span class="c1"># Mean difference tested</span>
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># Range of sample sizes per group</span>

<span class="c1"># Power analysis for subplot 1</span>
<span class="n">power_analysis</span> <span class="o">=</span> <span class="n">TTestIndPower</span><span class="p">()</span>
<span class="n">powers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">power_analysis</span><span class="o">.</span><span class="n">power</span><span class="p">(</span>
        <span class="n">effect_size</span><span class="o">=</span><span class="n">mean_difference</span><span class="o">/</span><span class="n">sd</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
        <span class="n">nobs1</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
        <span class="n">ratio</span><span class="o">=</span><span class="mf">1.0</span>
    <span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">]</span>

<span class="c1"># Calculate mean differences for subplot 2</span>
<span class="n">mean_diffs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">power_analysis</span><span class="o">.</span><span class="n">solve_power</span><span class="p">(</span>
        <span class="n">effect_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">power</span><span class="o">=</span><span class="n">power</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
        <span class="n">nobs1</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
        <span class="n">ratio</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="c1"># multiply the solved effect size by SD to get the mean difference</span>
    <span class="p">)</span><span class="o">*</span><span class="n">sd</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">]</span>

<span class="c1"># Plotting</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="c1"># 1 row, 2 columns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="c1"># Subplot 1: Power vs. Sample Size</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">sample_sizes</span><span class="p">,</span> <span class="n">powers</span><span class="p">,</span>
    <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size per group&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Power ($1 - \beta$)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Power vs. sample size</span><span class="se">\n</span><span class="s1"> </span><span class="se">\</span>
<span class="s1">(es=</span><span class="si">{</span><span class="n">mean_difference</span><span class="o">/</span><span class="n">sd</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, $</span><span class="se">\\</span><span class="s1">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">, $</span><span class="se">\\</span><span class="s1">sigma$=</span><span class="si">{</span><span class="n">sd</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1"># axes[0].grid(axis=&#39;y&#39;, linestyle=&#39;--&#39;)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">)</span><span class="o">+</span><span class="mi">5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>

<span class="c1"># Subplot 2: Mean Difference vs. Sample Size</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">sample_sizes</span><span class="p">,</span> <span class="n">mean_diffs</span><span class="p">,</span>
    <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size per group&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Detectable mean difference&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s1">&#39;Detectable mean difference vs. sample size</span><span class="se">\n</span><span class="s1"> </span><span class="se">\</span>
<span class="s1">(Power=</span><span class="si">{</span><span class="n">power</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">%, $</span><span class="se">\\</span><span class="s1">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">, $</span><span class="se">\\</span><span class="s1">sigma$=</span><span class="si">{</span><span class="n">sd</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1"># axes[1].grid(axis=&#39;y&#39;, linestyle=&#39;--&#39;)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">)</span><span class="o">+</span><span class="mi">5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b661ffd8ccf1ab2a48ee99219dbe818396a77c9d7341253436f196b35a4826bd.png" src="_images/b661ffd8ccf1ab2a48ee99219dbe818396a77c9d7341253436f196b35a4826bd.png" />
</div>
</div>
</section>
<section id="the-choice-of-80-power">
<h4>The choice of 80% power<a class="headerlink" href="#the-choice-of-80-power" title="Link to this heading">#</a></h4>
<p>The horizontal dashed line at 0.8 represents the commonly desired power level of 80%. The vertical dashed lines intersect the power curves at the points where 80% power is achieved. These intersection points reveal the minimum sample size required per group for each standard deviation to reliably detect the specified effect size.</p>
<p>The 80% power standard emerged as a compromise between Type I and Type II errors. In the early days of statistical practice, limited resources and computational power made it difficult to achieve very high power. A power of 80% was seen as a reasonable balance, offering a good chance of detecting a real effect without requiring excessively large sample sizes.</p>
<p>Now, setting power too low (e.g., 60%) increases the risk of Type II errors (e.g., 40%), meaning we’re more likely to miss a true effect. On the other hand, striving for very high power (e.g., 99%) often demands much larger sample sizes, which may not be feasible or ethical in many research settings. If the consequences of a Type II error (missing a real effect) are severe, we might opt for a higher power level, even if it means a larger sample size. For example, in a clinical trial for a life-saving drug, a higher power (e.g., 90%) might be preferred.</p>
<p>80% power strikes a middle ground, offering a reasonable probability of detecting an effect while keeping sample size requirements manageable. In early-stage research or pilot studies, a lower power level (e.g., 70%) might be acceptable, as the goal is often to gather preliminary evidence rather than draw definitive conclusions. Also, if we have strong <em>prior evidence suggesting the presence of an effect</em>, we might be comfortable with lower power. Don’t to forget that if we expect a very large effect size, we might be able to achieve adequate power with a smaller sample size. However, it’s still important to conduct a power analysis to confirm this.</p>
</section>
<section id="paired-t-test">
<h4>Paired t-test<a class="headerlink" href="#paired-t-test" title="Link to this heading">#</a></h4>
<p>Let’s dive into a concrete example of power calculation for a <em>paired t-test</em> using the <a class="reference external" href="https://pingouin-stats.org/build/html/generated/pingouin.power_ttest.html"><code class="docutils literal notranslate"><span class="pre">pingouin</span></code></a> package in Python.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">power_ttest</span></code> method provides a single function that can calculate power, sample size, effect size, or significance level, depending on which parameters are provided. It handles one-sample, paired-sample, and two-sample independent t-tests (with equal sample sizes; use <a class="reference external" href="https://pingouin-stats.org/build/html/generated/pingouin.power_ttest2n.html"><code class="docutils literal notranslate"><span class="pre">power_ttest2n</span></code></a> for unequal sample sizes). However, it doesn’t directly create plots like <code class="docutils literal notranslate"><span class="pre">plot_power</span></code> does.</p>
<p>Imagine a biotech is studying the effectiveness of a new cognitive training program aimed at improving memory scores. It plans to administer a memory test to participants <em>before and after</em> the training program. Researchers want to determine the sample size needed to have an 80% chance (power = 0.8) of detecting a statistically significant improvement in memory scores after the training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pingouin</span> <span class="k">as</span> <span class="nn">pg</span>

<span class="c1"># Parameters</span>
<span class="n">effect_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span> <span class="c1"># Different effect sizes (Cohen&#39;s d)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>                             <span class="c1"># Significance level</span>
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>      <span class="c1"># Sample sizes/number of pairs to consider</span>
<span class="n">power</span> <span class="o">=</span> <span class="mf">0.8</span>                              <span class="c1"># Desired power (80%)</span>

<span class="c1"># Calculate and print required sample sizes</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Required sample sizes for 80% power in a paired t-test:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">es</span> <span class="ow">in</span> <span class="n">effect_sizes</span><span class="p">:</span>
    <span class="n">required_n</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">power_ttest</span><span class="p">(</span>
        <span class="n">d</span><span class="o">=</span><span class="n">es</span><span class="p">,</span>
        <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># parameter to solve</span>
        <span class="n">power</span><span class="o">=</span><span class="n">power</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
        <span class="n">contrast</span><span class="o">=</span><span class="s1">&#39;paired&#39;</span><span class="p">,</span>
        <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;two-sided&#39;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ES of paired differences = </span><span class="si">{</span><span class="n">es</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">required_n</span><span class="p">))</span><span class="si">}</span><span class="s2"> pairs&quot;</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="c1"># plt.figure(figsize=(10, 6))</span>
<span class="k">for</span> <span class="n">es</span> <span class="ow">in</span> <span class="n">effect_sizes</span><span class="p">:</span>
    <span class="n">power_values</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">power_ttest</span><span class="p">(</span>
        <span class="n">d</span><span class="o">=</span><span class="n">es</span><span class="p">,</span>
        <span class="n">n</span><span class="o">=</span><span class="n">sample_sizes</span><span class="p">,</span>
        <span class="n">power</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># parameter we want to solve</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
        <span class="n">contrast</span><span class="o">=</span><span class="s1">&#39;paired&#39;</span><span class="p">,</span>
        <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;two-sided&#39;</span><span class="p">,</span> <span class="c1"># default</span>
    <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">sample_sizes</span><span class="p">,</span>
        <span class="n">power_values</span><span class="p">,</span>
        <span class="c1"># &#39;o-&#39;,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Effect Size = </span><span class="si">{</span><span class="n">es</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="n">power</span><span class="p">,</span>
    <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Desired power (</span><span class="si">{</span><span class="n">power</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size (paired observations)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Power ($1 - \beta$)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Power analysis for paired t-test ($</span><span class="se">\\</span><span class="s2">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Required sample sizes for 80% power in a paired t-test:
ES of paired differences = 0.2: 199 pairs
ES of paired differences = 0.5: 34 pairs
ES of paired differences = 0.8: 15 pairs
</pre></div>
</div>
<img alt="_images/0b879cfb5d29f2713a4448d71c7141b7c89d290d7bd879b02168df78ff1d37fa.png" src="_images/0b879cfb5d29f2713a4448d71c7141b7c89d290d7bd879b02168df78ff1d37fa.png" />
</div>
</div>
<p>The curves clearly demonstrate that power increases as the sample size per group increases. This means that with larger samples, we have a higher probability of detecting a statistically significant difference between the groups, even if the true effect size is small.</p>
<p>Furthermore, each curve represents a different level of standard deviation (SD) within the groups. As the SD increases (i.e., the data becomes more spread out), the curves shift to the right. This means we need a larger sample size to achieve the same level of power when the data is more variable.</p>
</section>
</section>
<section id="choosing-a-sample-size">
<h3>Choosing a sample size<a class="headerlink" href="#choosing-a-sample-size" title="Link to this heading">#</a></h3>
<section id="tools-for-power-calculations">
<h4>Tools for power calculations<a class="headerlink" href="#tools-for-power-calculations" title="Link to this heading">#</a></h4>
<p>As we’ve seen, sample size calculations for a desired level of statistical power require to specify the following:</p>
<ul class="simple">
<li><p>Significance level (<span class="math notranslate nohighlight">\(\alpha\)</span>): The acceptable risk of a false positive (Type I error).</p></li>
<li><p>Desired power (<span class="math notranslate nohighlight">\(1 - \beta\)</span>): The desired probability of correctly detecting a true effect.</p></li>
<li><p>Minimum effect size: The smallest effect size we consider meaningful and worth detecting.</p></li>
<li><p>Standard deviation (SD): A measure of the variability or scatter in the data. Alternatively, for proportion-based tests, we would need the expected proportion.</p></li>
</ul>
<p>The type of test (one-tailed or two-tailed) also affects sample size calculations. One-tailed tests generally require smaller sample sizes but are only appropriate when we have a strong directional hypothesis. Importantly, power analysis formulas often rely on <em>assumptions</em> about the distribution of the data (e.g., normality). Violating these assumptions can affect the accuracy of the calculations.</p>
<p>Numerous online tools and software packages can assist with power analysis. Some popular options include:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.gpower.hhu.de">G*Power</a> for comparing means</p></li>
<li><p><a class="reference external" href="https://www.powerandsamplesize.com">Power and Sample Size</a>, for comparing means, K means, proportions, time-to-event data, odds ratio, etc.</p></li>
<li><p>R packages such as <a class="reference external" href="https://github.com/heliosdrm/pwr"><code class="docutils literal notranslate"><span class="pre">pwr</span></code></a> and <a class="reference external" href="https://github.com/pitakakariki/simr"><code class="docutils literal notranslate"><span class="pre">simr</span></code></a> that offer comprehensive power analysis functions within the R statistical environment</p></li>
<li><p>the Python libraries <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>, with the <code class="docutils literal notranslate"><span class="pre">solve_power</span></code> method from the <code class="docutils literal notranslate"><span class="pre">TTestPower</span></code> and <code class="docutils literal notranslate"><span class="pre">TTestIndPower</span></code> classes, and <code class="docutils literal notranslate"><span class="pre">pingouin</span></code> with <code class="docutils literal notranslate"><span class="pre">power_ttest</span></code>, as we have seen in our previous examples. The first library offers another option with the direct function import of <a class="reference external" href="https://www.statsmodels.org/stable/generated/statsmodels.stats.power.tt_ind_solve_power.html"><code class="docutils literal notranslate"><span class="pre">tt_ind_solve_power</span></code></a>.</p></li>
</ul>
<p>For example, the required sample size per group to achieve 80% power in detecting a 17 kg mean difference between men and women, given a standard deviation of 17 kg and an alpha level of 0.05 can be obtained as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.stats.power</span> <span class="kn">import</span> <span class="n">tt_ind_solve_power</span>

<span class="c1"># Parameters</span>
<span class="n">effect_size</span> <span class="o">=</span> <span class="mi">17</span> <span class="o">/</span> <span class="mi">17</span>  <span class="c1"># Cohen&#39;s d (mean_diff / sd_diff)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>           <span class="c1"># Significance level</span>
<span class="n">desired_power</span> <span class="o">=</span> <span class="mf">0.8</span>    <span class="c1"># Target power</span>

<span class="c1"># Calculate sample size (directly)</span>
<span class="n">n_per_group</span> <span class="o">=</span> <span class="n">tt_ind_solve_power</span><span class="p">(</span>
    <span class="n">effect_size</span><span class="o">=</span><span class="n">effect_size</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
    <span class="n">power</span><span class="o">=</span><span class="n">desired_power</span><span class="p">,</span>
    <span class="n">ratio</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>          <span class="c1"># Equal sample sizes</span>
    <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;two-sided&#39;</span>
<span class="p">)</span>

<span class="c1"># Display result (clear and concise)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Required sample size per group: </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">n_per_group</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Required sample size per group: 17
</pre></div>
</div>
</div>
</div>
<p>If we collect weights from 17 men and 17 women in many experiments, assuming the standard deviation (SD) of 17 kg is close to the true population SD, we’d expect approximately 80% of those experiments to yield statistically significant results (P &lt; 0.05). This is because our power analysis indicated that a sample size of 17 per group is sufficient to achieve 80% power to detect a mean difference of 17 kg with this SD.</p>
<p>Important Considerations:</p>
<ul class="simple">
<li><p>Assumption of Equal Variance (Homoscedasticity): This calculation assumes that the <em>variances</em> in the men and women groups are <em>equal</em>. If we have reason to believe the variances might be different, we should use a different power analysis function or adjust the calculation accordingly.</p></li>
<li><p>Assumption of Normality: The power analysis and the interpretation of the results rely on the assumption that the weight data are <em>normally distributed</em> within each group. If this assumption is violated, the results might not be accurate.</p></li>
<li><p>Standard Deviation: The <em>standard deviation</em> of 17 kg is an estimate. It’s crucial to have a <em>reliable estimate</em> of this value, either from prior studies or pilot data.</p></li>
<li><p>Effect Size: The chosen effect size of 1.0 (17 kg / 17 kg) is considered a <em>large effect</em>. If we expect a smaller effect, we will need a larger sample size to achieve the same power.</p></li>
</ul>
</section>
<section id="sample-sizes-for-comparing-proportions-a-b-test">
<h4>Sample sizes for comparing proportions (A/B test)<a class="headerlink" href="#sample-sizes-for-comparing-proportions-a-b-test" title="Link to this heading">#</a></h4>
<p>We can use a simplified formula derived from the principles of hypothesis testing for proportions, assuming the sample proportions follow a normal distribution, and incorporating the desired power and significance level:</p>
<div class="math notranslate nohighlight">
\[n=(z_{\alpha/2}+z_\beta)^2 \times \frac{p_1(1-p_1)+p_2(1-p_2)}{(p_1-p_2)^2}\]</div>
<p>The term <span class="math notranslate nohighlight">\((p_1 - p_2)\)</span> represents the effect size, i.e., the difference in proportions we want to detect, <span class="math notranslate nohighlight">\(z_{\alpha/2}\)</span> is the z-score corresponding to the significance level (<span class="math notranslate nohighlight">\(\alpha\)</span>) divided by 2 which defines the critical region in each tail of the distribution where we would reject the null hypothesis (here it’s a two-sided test), and <span class="math notranslate nohighlight">\(z_\beta\)</span> is the z-score corresponding to the desired power (<span class="math notranslate nohighlight">\(1-\beta\)</span>) which defines the distance between the alternative hypothesis distribution and the critical region.</p>
<p>Some methods calculate power and sample size based on exact statistical distributions (like the binomial distribution for proportions), while others use approximations (like the normal approximation to the binomial). These approximations might introduce <em>slight discrepancies</em>, especially with smaller sample sizes. Furthermore, the specific numerical algorithms used to solve for power and sample size can vary between functions, leading to slight differences in the results. There are more sophisticated methods available (e.g., using the binomial distribution directly) for complex scenarios or when precise calculations are required.</p>
<p>While the normal distribution is often used to approximate the binomial distribution in hypothesis tests for proportions (especially with larger sample sizes), there are also a few tools, such as <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.samplesize_proportions_2indep_onetail.html"><code class="docutils literal notranslate"><span class="pre">samplesize_proportions_2indep_onetail</span></code></a>, that allow us to calculate sample sizes for comparing proportions without relying on the normality assumption.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># parameters</span>
<span class="n">p2</span><span class="o">=</span> <span class="mi">5</span><span class="o">/</span><span class="mi">100</span>   <span class="c1"># baseline proportion, e.g., complication rate</span>
<span class="n">p1</span> <span class="o">=</span> <span class="mi">3</span><span class="o">/</span><span class="mi">100</span>  <span class="c1"># 2% absolute change, i.e., from 5% to 3%</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">power</span> <span class="o">=</span> <span class="mf">.9</span>

<span class="c1"># Calculate sample size using manual calculations</span>
<span class="n">za</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
<span class="n">zb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">power</span><span class="p">))</span>

<span class="n">n_per_group_manual</span> <span class="o">=</span> <span class="p">(</span><span class="n">za</span><span class="o">+</span><span class="n">zb</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">p1</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p1</span><span class="p">)</span> <span class="o">+</span> <span class="n">p2</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Calculate sample size using statsmodels</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.proportion</span> <span class="kn">import</span> <span class="n">samplesize_proportions_2indep_onetail</span>
<span class="n">n_per_group_statsmodels</span> <span class="o">=</span> <span class="n">samplesize_proportions_2indep_onetail</span><span class="p">(</span>
    <span class="n">diff</span><span class="o">=-</span><span class="mi">2</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># here p1 = prop2 + diff</span>
    <span class="n">prop2</span><span class="o">=</span><span class="mi">5</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># here prop2 is the proportion for the reference</span>
    <span class="n">power</span><span class="o">=</span><span class="n">power</span><span class="p">,</span>
    <span class="n">ratio</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Display results</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Required sample size per group (using manual calculations): </span><span class="se">\</span>
<span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">n_per_group_manual</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Required sample size per group (using statsmodels): </span><span class="se">\</span>
<span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">n_per_group_statsmodels</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Required sample size per group (using manual calculations): 2013
Required sample size per group (using statsmodels): 2016
</pre></div>
</div>
</div>
</div>
</section>
<section id="incorporating-fprp-into-sample-size-decisions">
<h4>Incorporating FPRP into sample size decisions<a class="headerlink" href="#incorporating-fprp-into-sample-size-decisions" title="Link to this heading">#</a></h4>
<p>The False Positive Report Probability (FPRP) can be estimated before conducting an experiment based on the chosen significance level, desired power, and the prior probability of the null hypothesis (H0) being true.</p>
<p>As shown in the next table, the FPRP is dramatically affected by the prior probability:</p>
<ul class="simple">
<li><p>1% Prior Probability: This represents a <em>highly speculative scenario</em> with <em>little evidence</em> to support the alternative hypothesis. In this case, a significant result has an 86.1% chance of being a <em>false positive</em>. Less than 1% of significant results would be true positives.</p></li>
<li><p>50% Prior Probability: When there’s solid theoretical or empirical backing for the alternative hypothesis, the FPRP drops significantly to 5.9%. This suggests a <em>much higher likelihood</em> of a significant result being a <em>true discovery</em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.05</span><span class="p">,</span> <span class="mf">.01</span><span class="p">,</span> <span class="mf">.001</span><span class="p">]</span>     <span class="c1"># significance levels tested</span>
<span class="n">powers</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.80</span><span class="p">,</span> <span class="mf">.90</span><span class="p">,</span> <span class="mf">.95</span><span class="p">,</span> <span class="mf">.99</span><span class="p">]</span> <span class="c1"># power levels tested</span>
<span class="n">pprobas</span><span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">.25</span><span class="p">,</span> <span class="mf">.5</span><span class="p">]</span> <span class="c1"># estimates of prior probability</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;How alpha, power, and prior probability affect FPRP</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;alpha</span><span class="se">\t</span><span class="s2">power</span><span class="se">\t</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">pprobas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">%</span><span class="se">\t</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">pprobas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">%</span><span class="se">\t</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">pprobas</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">%</span><span class="se">\t</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">pprobas</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">45</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">4.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">power</span> <span class="ow">in</span> <span class="n">powers</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\t</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">power</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">proba</span> <span class="ow">in</span> <span class="n">pprobas</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">proba</span><span class="p">)</span><span class="o">*</span><span class="n">alpha</span><span class="p">)</span><span class="o">/</span><span class="p">(((</span><span class="mi">1</span><span class="o">-</span><span class="n">proba</span><span class="p">)</span><span class="o">*</span><span class="n">alpha</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">proba</span><span class="o">*</span><span class="n">power</span><span class="p">))</span><span class="si">:</span><span class="s2">3.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>How alpha, power, and prior probability affect FPRP

alpha	power	1.0%	10.0%	25.0%	50.0%
---------------------------------------------
0.050
	80.0%	86.1%	36.0%	15.8%	5.9%
	90.0%	84.6%	33.3%	14.3%	5.3%
	95.0%	83.9%	32.1%	13.6%	5.0%
	99.0%	83.3%	31.3%	13.2%	4.8%
0.010
	80.0%	55.3%	10.1%	3.6%	1.2%
	90.0%	52.4%	9.1%	3.2%	1.1%
	95.0%	51.0%	8.7%	3.1%	1.0%
	99.0%	50.0%	8.3%	2.9%	1.0%
0.001
	80.0%	11.0%	1.1%	0.4%	0.1%
	90.0%	9.9%	1.0%	0.3%	0.1%
	95.0%	9.4%	0.9%	0.3%	0.1%
	99.0%	9.1%	0.9%	0.3%	0.1%
</pre></div>
</div>
</div>
</div>
<p>In essence, a study’s context is crucial. A well-designed experiment with a high prior probability of a true effect can yield reliable significant findings, while a speculative study with low prior probability is more likely to produce misleading results.</p>
<p>Therefore, it’s imperative to consider the prior probability when designing and interpreting studies. When faced with a low prior probability, researchers should adjust their expectations and potentially employ stricter significance thresholds or gather additional evidence to strengthen the credibility of their findings.</p>
</section>
<section id="complexities-when-computing-sample-size">
<h4>Complexities when computing sample size<a class="headerlink" href="#complexities-when-computing-sample-size" title="Link to this heading">#</a></h4>
<p>While power analysis provides a valuable framework for calculating sample size, the process is rarely straightforward. Several factors can influence the accuracy and applicability of our calculations. This section will address these complexities and offer practical guidance for making informed sample size decisions.</p>
<ul class="simple">
<li><p>Choosing the significance level <span class="math notranslate nohighlight">\(\alpha\)</span></p>
<ul>
<li><p>Think beyond tradition: don’t blindly default to the conventional <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>.</p></li>
<li><p>Prior probability matters: consider the <strong>prior probability</strong> of a true effect (pre-study evidence).</p></li>
<li><p>FPRP as a guide: estimate the False Positive Report Probability (FPRP) to understand the <strong>risk of false positives</strong>.</p></li>
<li><p>One-tailed vs. two-tailed: choose the appropriate test directionality based on the research question.</p></li>
</ul>
</li>
<li><p>Choosing desired statistical power (<span class="math notranslate nohighlight">\(1 - \beta\)</span>)</p>
<ul>
<li><p>Balance the FPRP: aim for a power level that, in combination with the chosen <span class="math notranslate nohighlight">\(\alpha\)</span>, results in a <strong>reasonably low FPRP</strong>.</p></li>
<li><p>Weigh the consequences: consider the relative costs of Type I and Type II errors. If <strong>missing a true effect</strong> is very costly, opt for higher power.</p></li>
</ul>
</li>
<li><p>Selecting the effect size</p>
<ul>
<li><p>Scientific relevance: focus on the <strong>smallest effect size</strong> that would be <strong>scientifically meaningful</strong>, not just statistically significant.</p></li>
<li><p>Don’t rely solely on published estimates: published effect sizes might be biased or not relevant to our specific context.</p></li>
<li><p>Consider standard effect sizes: use <strong>Cohen’s guidelines</strong> (e.g., 1/5th the SD, or 0.2 → small, half the SD, or 0.5 → medium, 0.8 → large) as a starting point, but don’t blindly adhere to them.</p></li>
<li><p>Don’t anchor on the expectations: base the sample size on the effect size we want to detect, not necessarily what we expect to find.</p></li>
<li><p>Think in ratios: if estimating the exact effect size is difficult, consider framing it as a <em>ratio relative to the standard deviation</em> (e.g., “we want to detect an effect size equal to half the SD”).</p></li>
</ul>
</li>
<li><p>Estimating expected variability</p>
<ul>
<li><p>Minimize variability: if possible, design the study to <strong>reduce the variability</strong> within groups (e.g., through stricter inclusion criteria, more controlled experimental conditions).</p></li>
<li><p>Pilot studies: conduct <strong>pilot studies</strong> to get a realistic estimate of variability. Small pilot studies often underestimate true variability (see the table below).</p></li>
<li><p>Published estimates with caution: be aware that published standard deviations might be biased downward due to selective reporting.</p></li>
</ul>
</li>
<li><p>Planning for unequal sample sizes</p>
<ul>
<li><p>Trade-offs: if we need to reduce the sample size in one group, we should be prepared to increase the other group’s size even more to maintain power.</p></li>
<li><p>Statistical guidance: consult a statistician for optimal strategies in dealing with unequal group sizes.</p></li>
</ul>
</li>
<li><p>Accounting for dropouts</p>
<ul>
<li><p>Anticipate attrition: always overestimate the sample size to account for potential dropouts during the study.</p></li>
<li><p>Retention strategies: consider strategies to minimize dropout rates to preserve statistical power.</p></li>
</ul>
</li>
<li><p>Leveraging paired designs</p>
<ul>
<li><p>Increased power: if the study design allows for pairing observations (e.g., pre- and post-test measures), take advantage of it! <strong>Paired t-tests are more powerful</strong> than independent t-tests for the same effect size.</p></li>
<li><p>Sample size adjustment: the required sample size for a paired t-test is typically smaller than that for an unpaired test, but keep in mind that we’ll need twice the number of total observations (one for each member of the pair).</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2</span>

<span class="k">def</span> <span class="nf">W</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.95</span><span class="p">):</span>
    <span class="n">t2</span><span class="o">=</span><span class="n">chi2</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># based on the equation from Sheskin 2011</span>
    <span class="n">t1</span><span class="o">=</span><span class="n">chi2</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">t1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">t2</span><span class="p">)</span>
    
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;95% CI of a SD</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;n</span><span class="se">\t</span><span class="s2">95% CI of SD&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------------------------&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">sample_size</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">sample_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">W</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">×SD - </span><span class="si">{</span><span class="n">W</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">05.2f</span><span class="si">}</span><span class="s2">×SD&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>95% CI of a SD

n	95% CI of SD
--------------------------
2	0.45×SD - 31.91×SD
3	0.52×SD - 06.28×SD
5	0.60×SD - 02.87×SD
10	0.69×SD - 01.83×SD
25	0.78×SD - 01.39×SD
50	0.84×SD - 01.25×SD
100	0.88×SD - 01.16×SD
500	0.94×SD - 01.07×SD
1000	0.96×SD - 01.05×SD
</pre></div>
</div>
</div>
</div>
<p>The discrepancy between a sample’s standard deviation (SD) and the true population SD can be substantial, particularly when dealing with small sample sizes. For example, with a sample size of only 3, the population SD could be up to 6.28 times larger than the sample SD, as reflected in the confidence interval for the standard deviation. Since the required sample size for a t-test is proportional to the square of the SD, if the SD obtained in a pilot study is half the true population SD, the calculated sample size would be merely one-quarter of what it should be.</p>
<p>This highlights the importance of using caution when relying on pilot studies or <em>small samples</em> to estimate variability for power analysis. It underscores the value of obtaining <strong>reliable estimates of the population standard deviation</strong>, either from larger pilot studies, previous research, or expert knowledge. When such information is unavailable, conservative estimates of variability or adjustments to sample size calculations should be considered to avoid underpowered studies.</p>
</section>
</section>
<section id="the-simulation-based-approach">
<h3>The simulation-based approach<a class="headerlink" href="#the-simulation-based-approach" title="Link to this heading">#</a></h3>
<p>While traditional power analysis methods (like those we’ve discussed) rely on theoretical formulas and assumptions, a simulation-based approach offers a more flexible and intuitive way to estimate sample size.</p>
<p>Here’s how it works:</p>
<ol class="arabic simple">
<li><p>Define the study design: clearly articulate the research question, the type of study (e.g., randomized controlled trial, observational study), and the primary outcome measure.</p></li>
<li><p>Simulate data Under H0: generate multiple datasets under the assumption that the null hypothesis is true (no effect). We can use distributions (e.g., normal, binomial) that mimic the expected characteristics of the data.</p></li>
<li><p>Simulate data under Ha: generate multiple datasets under the assumption that the alternative hypothesis is true, using an effect size we consider clinically meaningful.</p></li>
<li><p>Analyze and compare: perform the planned statistical test (e.g., t-test, chi-square) on each simulated dataset. Compare the proportion of significant results under H0 (false positives) to the proportion under Ha (true positives) for various sample sizes.</p></li>
<li><p>Determine sample size: choose the smallest sample size where the proportion of true positives is acceptably high (e.g., 80% power) while keeping the proportion of false positives at or below the chosen significance level (e.g., 5%).</p></li>
</ol>
<p>As a practical example, let’s say we’re designing a study to evaluate a new drug’s effect on systolic blood pressure. We hypothesize that the drug will reduce systolic blood pressure by 10 mmHg compared to a placebo. We want to determine the sample size needed to have an 80% chance of detecting this effect.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">simulate_study</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mean_difference</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">control_group</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">130</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># Mean = 130, SD = 15</span>
    <span class="n">treatment_group</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">130</span> <span class="o">-</span> <span class="n">mean_difference</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">control_group</span><span class="p">,</span> <span class="n">treatment_group</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p_value</span> <span class="o">&lt;</span> <span class="mf">0.05</span>  <span class="c1"># True if significant, False otherwise</span>

<span class="c1"># Simulation parameters</span>
<span class="n">mean_difference</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">power</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># Number of simulations</span>
<span class="n">max_sample_size</span> <span class="o">=</span> <span class="mi">80</span>   <span class="c1"># Max number of samples per group</span>

<span class="c1"># Run simulations</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># Iterate over sample sizes, starting from 10, up to the max_sample_size, in steps of 5</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_sample_size</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="c1"># Simulate num_simulations (1000 in this case) studies for the current sample size (n)</span>
    <span class="c1"># The simulate_study function returns True for significant results (p &lt; 0.05)</span>
    <span class="n">signif_results</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">simulate_study</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mean_difference</span><span class="o">=</span><span class="n">mean_difference</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">))</span>

    <span class="c1"># Calculate the power as the proportion of simulations that resulted in a significant finding</span>
    <span class="n">power</span> <span class="o">=</span> <span class="n">signif_results</span> <span class="o">/</span> <span class="n">num_simulations</span>

    <span class="c1"># Store the current sample size and the calculated power for later</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">power</span><span class="p">))</span>

<span class="c1"># Find the smallest sample size that achieves 80% power</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">power</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">power</span> <span class="o">&gt;=</span> <span class="mf">0.8</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Required sample size per group: </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Required sample size per group: 35
</pre></div>
</div>
</div>
</div>
<p>This simulation would run t-tests on randomly generated data and find the minimum sample size needed to detect the hypothesized effect with 80% power. While it can handle <strong>complex study designs</strong> and <strong>various outcome distributions</strong>, especially when normality assumptions don’t hold, it can be <strong>computationally intensive</strong> for large simulations.</p>
</section>
</section>
<section id="testing-for-equivalence-or-noninferiority">
<h2>Testing for equivalence or noninferiority<a class="headerlink" href="#testing-for-equivalence-or-noninferiority" title="Link to this heading">#</a></h2>
<p>Traditional hypothesis testing focuses on demonstrating a significant difference between treatments. However, in certain contexts, like generic drug development, the goal shifts towards demonstrating equivalence or noninferiority compared to a standard drug.</p>
<section id="bioequivalence">
<h3>Bioequivalence<a class="headerlink" href="#bioequivalence" title="Link to this heading">#</a></h3>
<p>The U.S. Food and Drug Administration (FDA) defines two drug formulations as <strong>bioequivalent</strong> if the 90% confidence interval (CI) of the ratio of their peak concentrations in blood plasma falls entirely within the range of <strong>0.80 to 1.25</strong> (note that the reciprocal of 80% is <span class="math notranslate nohighlight">\(1/0.8=125%\)</span>). This means the generic drug’s performance is expected to be within 20% of the reference drug’s performance in most cases, a difference considered clinically insignificant.</p>
<p>Using a 90% confidence interval (CI) for each of the two one-sided tests results in an overall 95% confidence level for the equivalence conclusion. This might seem counterintuitive, but it’s a result of how the two tests and their associated CIs combine.</p>
</section>
<section id="interpreting-equivalence-and-noninferiority-tests">
<h3>Interpreting equivalence and noninferiority tests<a class="headerlink" href="#interpreting-equivalence-and-noninferiority-tests" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Equivalence</strong>: when the entire 90% CI lies within the 0.80-1.25 equivalence zone, the generic drug is deemed equivalent to the standard drug.</p></li>
<li><p><strong>Non-equivalence</strong>: if the 90% CI falls completely outside the equivalence zone, the drugs are considered not equivalent.</p></li>
<li><p><strong>Inconclusive results</strong>: a 90% CI that straddles the equivalence zone (partially inside, partially outside) means the data are inconclusive regarding equivalence.</p></li>
<li><p><strong>Noninferiority</strong>: when the lower limit of the 90% CI exceeds 0.80, the new drug is considered noninferior. This indicates that the new drug is either superior to the standard drug or only slightly inferior, remaining within the zone of practical equivalence.</p></li>
</ul>
<img src="https://static.s4be.cochrane.org/app/uploads/2022/01/Figure-1-scaled.jpg" alt="Defining superiority, equivalence and non-inferiority in clinical trials" style="width: 800px;"/>
<p>There is a nice interactive visualization of <a class="reference external" href="https://rpsychologist.com/d3/equivalence/">equivalence, non-inferiority and superiority testing</a>, where we can play with the effect size, the sample, size and the margin and see how these parameters affect the equivalence.</p>
</section>
<section id="two-one-sided-tests-tost">
<h3>Two One-Sided Tests (TOST)<a class="headerlink" href="#two-one-sided-tests-tost" title="Link to this heading">#</a></h3>
<p>Standard hypothesis testing (null hypothesis significance testing or NHST) is designed to detect differences between groups or treatments. However, in equivalence testing, our goal is to demonstrate that two treatments are not meaningfully different - that they fall within a pre-defined range of equivalence.</p>
<p>The problem with applying NHST directly to equivalence testing is that failing to reject the null hypothesis (H0: no difference) doesn’t prove equivalence. It could simply mean our study lacked sufficient power to detect a small difference. The TOST procedure cleverly adapts NHST to the equivalence framework by posing <strong>two separate null hypotheses</strong>:</p>
<ol class="arabic simple">
<li><p>H01: the true mean ratio is less than or equal to the lower equivalence bound (0.80 in the FDA example).</p></li>
<li><p>H02: the true mean ratio is greater than or equal to the upper equivalence bound (1.25 in the FDA example).</p></li>
</ol>
<p>By conducting two <strong>one-sided tests</strong>, each with a significance level of 0.05, we essentially create two “null hypotheses of non-equivalence.”</p>
<p>Now, how do we interpret TOST results?</p>
<ul class="simple">
<li><p>Reject Both Null Hypotheses: if we reject both H01 and H02 (i.e., both one-sided p-values are less than 0.05), we conclude that the true mean ratio lies within the equivalence interval with 90% confidence. This is the desired outcome for demonstrating equivalence.</p></li>
<li><p>Fail to Reject Either Null Hypothesis: if we fail to reject either H01 or H02 (p-value ≥ 0.05), we cannot conclude equivalence. This doesn’t necessarily mean the treatments are not equivalent; it could be due to insufficient statistical power.</p></li>
</ul>
<p>Equivalence and non-inferiority testing present unique challenges and require specialized methods beyond the scope of this jupyter-book, so that consulting with a statistician familiar with the relevant regulatory guidelines is strongly recommended.</p>
</section>
</section>
<section id="assessing-diagnostic-accuracy">
<h2>Assessing diagnostic accuracy<a class="headerlink" href="#assessing-diagnostic-accuracy" title="Link to this heading">#</a></h2>
<p>Statistical significance is crucial for determining if an effect exists, but it doesn’t tell the whole story when evaluating the <strong>performance</strong> of diagnostic tests or classification models. In the realm of medical diagnosis, epidemiology, and machine learning, we need metrics that assess how well a test or model can distinguish between different conditions or classes.</p>
<p>Sensitivity, specificity, and Receiver Operating Characteristic (ROC) curves provide a powerful framework for quantifying and visualizing diagnostic <strong>accuracy</strong>. They offer insights into the trade-offs between true positive and false positive rates, allowing us to select optimal decision thresholds and compare the performance of different tests or models.</p>
<section id="definitions">
<h3>Definitions<a class="headerlink" href="#definitions" title="Link to this heading">#</a></h3>
<p>Deciding whether a clinical laboratory result is normal or abnormal involves a similar logic to statistical hypothesis testing, but with a focus on diagnostic accuracy:</p>
<ul class="simple">
<li><p><strong>False Negative (FN)</strong>: a test result is classified as negative (normal) when the patient actually has the disease.</p></li>
<li><p><strong>False Positive (FP)</strong>: a test result is classified as positive (abnormal) when the patient actually does not have the disease.</p></li>
</ul>
<p>The accuracy of a diagnostic test is assessed by two fundamental metrics:</p>
<ul class="simple">
<li><p><strong>Sensitivity</strong>, also commonly referred to as <strong>recall</strong>, <strong>hit rate</strong> or <strong>True Positive Rate (TPR)</strong>, quantifies how well the test <em>identifies individuals with the disease</em>. It’s the proportion of true positives (those who have the disease and test positive; TP) out of all individuals with the disease. Mathematically, it’s represented as <span class="math notranslate nohighlight">\(\text{sensitivity}=\frac{\text{TP}}{\text{TP}+\text{FN}}\)</span>, where FN represents false negatives (those with the disease who test negative).</p></li>
<li><p><strong>Specificity</strong>, also commonly referred to as <strong>selectivity</strong> or <strong>True Negative Rate (TNR)</strong>, quantifies how well the test <em>identifies individuals without the disease</em>. It’s the proportion of true negatives (those who don’t have the disease and test negative; TN) out of all individuals without the disease. Mathematically, it’s represented as: <span class="math notranslate nohighlight">\(\text{specificity}=\frac{\text{TN}}{\text{TN}+\text{FP}}\)</span>, where FP represents false positives (those without the disease who test positive).</p></li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head text-center"><p>Disease present (true)</p></th>
<th class="head text-center"><p>Disease absent (false)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Test positive</p></td>
<td class="text-center"><p>True Positive (TP)</p></td>
<td class="text-center"><p>False Positive (FP)</p></td>
</tr>
<tr class="row-odd"><td><p>Test negative</p></td>
<td class="text-center"><p>False Negative (FN)</p></td>
<td class="text-center"><p>True Negative (TN)</p></td>
</tr>
</tbody>
</table>
<p>Test <strong>accuracy</strong>, sometimes abbreviated as <strong>ACC</strong>, is a measure of how often a diagnostic test correctly classifies individuals as having or not having a particular disease or condition. It’s the proportion of true results (both true positives and true negatives) out of all test results: <span class="math notranslate nohighlight">\(\text{accuracy}=\frac{\text{TP}+\text{TN}}{\text{TP}+\text{FP}+\text{TN}+\text{FN}}\)</span>.</p>
<p>While accuracy is a useful overall metric, it can be misleading when dealing with <em>imbalanced datasets</em>, where the number of samples in one class is significantly higher than the other. In such cases, a classifier could achieve high accuracy by simply predicting the majority class most of the time, even if it performs poorly on the minority class. <strong>Balanced accuracy (BA)</strong> addresses this issue by taking the average of sensitivity (true positive rate) and specificity (true negative rate): <span class="math notranslate nohighlight">\(\text{BA}=\frac{\text{TPR}+\text{TNR}}{2}\)</span>.</p>
<p>While sensitivity and specificity are fundamental metrics, they don’t directly answer the questions that matter most to patients and clinicians:</p>
<ul class="simple">
<li><p>If a test result is positive, what is the probability that the patient actually has the disease?</p></li>
<li><p>If a test result is negative, what is the probability that the patient truly does not have the disease?</p></li>
</ul>
<p>These questions are addressed by two crucial measures of diagnostic accuracy:</p>
<ul class="simple">
<li><p><strong>Positive Predictive Value (PPV)</strong>, also commonly referred to as <strong>precision</strong>, is the probability that a positive test result correctly indicates the presence of the disease, i.e., <span class="math notranslate nohighlight">\(\text{PPV}=\frac{\text{TP}}{\text{TP}+\text{FP}}\)</span>.</p></li>
<li><p><strong>Negative Predictive Value (NPV)</strong>: the probability that a negative test result correctly indicates the absence of the disease., i.e., <span class="math notranslate nohighlight">\(\text{NPV}=\frac{\text{TN}}{\text{TN}+\text{FN}}\)</span>.</p></li>
</ul>
<p>While the Benjamini-Hochberg procedure offers a valuable method for controlling the false discovery rate in multiple hypothesis testing, the general concept of FDR extends beyond this specific method. In the context of diagnostic accuracy, we can also define the FDR as the proportion of positive test results that are actually incorrect (false positives) Mathematically, the <strong>False Discovery Rate (FDR)</strong> in diagnostic testing can be expressed as <span class="math notranslate nohighlight">\(\text{FDR}=\frac{\text{FP}}{\text{FP}+\text{TP}}\)</span>. The FDR is the complement of precision: <span class="math notranslate nohighlight">\(\text{PPV} = 1 - \text{FDR}\)</span>.</p>
<p>Similarly, the <strong>False Positive Rate (FPR)</strong>, sometimes called the “fall-out” or “false alarm rate”, is the probability of a positive test result (or a positive prediction by a model) when the condition or event being tested for is not actually present, i.e., <span class="math notranslate nohighlight">\(\text{FDR}=\frac{\text{FP}}{\text{FP}+\text{TN}}\)</span>. The FPR is the complement of specificity: <span class="math notranslate nohighlight">\(\text{TNR}=1-\text{FDR}\)</span>.</p>
<p>Finally, while sensitivity and specificity are crucial, they often paint an incomplete picture of a test’s <strong>overall performance</strong>, especially in situations with imbalanced class distributions (e.g., when the disease is rare). The <strong>F1-score</strong> addresses this by <em>combining both precision (positive predictive value) and recall (sensitivity)</em> into a single metric. The F1-score is the <em>harmonic mean of precision and recall</em>. It’s a balanced measure that gives equal weight to both the ability to correctly identify positive cases (recall) and the ability to avoid false positives (precision), i.e., <span class="math notranslate nohighlight">\(\text{F1-score}=2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}\)</span>.</p>
</section>
<section id="roc-curves">
<h3>ROC curves<a class="headerlink" href="#roc-curves" title="Link to this heading">#</a></h3>
<section id="decision-thresholds">
<h4>Decision thresholds<a class="headerlink" href="#decision-thresholds" title="Link to this heading">#</a></h4>
<p>Sensitivity and specificity provide valuable insights into a test’s performance at a fixed threshold. However, in practice, the optimal <strong>threshold</strong> for a diagnostic test can vary depending on the clinical setting and the consequences of different types of errors (false positives and false negatives).</p>
<p>Receiver Operating Characteristic (ROC) curves offer a solution by visualizing the trade-offs between sensitivity and specificity across a range of possible thresholds. An ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.</p>
<img src="https://docs.eyesopen.com/toolkits/cookbook/python/_images/roc2img.svg" alt="Example of a simple ROC curve" style="width: 400px;"/>
<p>An ROC curve is created by plotting the true positive rate (sensitivity) on the y-axis against the false positive rate (1-specificity) on the x-axis for <em>various decision thresholds</em> of a test. The resulting curve illustrates the relationship between <em>sensitivity and specificity</em> as the threshold changes. A perfect test would have a curve that hugs the top-left corner of the plot (high sensitivity and high specificity at all thresholds). A random guess would result in a diagonal line (no better than chance). At one extreme (bottom-left), the test is overly <em>conservative</em>, never producing a positive result, even for true cases. This translates to zero sensitivity but perfect specificity. At the other extreme (upper-right), the test is overly <em>liberal</em>, always yielding a positive result, regardless of the true condition. This results in 100% sensitivity but zero specificity.</p>
<p>The choice of threshold for a diagnostic test significantly affects its sensitivity and specificity, creating an inherent trade-off:</p>
<ul class="simple">
<li><p>High threshold: setting a high threshold (requiring stronger evidence for a positive result) leads to:</p>
<ul>
<li><p>Low sensitivity: the test is less likely to detect true cases of the disease, resulting in more false negatives.</p></li>
<li><p>High specificity: the test is more likely to correctly identify those without the disease, resulting in fewer false positives.
-Low threshold: setting a low threshold (requiring less evidence for a positive result) leads to:</p></li>
<li><p>High sensitivity: the test is more likely to detect true cases of the disease, minimizing false negatives.</p></li>
<li><p>Low specificity: the test is less likely to correctly identify those without the disease, leading to more false positives.</p></li>
</ul>
</li>
</ul>
<p>The optimal threshold depends on the specific clinical context and the relative consequences of false negatives and false positives. For example, screening tests for serious diseases often use a lower threshold to maximize sensitivity (catching as many cases as possible), even if it leads to more false positives that require further testing.</p>
</section>
<section id="area-under-the-curve-auc">
<h4>Area Under the Curve (AUC)<a class="headerlink" href="#area-under-the-curve-auc" title="Link to this heading">#</a></h4>
<p>The <strong>Area Under the ROC Curve (AUC)</strong> is a single metric summarizing a diagnostic/classifier’s overall ability to discriminate between positive and negative classes across all possible thresholds.:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{AUC} = 1.0\)</span>: this represents a <strong>perfect</strong> classifier, one that can flawlessly distinguish between positive and negative cases across all thresholds.</p></li>
<li><p><span class="math notranslate nohighlight">\(0.5 &lt; \text{AUC} &lt; 1.0\)</span>: a classifier with an AUC greater than 0.5 is considered a <strong>good</strong> classifier, as it performs better than random chance. The closer the AUC is to 1.0, the better the classifier’s ability to discriminate between classes.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{AUC} = 0.5\)</span>: an AUC of 0.5 indicates that the classifier is no better than <strong>random</strong> guessing. It’s essentially a coin flip whether the classifier will make a correct prediction.</p></li>
<li><p><span class="math notranslate nohighlight">\(0.0 &lt; \text{AUC} &lt; 0.5\)</span>: a classifier with an AUC less than 0.5 is performing worse than random chance. It’s making <strong>systematic errors</strong> and could potentially be improved by simply inverting its predictions.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{AUC} = 0.0\)</span>: this represents a completely <strong>incorrect</strong> classifier. It consistently makes the wrong prediction for every instance.</p></li>
</ul>
</section>
<section id="roc-and-python">
<h4>ROC and Python<a class="headerlink" href="#roc-and-python" title="Link to this heading">#</a></h4>
<p>In practice, these values (true positives, false positives, etc.) are derived from the results of a diagnostic test or a <strong>machine learning</strong> model applied to a dataset where the true labels are known. For instance, using Python’s <a class="reference external" href="https://scikit-learn.org/stable/index.html"><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> library</a>, we can train and evaluate various machine learning classifiers on labeled data, obtaining the confusion matrix that allows us to compute metrics like the F1-score and <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_det.html#sphx-glr-auto-examples-model-selection-plot-det-py">draw the ROC curve</a>.</p>
<p>While a thorough exploration of machine learning is beyond the scope of this book, it’s important to recognize that these tools offer powerful ways to assess and optimize the performance of diagnostic tests and classification models.</p>
</section>
</section>
<section id="case-studies">
<h3>Case studies<a class="headerlink" href="#case-studies" title="Link to this heading">#</a></h3>
<section id="porphyria-test-in-0-01-prevalence">
<h4>Porphyria test in 0.01% prevalence<a class="headerlink" href="#porphyria-test-in-0-01-prevalence" title="Link to this heading">#</a></h4>
<p>Porphyria is a rare group of disorders that affect the body’s ability to produce heme, a component of hemoglobin. The prevalence is estimated to be 1 in 10,000 individuals. We have a diagnostic test for porphyria with the following characteristics:</p>
<ul class="simple">
<li><p>Sensitivity: 82% (82 out of 100 patients with porphyria will test positive).</p></li>
<li><p>Specificity: 96.3% (963 out of 1000 individuals <em>without</em> porphyria will test negative, 3.7% of individuals <em>without</em> porphyria will test positive).</p></li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-center"><p>Has Porphyria</p></th>
<th class="head text-center"><p>Does not have Porphyria</p></th>
<th class="head text-right"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Positive test</p></td>
<td class="text-center"><p>82</p></td>
<td class="text-center"><p>36,996</p></td>
<td class="text-right"><p>37,078</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Negative test</p></td>
<td class="text-center"><p>18</p></td>
<td class="text-center"><p>962,904</p></td>
<td class="text-right"><p>962,922</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Total</p></td>
<td class="text-center"><p>100</p></td>
<td class="text-center"><p>999,900</p></td>
<td class="text-right"><p>1,000,000</p></td>
</tr>
</tbody>
</table>
<p>Despite the test’s high sensitivity (82%) and specificity (96.3%), the probability that a patient with a positive (abnormal) test result actually has porphyria is surprisingly low: only 82 out of 37078 individuals, i.e., 0.22%, or approximately 1 in 500. This means that the vast majority (99.8%) of positive results are false positives.</p>
<p>On the other hand, the test is highly reliable for ruling out porphyria. Of those who test negative, a staggering 99.998% truly do not have the disease. This high negative predictive value offers strong reassurance to individuals who receive a negative test result. A negative result on this test provides strong evidence that an individual does not have porphyria.</p>
</section>
<section id="porphyria-test-in-50-prevalence">
<h4>Porphyria test in 50% prevalence<a class="headerlink" href="#porphyria-test-in-50-prevalence" title="Link to this heading">#</a></h4>
<p>The low prevalence of porphyria (1 in 10,000) significantly impacts the test’s predictive value, even with high sensitivity and specificity.</p>
<p>Let’s analyze a scenario where we’re testing whether individuals have siblings, with a prevalence of 50% (meaning half the population has siblings). We’ll consider a sample of 1000 individuals and the same test characteristics (sensitivity = 82%, specificity = 96.3%) to see how the predictive values change compared to the rare disease example.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-center"><p>Has Porphyria</p></th>
<th class="head text-center"><p>Does not have Porphyria</p></th>
<th class="head text-right"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Positive test</p></td>
<td class="text-center"><p>410</p></td>
<td class="text-center"><p>19</p></td>
<td class="text-right"><p>429</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Negative test</p></td>
<td class="text-center"><p>90</p></td>
<td class="text-center"><p>481</p></td>
<td class="text-right"><p>571</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Total</p></td>
<td class="text-center"><p>500</p></td>
<td class="text-center"><p>500</p></td>
<td class="text-right"><p>1000</p></td>
</tr>
</tbody>
</table>
<p>In this scenario, approximately only 4.4% of positive tests are false positives (19 out of 429).</p>
</section>
<section id="hiv-test">
<h4>HIV test<a class="headerlink" href="#hiv-test" title="Link to this heading">#</a></h4>
<p>This case study examines the performance of an HIV test with a sensitivity of 99.9% and a specificity of 99.6%. We will simulate screening a population of 1 million individuals where the prevalence of HIV is 10%.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-center"><p>Has HIV</p></th>
<th class="head text-center"><p>Does not have HIV</p></th>
<th class="head text-right"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Positive test</p></td>
<td class="text-center"><p>99,900</p></td>
<td class="text-center"><p>3,600</p></td>
<td class="text-right"><p>103,500</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Negative test</p></td>
<td class="text-center"><p>100</p></td>
<td class="text-center"><p>896,400</p></td>
<td class="text-right"><p>896,500</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Total</p></td>
<td class="text-center"><p>100,000</p></td>
<td class="text-center"><p>900,000</p></td>
<td class="text-right"><p>1,000,000</p></td>
</tr>
</tbody>
</table>
<p>Of the approximately 103,500 positive tests, we estimate that 3.6% will be false positives due to the inherent limitations of the test.</p>
<p>Let’s now simulate the screening of 1 million individuals where the prevalence of HIV is 0.1%.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-center"><p>Has HIV</p></th>
<th class="head text-center"><p>Does not have HIV</p></th>
<th class="head text-right"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Positive test</p></td>
<td class="text-center"><p>999</p></td>
<td class="text-center"><p>3,996</p></td>
<td class="text-right"><p>4,995</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Negative test</p></td>
<td class="text-center"><p>1</p></td>
<td class="text-center"><p>995,004</p></td>
<td class="text-right"><p>995,005</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Total</p></td>
<td class="text-center"><p>1000</p></td>
<td class="text-center"><p>999,000</p></td>
<td class="text-right"><p>1,000,000</p></td>
</tr>
</tbody>
</table>
<p>In a scenario with a low disease prevalence of 0.1%, a significant proportion of positive test results, i.e., 80%, are likely to be false positives. Even with a highly specific test, the low prevalence means that the number of false positives can far outweigh the number of true positives. This leads to a higher chance of healthy individuals being incorrectly identified as positive.</p>
<p>The false positive rate isn’t solely determined by the test’s specificity. The prevalence of the disease within the tested population significantly influences the proportion of positive tests that are false positives.</p>
</section>
</section>
<section id="analogy-with-statistical-hypothesis-testing">
<h3>Analogy with statistical hypothesis testing<a class="headerlink" href="#analogy-with-statistical-hypothesis-testing" title="Link to this heading">#</a></h3>
<p>Interpreting the results of a diagnostic test, much like interpreting statistical significance, requires a nuanced understanding of the context. Just as the prevalence of a disease influences the meaning of a positive test, the prior probability of a true effect shapes our interpretation of statistical significance. Here’s a closer look at the parallels between the two frameworks:</p>
<ul class="simple">
<li><p><em>False negatives</em> in diagnostic tests are akin to <em>Type II errors</em> in hypothesis testing. Both represent missed opportunities to detect a true condition or effect.</p></li>
<li><p><em>False positives</em> in diagnostic tests are akin to <em>Type I errors</em> in hypothesis testing. Both represent erroneous claims of a condition or effect when none truly exists.</p></li>
<li><p><em>Sensitivity</em> in diagnostic tests mirrors <em>power</em> (<span class="math notranslate nohighlight">\(1 - \beta\)</span>) in hypothesis testing. Both quantify the ability to correctly identify a true positive.</p></li>
<li><p><em>Specificity</em> in diagnostic tests mirrors <em>one minus the significance level</em> (<span class="math notranslate nohighlight">\(1 - \alpha\)</span>) in hypothesis testing. Both measure the ability to correctly identify a true negative.</p></li>
</ul>
</section>
<section id="bayes-revisited">
<h3>Bayes revisited<a class="headerlink" href="#bayes-revisited" title="Link to this heading">#</a></h3>
<p>In the realm of diagnostic testing, Bayes’ Theorem provides a powerful framework for <em>updating</em> our beliefs about a patient’s disease status based on the results of a test. By incorporating <strong>prior knowledge</strong> (prevalence) with the test’s characteristics (sensitivity and specificity), we can calculate the probability of the patient <em>actually</em> having the disease given a positive test result (positive predictive value, PPV) or the probability of not having the disease given a negative result (negative predictive value, NPV).</p>
<p>A key component of Bayes’ Theorem is the <strong>likelihood ratio (LR)</strong>, which quantifies how much a test result should change our belief in the presence of the disease. The <strong>positive likelihood ratio (LR+)</strong> is the probability of a positive test result in a patient with the disease divided by the probability of a positive test result in a patient without the disease:</p>
<div class="math notranslate nohighlight">
\[\text{LR+} = \frac{\text{sensitivity}}{1 - \text{specificity}}\]</div>
<p>For example, if we take the previous case studies:</p>
<ul class="simple">
<li><p>Porphyria: <span class="math notranslate nohighlight">\(\text{LR+} = 0.82 / (1 - 0.963) = 22.2\)</span></p></li>
<li><p>HIV: <span class="math notranslate nohighlight">\(\text{LR+} = 0.999 / (1 - 0.996) = 249.75\)</span></p></li>
</ul>
<p>This means that a positive result is 22.2 times <em>more likely</em> in someone with porphyria than someone without it, and 249.75 times more likely in someone with HIV.</p>
<p>Now, Bayes’ theorem allows us to <strong>update</strong> the <strong>pretest odds</strong> (odds of disease before the test) to the <strong>posttest odds</strong> (odds of disease after the test) using the likelihood ratio as <span class="math notranslate nohighlight">\(\text{posttest odds} = \text{pretest odds} \times \frac{\text{sensitivity}}{1 - \text{specificity}} = \text{pretest odds} \times \text{likelihood ratio}\)</span>.</p>
<p>The following table illustrates this process for the porphyria and HIV examples:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Who was tested?</p></th>
<th class="head text-center"><p>Pretest probability</p></th>
<th class="head text-center"><p>Pretest odds</p></th>
<th class="head text-center"><p>LR+</p></th>
<th class="head text-center"><p>Posttest probability</p></th>
<th class="head text-center"><p>Posttest odds</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Porphyria (random screen)</p></td>
<td class="text-center"><p>0.0001</p></td>
<td class="text-center"><p>0.0001</p></td>
<td class="text-center"><p>22.2</p></td>
<td class="text-center"><p>0.0022</p></td>
<td class="text-center"><p>0.22%</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Porphyria (sibling)</p></td>
<td class="text-center"><p>0.50</p></td>
<td class="text-center"><p>1.0</p></td>
<td class="text-center"><p>22.2</p></td>
<td class="text-center"><p>22.2</p></td>
<td class="text-center"><p>95.7%</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>HIV (high prevalence)</p></td>
<td class="text-center"><p>0.1</p></td>
<td class="text-center"><p>0.111</p></td>
<td class="text-center"><p>249.75</p></td>
<td class="text-center"><p>27.5</p></td>
<td class="text-center"><p>96.5%</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>HIV (low prevalence)</p></td>
<td class="text-center"><p>0.001</p></td>
<td class="text-center"><p>0.001</p></td>
<td class="text-center"><p>249.75</p></td>
<td class="text-center"><p>0.25</p></td>
<td class="text-center"><p>20.0%</p></td>
</tr>
</tbody>
</table>
<p>with <span class="math notranslate nohighlight">\(\text{odds} = \frac{\text{probability}}{1-\text{probability}}\)</span> and <span class="math notranslate nohighlight">\(\text{probability} = \frac{\text{odds}}{1 + \text{odds}}\)</span>.</p>
<p>Even with a high LR+, a positive result in a random screen only slightly increases the probability of having porphyria (0.22%). This is because the disease is very rare. A positive result with a high LR+ is much more informative when the pretest probability is high, as in the case of a sibling test (95.7%). Finally, for the same test (HIV), the posttest probability varies dramatically depending on the pretest probability. A positive result in a high-prevalence population is much more likely to be a true positive than in a low-prevalence population.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Throughout this chapter, we’ve explored the multifaceted concept of statistical significance, delving into its strengths, limitations, and potential pitfalls. We’ve seen that p-values, while valuable, can be misleading if interpreted in isolation or without considering factors like sample size, effect size, power, and prior probability.</p>
<p>We’ve also highlighted the importance of alternative approaches like confidence intervals, effect size estimation, and Bayesian methods, which offer a more comprehensive understanding of statistical inference. These tools can help us move beyond simple “significant” or “non-significant” dichotomies and towards a more nuanced interpretation of research findings.</p>
<p>By integrating these additional layers of evidence, we can arrive at more robust and informed conclusions. We can avoid the pitfalls of p-value hacking and publication bias, and we can focus on research that truly advances our understanding of the biological world.</p>
</section>
<section id="session-information">
<h2>Session Information<a class="headerlink" href="#session-information" title="Link to this heading">#</a></h2>
<p>The output below details all packages and version necessary to reproduce the results in this report.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>python<span class="w"> </span>--version
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------&quot;</span><span class="p">)</span>
<span class="c1"># List of packages we want to check the version</span>
<span class="n">packages</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;numpy&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy&#39;</span><span class="p">,</span> <span class="s1">&#39;statsmodels&#39;</span><span class="p">,</span> <span class="s1">&#39;pingouin&#39;</span><span class="p">,</span> <span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;seaborn&#39;</span><span class="p">]</span>

<span class="c1"># Initialize an empty list to store the versions</span>
<span class="n">versions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop over the packages</span>
<span class="k">for</span> <span class="n">package</span> <span class="ow">in</span> <span class="n">packages</span><span class="p">:</span>
    <span class="c1"># Get the version of the package</span>
    <span class="n">output</span> <span class="o">=</span> <span class="o">!</span>pip<span class="w"> </span>show<span class="w"> </span><span class="o">{</span>package<span class="o">}</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>findstr<span class="w"> </span><span class="s2">&quot;Version&quot;</span>
    <span class="c1"># If the output is not empty, get the version</span>
    <span class="k">if</span> <span class="n">output</span><span class="p">:</span>
        <span class="n">version</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">version</span> <span class="o">=</span> <span class="s1">&#39;Not installed&#39;</span>
    <span class="c1"># Append the version to the list</span>
    <span class="n">versions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">version</span><span class="p">)</span>

<span class="c1"># Print the versions</span>
<span class="k">for</span> <span class="n">package</span><span class="p">,</span> <span class="n">version</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">packages</span><span class="p">,</span> <span class="n">versions</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">package</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Python 3.12.3
-------------
numpy: 1.26.4
scipy: 1.13.1
statsmodels: 0.14.2
pingouin: 0.5.4
matplotlib: 3.9.0
seaborn: 0.13.2
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="12%20-%20Confidence%20Interval%20of%20a%20Mean.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Confidence Interval of a Mean</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-reproducibility-challenge-of-p-values">The reproducibility challenge of P-values</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-variability-of-p-values-under-a-true-effect">The variability of P-values under a true effect</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-risk-of-false-positives-under-the-null-hypothesis">The risk of false positives under the null hypothesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-influence-of-sample-size-on-p-values">The influence of sample size on P-values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-danger-of-ad-hoc-sample-size-decisions-and-cumulative-p-values">The danger of ad hoc sample size decisions and cumulative P-values</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-and-p-values">Confidence intervals and P-values</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-significance-and-hypothesis-testing">Statistical significance and hypothesis testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-i-and-type-ii-errors">Type I and type II errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-statistical-significance">Interpreting statistical significance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-significance-level-alpha">The significance level (<span class="math notranslate nohighlight">\(\alpha\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-false-positive-report-probability-fprp">The False Positive Report Probability (FPRP)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prior-probability-influences-the-fprp">The prior probability influences the FPRP</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-1">Prior probability = 1%</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-50">Prior probability = 50%</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-challenge-of-multiple-comparisons">The challenge of multiple comparisons</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bonferroni-correction">The Bonferroni correction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-false-discovery-rate-fdr">The False Discovery Rate (FDR)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-power">Statistical power</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interlude-the-effect-size">Interlude - The effect size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relationship-between-power-effect-size-sample-size-and-variability">Relationship between power, effect size, sample size, and variability</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-sample-t-test">One-sample t-test</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#unpaired-t-test">Unpaired t-test</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-choice-of-80-power">The choice of 80% power</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#paired-t-test">Paired t-test</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-a-sample-size">Choosing a sample size</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-for-power-calculations">Tools for power calculations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-sizes-for-comparing-proportions-a-b-test">Sample sizes for comparing proportions (A/B test)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#incorporating-fprp-into-sample-size-decisions">Incorporating FPRP into sample size decisions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#complexities-when-computing-sample-size">Complexities when computing sample size</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-simulation-based-approach">The simulation-based approach</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-for-equivalence-or-noninferiority">Testing for equivalence or noninferiority</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bioequivalence">Bioequivalence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-equivalence-and-noninferiority-tests">Interpreting equivalence and noninferiority tests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#two-one-sided-tests-tost">Two One-Sided Tests (TOST)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-diagnostic-accuracy">Assessing diagnostic accuracy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">Definitions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-curves">ROC curves</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-thresholds">Decision thresholds</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#area-under-the-curve-auc">Area Under the Curve (AUC)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-and-python">ROC and Python</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#case-studies">Case studies</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#porphyria-test-in-0-01-prevalence">Porphyria test in 0.01% prevalence</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#porphyria-test-in-50-prevalence">Porphyria test in 50% prevalence</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hiv-test">HIV test</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-with-statistical-hypothesis-testing">Analogy with statistical hypothesis testing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-revisited">Bayes revisited</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#session-information">Session Information</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sébastien Wieckowski
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>